{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# %pip install -q evaluate\n","# %pip install -q opendatasets\n","# %pip install -q --upgrade accelerate\n","# %pip install -q --upgrade transformers\n","# %pip install -q peft\n","# %pip install -q --upgrade bitsandbytes\n","# %pip install -q accelerate"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:17:17.544968Z","iopub.status.busy":"2024-04-17T16:17:17.544513Z","iopub.status.idle":"2024-04-17T16:17:34.048052Z","shell.execute_reply":"2024-04-17T16:17:34.047162Z","shell.execute_reply.started":"2024-04-17T16:17:17.544934Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-17 16:17:26.974111: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-17 16:17:26.974223: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-17 16:17:27.104202: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import pandas as pd \n","import torch\n","import torch.nn as nn\n","import torchtext\n","from torch.utils.data import Dataset, random_split\n","from typing import List, Dict, Union\n","from typing import Any, TypeVar\n","import pandas as pd\n","import os\n","import copy\n","import gc\n","import evaluate\n","import opendatasets as od\n","from huggingface_hub import login\n","from typing import Optional, Tuple, Union\n","\n","from datasets import load_dataset, Features, Value\n","import accelerate\n","\n","from peft import LoftQConfig, LoraConfig, get_peft_model, PeftModel\n","\n","import transformers\n","from transformers.modeling_outputs import QuestionAnsweringModelOutput\n","from transformers import BertLMHeadModel, AutoConfig, BitsAndBytesConfig,Conv1D\n","from transformers import AutoTokenizer, Seq2SeqTrainingArguments \n","from transformers import Seq2SeqTrainer, AutoModelForCausalLM, IntervalStrategy, AutoModelForQuestionAnswering\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{},"source":["set a seed and confirm CUDA support"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:17:37.918255Z","iopub.status.busy":"2024-04-17T16:17:37.917498Z","iopub.status.idle":"2024-04-17T16:17:37.925228Z","shell.execute_reply":"2024-04-17T16:17:37.924209Z","shell.execute_reply.started":"2024-04-17T16:17:37.918219Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch Version:  2.1.2\n","torchtext Version:  0.16.2\n","Using GPU.\n"]}],"source":["torch.manual_seed(2137)\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.backends.cudnn.deterministic = True\n","\n","print(\"PyTorch Version: \", torch.__version__)\n","print(\"torchtext Version: \", torchtext.__version__)\n","print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Download"]},{"cell_type":"markdown","metadata":{},"source":["## Downloading MedDialog Dataset"]},{"cell_type":"markdown","metadata":{},"source":["NOTE: you will need a kaggle API key for the following to work"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:17:40.116801Z","iopub.status.busy":"2024-04-17T16:17:40.115881Z","iopub.status.idle":"2024-04-17T16:17:40.828713Z","shell.execute_reply":"2024-04-17T16:17:40.827340Z","shell.execute_reply.started":"2024-04-17T16:17:40.116762Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Error: JSON file not found at kaggle.json\n"]},{"ename":"NameError","evalue":"name 'json_data' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Access username and key from the JSON data\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m   username \u001b[38;5;241m=\u001b[39m \u001b[43mjson_data\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m   key \u001b[38;5;241m=\u001b[39m json_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n","\u001b[0;31mNameError\u001b[0m: name 'json_data' is not defined"]}],"source":["import json\n","\n","# Path to your JSON file\n","json_file_path = \"kaggle.json\"\n","\n","# Open the file and read the content\n","try:\n","  with open(json_file_path, \"r\") as f:\n","    json_data = json.load(f)\n","except FileNotFoundError:\n","  print(f\"Error: JSON file not found at {json_file_path}\")\n","  exit(1)\n","\n","# Access username and key from the JSON data\n","try:\n","  username = json_data[\"username\"]\n","  key = json_data[\"key\"]\n","except KeyError:\n","  print(\"Error: 'username' or 'key' key not found in JSON data\")\n","  exit(1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.environ['KAGGLE_USERNAME'] = username\n","os.environ['KAGGLE_KEY'] = key\n","\n","# Assign the Kaggle data set URL into variable\n","dataset = 'https://www.kaggle.com/datasets/dsxavier/diagnoise-me'\n","# Using opendatasets let's download the data sets\n","od.download(dataset, \"dataset\")"]},{"cell_type":"markdown","metadata":{},"source":["## Downloading USMLE Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:17:44.705348Z","iopub.status.busy":"2024-04-17T16:17:44.704621Z","iopub.status.idle":"2024-04-17T16:17:46.819493Z","shell.execute_reply":"2024-04-17T16:17:46.818681Z","shell.execute_reply.started":"2024-04-17T16:17:44.705311Z"},"trusted":true},"outputs":[],"source":["USMLE_dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options\", split=\"test\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:17:48.218918Z","iopub.status.busy":"2024-04-17T16:17:48.217993Z","iopub.status.idle":"2024-04-17T16:17:48.236423Z","shell.execute_reply":"2024-04-17T16:17:48.235451Z","shell.execute_reply.started":"2024-04-17T16:17:48.218877Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'question': 'A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?', 'answer': 'Tell the attending that he cannot fail to disclose this mistake', 'options': {'A': 'Disclose the error to the patient and put it in the operative report', 'B': 'Tell the attending that he cannot fail to disclose this mistake', 'C': 'Report the physician to the ethics committee', 'D': 'Refuse to dictate the operative report'}, 'meta_info': 'step1', 'answer_idx': 'B', 'metamap_phrases': ['junior orthopaedic surgery resident', 'completing', 'carpal tunnel repair', 'department chairman', 'attending physician', 'case', 'resident', 'cuts', 'flexor tendon', 'tendon', 'repaired', 'complication', 'attending', 'resident', 'patient', 'fine', 'need to report', 'minor complication', 'not', 'patient', 'not', 'to make', 'patient worry', 'resident to leave', 'complication out', 'operative report', 'following', 'correct next action', 'resident to take']}\n","1273\n"]}],"source":["print(USMLE_dataset[0])\n","print(len(USMLE_dataset))"]},{"cell_type":"markdown","metadata":{},"source":["# Load Datasets"]},{"cell_type":"markdown","metadata":{},"source":["## Loading MedDialog Dataset"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:45:35.691496Z","iopub.status.busy":"2024-04-17T16:45:35.691081Z","iopub.status.idle":"2024-04-17T16:45:36.514042Z","shell.execute_reply":"2024-04-17T16:45:36.513013Z","shell.execute_reply.started":"2024-04-17T16:45:35.691464Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['id', 'Description', 'Doctor', 'Patient'], dtype='object')\n","2574\n"]}],"source":["DATA_PATH = \"dataset\\\\diagnoise-me\\\\diagnose_en_dataset.feather\"\n","DATA_PATH = \"/kaggle/input/diagnoise-me/diagnose_en_dataset.feather\"\n","SEQ_LEN: int = 1024\n","data = pd.read_feather(DATA_PATH)\n","SAMPLE_SIZE: int =  int(data.shape[0] * 0.01) #get 1% of the data\n","data = data[:SAMPLE_SIZE]\n","print(data.keys())\n","print(len(data))\n","\n","\n"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:45:38.158546Z","iopub.status.busy":"2024-04-17T16:45:38.157643Z","iopub.status.idle":"2024-04-17T16:45:38.167966Z","shell.execute_reply":"2024-04-17T16:45:38.166970Z","shell.execute_reply.started":"2024-04-17T16:45:38.158511Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train data shape: (2316, 4)\n","Eval data shape: (258, 4)\n"]}],"source":["# Split data into train and eval sets with 70% for training\n","train_data, eval_data = train_test_split(data, test_size=0.1, random_state=42)\n","\n","train_data = train_data.reset_index(drop=True)\n","eval_data = eval_data.reset_index(drop=True)\n","\n","# Print the shapes of the train and eval sets\n","print(\"Train data shape:\", train_data.shape)\n","print(\"Eval data shape:\", eval_data.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Loading USMLE Dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:18:14.246248Z","iopub.status.busy":"2024-04-17T16:18:14.245573Z","iopub.status.idle":"2024-04-17T16:18:14.278870Z","shell.execute_reply":"2024-04-17T16:18:14.277916Z","shell.execute_reply.started":"2024-04-17T16:18:14.246217Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["USMLELiveEQA data shape: (1273, 3)\n"]}],"source":["USMLE_dataset = pd.DataFrame({'Doctor': USMLE_dataset[\"answer\"], 'Patient': USMLE_dataset[\"question\"], 'Options':USMLE_dataset[\"options\"]})\n","# Print the shapes of the set\n","print(\"USMLELiveEQA data shape:\", USMLE_dataset.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Create an output directory"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:18:16.694011Z","iopub.status.busy":"2024-04-17T16:18:16.693190Z","iopub.status.idle":"2024-04-17T16:18:16.698604Z","shell.execute_reply":"2024-04-17T16:18:16.697545Z","shell.execute_reply.started":"2024-04-17T16:18:16.693979Z"},"trusted":true},"outputs":[],"source":["os.makedirs('./results', exist_ok = True)\n","OUTPUT_DIR: str = './results'"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:44:02.343683Z","iopub.status.busy":"2024-04-17T16:44:02.342866Z","iopub.status.idle":"2024-04-17T16:44:02.348971Z","shell.execute_reply":"2024-04-17T16:44:02.347798Z","shell.execute_reply.started":"2024-04-17T16:44:02.343641Z"},"trusted":true},"outputs":[],"source":["# tokens for the datset\n","MODEL_NAME: str = 'UnfilteredAI/Mia-1B'\n","BOS_TOKEN: str = '<|startoftext|>'\n","EOS_TOKEN: str = '<|endoftext|>'\n","PAD_TOKEN: str = '<|pad|>'"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:44:04.022823Z","iopub.status.busy":"2024-04-17T16:44:04.022104Z","iopub.status.idle":"2024-04-17T16:44:05.670137Z","shell.execute_reply":"2024-04-17T16:44:05.669319Z","shell.execute_reply.started":"2024-04-17T16:44:04.022787Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28002bb9dc3346d7a4837681f1a3fc20","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02052bb42ccf4281bc1ad8d913acf94d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/462 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"763bf67273e64cccbc37dfc8ea0d037b","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0010f874633e4162b4d3fca796010613","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load tokenizer \n","MAX_TOKEN_LENGTH = 1024\n","\n","# for evaluation\n","ltokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, bos_token = BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN)\n","ltokenizer.padding_side = 'left'\n","ltokenizer.truncation_side = 'left'\n","\n","# for training\n","rtokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, bos_token = BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN)\n","rtokenizer.padding_side = 'right'\n","rtokenizer.truncation_side = 'right'"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:44:14.419028Z","iopub.status.busy":"2024-04-17T16:44:14.418328Z","iopub.status.idle":"2024-04-17T16:44:17.549631Z","shell.execute_reply":"2024-04-17T16:44:17.548604Z","shell.execute_reply.started":"2024-04-17T16:44:14.418990Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00dc875cdedb4621b3c41ca8fc3b3ca1","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n","Some weights of BertLMHeadModel were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["Embedding(28999, 768)"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map={\"\": 0})\n","base_model.resize_token_embeddings(len(rtokenizer))"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:44:24.865824Z","iopub.status.busy":"2024-04-17T16:44:24.865409Z","iopub.status.idle":"2024-04-17T16:44:24.872863Z","shell.execute_reply":"2024-04-17T16:44:24.871896Z","shell.execute_reply.started":"2024-04-17T16:44:24.865790Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BertLMHeadModel(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28999, 768)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (cls): BertOnlyMLMHead(\n","    (predictions): BertLMPredictionHead(\n","      (transform): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (decoder): Linear(in_features=768, out_features=28999, bias=True)\n","    )\n","  )\n",")\n"]}],"source":["print(base_model)"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:45:04.977791Z","iopub.status.busy":"2024-04-17T16:45:04.976587Z","iopub.status.idle":"2024-04-17T16:45:05.037977Z","shell.execute_reply":"2024-04-17T16:45:05.036770Z","shell.execute_reply.started":"2024-04-17T16:45:04.977693Z"},"trusted":true},"outputs":[],"source":["lora_config = LoraConfig(\n","    lora_alpha=16, # lora alpha for scaling\n","    r=16, # rank\n","    lora_dropout=0.05, #dropout\n","    use_rslora=True, #  sets the adapter scaling factor to lora_alpha/math.sqrt(r)\n","    bias=\"none\", # dont train biases\n","    #target_modules=[\"query\", \"value\"],\n","    #layers_to_transform=[20]\n",")\n","model = get_peft_model(base_model, lora_config)\n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:45:10.338297Z","iopub.status.busy":"2024-04-17T16:45:10.337047Z","iopub.status.idle":"2024-04-17T16:45:10.350637Z","shell.execute_reply":"2024-04-17T16:45:10.349606Z","shell.execute_reply.started":"2024-04-17T16:45:10.338242Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 589824 || all params: 108961931 || trainable%: 0.5413119927178971\n"]},{"data":{"text/plain":["{'trainable': 589824, 'all': 108961931, 'trainable%': 0.5413119927178971}"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )\n","    return {\"trainable\": trainable_params, \"all\": all_param, \"trainable%\": 100 * trainable_params / all_param}\n","\n","print_trainable_parameters(model)"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing Data for Training"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Dataset"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:45:43.225569Z","iopub.status.busy":"2024-04-17T16:45:43.224906Z","iopub.status.idle":"2024-04-17T16:45:43.233153Z","shell.execute_reply":"2024-04-17T16:45:43.232046Z","shell.execute_reply.started":"2024-04-17T16:45:43.225533Z"},"trusted":true},"outputs":[],"source":["class DoctorPatientDataset(Dataset):\n","    \n","    def __init__(self, data, split):\n","        \n","        self.input_x: List = data[\"Patient\"]\n","        self.target: List = data[\"Doctor\"]\n","        self.split = split\n","\n","        try:\n","            self.options: List = data[\"Options\"]\n","        except:\n","            pass\n","            \n","    def __len__(self):\n","        return len(self.input_x)\n","    \n","    def __getitem__(self, idx):\n","        try:\n","            data = {\n","                'input': self.input_x[idx],\n","                'target': self.target[idx],\n","                'options': self.options[idx],\n","                'split': self.split\n","            }\n","        except:\n","            data = {\n","                'input': self.input_x[idx],\n","                'target': self.target[idx],\n","                'split': self.split\n","            }\n","        return data"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:45:45.390378Z","iopub.status.busy":"2024-04-17T16:45:45.389672Z","iopub.status.idle":"2024-04-17T16:45:45.395828Z","shell.execute_reply":"2024-04-17T16:45:45.394824Z","shell.execute_reply.started":"2024-04-17T16:45:45.390340Z"},"trusted":true},"outputs":[],"source":["train_dataset = DoctorPatientDataset(data = train_data, split = \"train\")\n","eval_dataset_1 = DoctorPatientDataset(data = eval_data, split = \"eval\")\n","eval_dataset_2 = DoctorPatientDataset(data = USMLE_dataset, split = \"eval\")"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Data Collator"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:48:33.846940Z","iopub.status.busy":"2024-04-17T16:48:33.846467Z","iopub.status.idle":"2024-04-17T16:48:33.862991Z","shell.execute_reply":"2024-04-17T16:48:33.862074Z","shell.execute_reply.started":"2024-04-17T16:48:33.846904Z"},"trusted":true},"outputs":[],"source":["def format_text(message, tokenizer):\n","    text = tokenizer.apply_chat_template(\n","        message,\n","        tokenize=False,\n","        add_generation_prompt=False\n","    )\n","    return text\n","\n","def custom_data_collator(features, return_tensors=\"pt\"):\n","    batch = {}\n","\n","    questions = [feature[\"input\"] for feature in features]\n","    answers = [feature[\"target\"] for feature in features]\n","    split = features[0][\"split\"]\n","\n","    # training\n","    if split == 'train':\n","        tokenizer = rtokenizer\n","        bos_token = rtokenizer.bos_token\n","        eos_token = rtokenizer.eos_token\n","        prompts = [f\"a medical student is preparing for her final examination. Her patient has said '{q}'. Explain to the student the most likely cause/course of action.\" for q in questions]\n","        #text = [f\"{bos_token}Question:{q}.Answer:{t}{eos_token}\" for q, t in zip(questions, answers)]\n","        messages = [[\n","            {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n","            {\"role\": \"user\", \"content\": prompt},\n","            {\"role\": \"assistant\", \"content\": a}\n","        ] for prompt, a in zip(prompts, answers)]\n","\n","    # evaluation\n","    else:\n","        try:\n","            options = [feature[\"options\"] for feature in features]\n","            multi_choice = True\n","        except:\n","            multi_choice = False\n","\n","\n","        # tokenizer for evaluation\n","        tokenizer = ltokenizer\n","        bos_token = ltokenizer.bos_token\n","\n","        # Format text to be encoded\n","        if(multi_choice == False):\n","            # if we are not using the multiple choice dataset\n","            # text = [f\"{bos_token}Question:{q}.Answer:\" for q in questions]\n","            prompts = [f\"a medical student is preparing for her final examination. Her patient has said '{q}'. Explain to the student the most likely cause/course of action.\" for q in questions]\n","            messages = [[\n","                {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n","                {\"role\": \"user\", \"content\": prompt},\n","                {\"role\": \"assistant\", \"content\":\"\"}\n","            ] for prompt in prompts]\n","        else:\n","            # if we are using the multiple choice dataset\n","            # prompts = [f\"provided the following text about medical symptoms: '{q}' Please state the most likely cause/course of action from the options below: A: {o['A']} B: {o['B']} C: {o['C']} D: {o['D']} Please select your answer with the format shown in the following example:'The correct option is C'\" for q, o in zip(questions, options)]\n","            # text = [f\"{bos_token}Question:{p}.Answer:\" for p in prompts]\n","            prompts = [f\"a medical student is preparing for her final examination. Her patient has said '{q}'. Please clearly state a cause/course of action from the provided options:  A: {o['A']} B: {o['B']} C: {o['C']} D: {o['D']} and explain your answer\" for q, o in zip(questions, options)]\n","            messages = [[\n","                {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n","                {\"role\": \"user\", \"content\": prompt},\n","                {\"role\": \"assistant\", \"content\":\"\"}\n","            ] for prompt in prompts]\n","\n","\n","    # Tokenize the text\n","    text = list(map(lambda x: format_text(x, tokenizer), messages))\n","    \n","    #encoding = tokenizer(text, truncation=True, padding='max_length', max_length=MAX_TOKEN_LENGTH, return_tensors=return_tensors, add_special_tokens=False)\n","    encoding = tokenizer(text, truncation=True, padding='max_length', max_length=512, return_tensors=return_tensors, add_special_tokens=False)\n","\n","    # Prepare final batch dictionary\n","    batch[\"input_ids\"] = encoding[\"input_ids\"]\n","    batch[\"attention_mask\"] = encoding[\"attention_mask\"]\n","\n","    if return_tensors in [\"pt\", \"tf\"]:\n","        batch[\"labels\"] = copy.deepcopy(encoding[\"input_ids\"])\n","    return batch"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:48:42.842369Z","iopub.status.busy":"2024-04-17T16:48:42.841949Z","iopub.status.idle":"2024-04-17T16:48:42.869810Z","shell.execute_reply":"2024-04-17T16:48:42.869035Z","shell.execute_reply.started":"2024-04-17T16:48:42.842337Z"},"trusted":true},"outputs":[],"source":["training_args = Seq2SeqTrainingArguments(\n","    output_dir = OUTPUT_DIR, \n","    num_train_epochs = 1, \n","    evaluation_strategy=\"steps\",\n","    eval_steps = 10,\n","    logging_steps = 10,\n","    save_total_limit = 1,\n","    per_device_train_batch_size=8, \n","    per_device_eval_batch_size=1, \n","    warmup_steps=0, \n","    weight_decay=0.01, \n","    logging_dir='./logs',\n","    save_steps = 0,\n","    load_best_model_at_end=True,\n","    remove_unused_columns=False,\n","    report_to=['tensorboard']\n","    )"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:48:44.747250Z","iopub.status.busy":"2024-04-17T16:48:44.746287Z","iopub.status.idle":"2024-04-17T16:48:44.763062Z","shell.execute_reply":"2024-04-17T16:48:44.762072Z","shell.execute_reply.started":"2024-04-17T16:48:44.747213Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"source":["trainer = Seq2SeqTrainer(\n","    model=model, \n","    args=training_args, \n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset_1, \n","    data_collator=custom_data_collator\n",")"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:48:49.493561Z","iopub.status.busy":"2024-04-17T16:48:49.492787Z","iopub.status.idle":"2024-04-17T16:48:49.891296Z","shell.execute_reply":"2024-04-17T16:48:49.890243Z","shell.execute_reply.started":"2024-04-17T16:48:49.493523Z"},"trusted":true},"outputs":[{"data":{"text/plain":["2022"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["# trainer = None\n","# model = None\n","# base_model = None\n","# train_dataset = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:48:52.123597Z","iopub.status.busy":"2024-04-17T16:48:52.123138Z","iopub.status.idle":"2024-04-17T16:54:59.814306Z","shell.execute_reply":"2024-04-17T16:54:59.813394Z","shell.execute_reply.started":"2024-04-17T16:48:52.123563Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [290/290 06:06, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>10.621500</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>9.819500</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>8.961700</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>8.058400</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>7.475900</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>7.090800</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>6.666000</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>6.556900</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>6.164200</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>5.879000</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>5.913700</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>5.845800</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>5.798500</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>5.648400</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>5.602500</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>5.462100</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>5.339300</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>5.568600</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>5.556600</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>5.660500</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>5.303600</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>5.262800</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>5.225700</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>5.357800</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>5.303200</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>5.644900</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>5.093500</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>5.003100</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>5.182100</td>\n","      <td>No log</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=290, training_loss=6.243679664874899, metrics={'train_runtime': 367.3037, 'train_samples_per_second': 6.305, 'train_steps_per_second': 0.79, 'total_flos': 613974261510144.0, 'train_loss': 6.243679664874899, 'epoch': 1.0})"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.model.save_pretrained(f\"{OUTPUT_DIR}/model_save\")"]},{"cell_type":"markdown","metadata":{},"source":["# Load the Model"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:18:34.262823Z","iopub.status.busy":"2024-04-17T16:18:34.262181Z","iopub.status.idle":"2024-04-17T16:18:52.190938Z","shell.execute_reply":"2024-04-17T16:18:52.190034Z","shell.execute_reply.started":"2024-04-17T16:18:34.262792Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"source":["base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n","base_model.resize_token_embeddings(len(rtokenizer))\n","model = PeftModel.from_pretrained(base_model, f\"{OUTPUT_DIR}/model_save\")\n","trainer = Seq2SeqTrainer(\n","    model=model, \n","    args=training_args, \n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset_1, \n","    data_collator=custom_data_collator\n",")\n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:24:28.564411Z","iopub.status.busy":"2024-04-17T16:24:28.563624Z","iopub.status.idle":"2024-04-17T16:24:29.029371Z","shell.execute_reply":"2024-04-17T16:24:29.028311Z","shell.execute_reply.started":"2024-04-17T16:24:28.564380Z"},"trusted":true},"outputs":[{"data":{"text/plain":["1310"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["base_model = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["eval_result_1 = trainer.predict(eval_dataset_1, max_new_tokens=MAX_TOKEN_LENGTH)\n","eval_result_2 = trainer.predict(eval_dataset_2, max_new_tokens=MAX_TOKEN_LENGTH)\n","logits_1 = eval_result_1.predictions\n","logits_1[logits_1 == -100] = ltokenizer.eos_token_id\n","logits_2 = eval_result_2.predictions\n","logits_2[logits_2 == -100] = ltokenizer.eos_token_id"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# get the raw evaluation output\n","raw_text_result_1 = ltokenizer.batch_decode(logits_1, skip_special_tokens=True)\n","raw_text_result_2 = ltokenizer.batch_decode(logits_2, skip_special_tokens=True)\n","\n","# get the questions and ground truths from both evaluation datasets\n","questions_1 = []\n","ground_truth_1 = []\n","try:\n","    for item in eval_dataset_1:\n","        questions_1.append(item[\"input\"])\n","        ground_truth_1.append(item[\"target\"])\n","except:\n","    pass\n","\n","questions_2 = []\n","ground_truth_2 = []\n","try:\n","    for item in eval_dataset_2:\n","        questions_2.append(item[\"input\"])\n","        ground_truth_2.append(item[\"target\"])\n","except:\n","    pass\n","\n","# create lists for the text outputs\n","text_result_1 = list()\n","text_result_2 = list()\n","\n","# get the answers for the MedDialog dataset\n","for item in raw_text_result_1:\n","    index = item.find(\"Answer:\")\n","    output = item[index+7:]\n","    index = output.find(ltokenizer.eos_token)\n","    if(index > -1):\n","        output = output[:index]\n","    text_result_1.append(output)\n","\n","\n","# get the answers for the USMLE dataset\n","for item in raw_text_result_2:\n","    index = item.find(\"Answer:\")\n","    output = item[index+7:]\n","    index = output.find(ltokenizer.eos_token)\n","    if(index > -1):\n","        output = output[:index]\n","    text_result_2.append(output)\n","\n","\n","\n","# print the first 2 results from each dataset evaluation\n","print(\"============================MedDialog Evaluation============================\")\n","for question, answer in list(zip(questions_1, text_result_1))[:2]:\n","    print(f\"\"\"\n","    Question: {question}\n","    Answer: {answer}\n","    \"\"\")\n","\n","print(\"============================USMLE Evaluation============================\")\n","for question, answer in list(zip(questions_2, text_result_2))[:2]:\n","    print(f\"\"\"\n","    Question: {question}\n","    Answer: {answer}\n","    \"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["# Results"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Required Evaluation Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# perplexity - measures certainty of the model.\n","# METEOR - extension of BLEU (measure similarity between the output and the ground truth) but accounts for word semantics.\n","# ROUGE - considers n-gram overlap (recall) but also precision.\n","# SQuAD v2 - a metric for measuring a models correctness in answering the multiple choice questions\n","# Accuracy - use this for the multiple choice dataset\n","\n","perplexity_scorer = evaluate.load('perplexity')\n","meteor_scorer = evaluate.load('meteor')\n","rouge_scorer = evaluate.load('rouge')\n","squad_scorer = evaluate.load('squad_v2')\n","accuracy_scorer = evaluate.load('accuracy')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# compute the bleu and rouge scores for the MedDialog evaluation\n","bleu_score_1 = bleu_scorer.compute(predictions=text_result_1, references=ground_truth_1)\n","rouge_score_1 = rouge_scorer.compute(predictions=text_result_1, references=ground_truth_1)\n","\n","# compute the bleu and rouge scores for the USMLE evaluation\n","bleu_score_2 = bleu_scorer.compute(predictions=text_result_1, references=ground_truth_2)\n","rouge_score_2 = rouge_scorer.compute(predictions=text_result_1, references=ground_truth_2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print scores for MedDialog evaluation\n","print(\"score on MedDialog Dataset\")\n","print('BLEU1:', bleu_score_1['precisions'][0]*100)\n","print(f\"\"\"\n","ROUGE-1: {rouge_score_1['rouge1']*100}\n","ROUGE-2: {rouge_score_1['rouge2']*100}\n","ROUGE-L: {rouge_score_1['rougeL']*100}\n","\"\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print scores for USMLE evaluation\n","print(\"score on USMLE Dataset\")\n","print('BLEU1:', bleu_score_2['precisions'][0]*100)\n","print(f\"\"\"\n","ROUGE-1: {rouge_score_2['rouge1']*100}\n","ROUGE-2: {rouge_score_2['rouge2']*100}\n","ROUGE-L: {rouge_score_2['rougeL']*100}\n","\"\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import shutil\n","shutil.rmtree(\"/kaggle/working/logs\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(model.config)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:19:06.124119Z","iopub.status.busy":"2024-04-17T16:19:06.123246Z","iopub.status.idle":"2024-04-17T16:19:20.356913Z","shell.execute_reply":"2024-04-17T16:19:20.355878Z","shell.execute_reply.started":"2024-04-17T16:19:06.124087Z"},"trusted":true},"outputs":[{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["eval_result = trainer.predict(eval_dataset_1, max_new_tokens=20)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:20:12.152109Z","iopub.status.busy":"2024-04-17T16:20:12.151316Z","iopub.status.idle":"2024-04-17T16:20:12.157930Z","shell.execute_reply":"2024-04-17T16:20:12.156827Z","shell.execute_reply.started":"2024-04-17T16:20:12.152076Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[9.339613  6.566526  6.977488  9.339613  6.8662615 6.7352104 6.104882\n"," 6.8662615 9.339613  6.2845745 8.641244  6.459679  6.8199744 8.928269\n"," 9.270691  9.898782  9.339613  6.91693   8.992699  9.339613  9.339613\n"," 9.339613  9.474567  9.339613  6.243994  6.7588525]\n"]}],"source":["print(eval_result.predictions[0])"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:23:43.922804Z","iopub.status.busy":"2024-04-17T16:23:43.922052Z","iopub.status.idle":"2024-04-17T16:23:44.377958Z","shell.execute_reply":"2024-04-17T16:23:44.376043Z","shell.execute_reply.started":"2024-04-17T16:23:43.922767Z"},"trusted":true},"outputs":[{"ename":"TypeError","evalue":"argument 'ids': 'float' object cannot be interpreted as an integer","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mltokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3825\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3822\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3829\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3830\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:625\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    624\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 625\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    628\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    631\u001b[0m )\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n","\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'float' object cannot be interpreted as an integer"]}],"source":["test = ltokenizer.decode(eval_result.predictions[0], skip_special_tokens=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:13:33.125633Z","iopub.status.busy":"2024-04-17T16:13:33.124872Z"},"trusted":true},"outputs":[],"source":["text = ltokenizer.batch_decode(eval_result, skip_special_tokens=True)"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:56:14.086677Z","iopub.status.busy":"2024-04-17T16:56:14.085649Z","iopub.status.idle":"2024-04-17T16:56:14.092048Z","shell.execute_reply":"2024-04-17T16:56:14.091019Z","shell.execute_reply.started":"2024-04-17T16:56:14.086635Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Hi doctor, I am a 46 year old male. My weight is 74 kg and I am having an excellent physical shape. I think I have some sleep issue. I did not get proper sleep for any days and hence, I feel tired and irritable at times. I am very active professionally and leading very engaging life. I am not suffering from any depression or mental issues. I get a little bit of anxiety at times, but nothing serious that I can feel. What could be the reason for this? What are the remedies? Please suggest me.\n"]}],"source":["print(eval_dataset_1[0]['input'])"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:58:21.860469Z","iopub.status.busy":"2024-04-17T16:58:21.860042Z","iopub.status.idle":"2024-04-17T16:58:21.868697Z","shell.execute_reply":"2024-04-17T16:58:21.867712Z","shell.execute_reply.started":"2024-04-17T16:58:21.860437Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<|im_start|>system\n","You are a medical professional providing consultation and medical diagnostics.<|im_end|>\n","<|im_start|>user\n","a medical student is preparing for her final examination. Her patient has come to her asking: 'Hi doctor, I am a 46 year old male. My weight is 74 kg and I am having an excellent physical shape. I think I have some sleep issue. I did not get proper sleep for any days and hence, I feel tired and irritable at times. I am very active professionally and leading very engaging life. I am not suffering from any depression or mental issues. I get a little bit of anxiety at times, but nothing serious that I can feel. What could be the reason for this? What are the remedies? Please suggest me.'. Explain to the student the most likely cause/course of action.<|im_end|>\n","<|im_start|>assistant\n","\n"]}],"source":["prompt = f\"a medical student is preparing for her final examination. Her patient has come to her asking: '{eval_dataset_1[0]['input']}'. Explain to the student the most likely cause/course of action.\"\n","messages = [\n","    {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n","    {\"role\": \"user\", \"content\": prompt}\n","]\n","text = ltokenizer.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True\n",")\n","print(text)\n","model_inputs = ltokenizer(text, return_tensors=\"pt\").to(DEVICE)"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:57:13.157784Z","iopub.status.busy":"2024-04-17T16:57:13.156946Z","iopub.status.idle":"2024-04-17T16:57:18.276348Z","shell.execute_reply":"2024-04-17T16:57:18.275520Z","shell.execute_reply.started":"2024-04-17T16:57:13.157748Z"},"trusted":true},"outputs":[],"source":["generated_ids = model.generate(\n","    model_inputs.input_ids,\n","    max_new_tokens=256\n",")\n","generated_ids = [\n","    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","]\n","\n","response = ltokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T16:57:20.289440Z","iopub.status.busy":"2024-04-17T16:57:20.288645Z","iopub.status.idle":"2024-04-17T16:57:20.294269Z","shell.execute_reply":"2024-04-17T16:57:20.293279Z","shell.execute_reply.started":"2024-04-17T16:57:20.289406Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["at pulsing warmid legally legally legally isolatedlate fresh legally isolatedoso most that Willuriuate am to will will most to to to will will will will will will will will will will will will will will will will will will will will will will willlideiu am inlate most most to to will most most most to to to will most most most to to will most to to will most to to will most to to will most most most to to will most to to will most to to will most to to will most to to will most to to will most catlorlorlor reduid \" will itmenpi most most to to to to will they will most most to to to to will will they will will will reduidu to to to to\n"]}],"source":["print(response)"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T17:04:35.098944Z","iopub.status.busy":"2024-04-17T17:04:35.098515Z","iopub.status.idle":"2024-04-17T17:04:36.510284Z","shell.execute_reply":"2024-04-17T17:04:36.509041Z","shell.execute_reply.started":"2024-04-17T17:04:35.098910Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.5924,  0.1630,  0.6396,  ...,  0.7088, -0.3500, -0.0225],\n","         [-0.7186,  0.7437,  0.4885,  ...,  0.4310, -0.2094,  0.2431],\n","         [-0.4013,  0.2324,  0.3076,  ...,  0.1691, -0.4022,  0.0109],\n","         ...,\n","         [ 0.0802,  0.5385, -0.0017,  ..., -0.2361, -0.2471,  0.0284],\n","         [-0.0093,  0.0155, -0.0356,  ..., -0.0494, -0.1186, -0.0447],\n","         [-0.5201, -0.0494,  0.9739,  ...,  0.5900, -0.1954,  0.0582]]],\n","       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.3146, -0.2337, -0.2550,  ...,  0.2328,  0.0557,  0.1983]],\n","       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"]},{"ename":"TypeError","evalue":"argument 'ids': Can't extract `str` to `Vec`","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[81], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m encoded_output \u001b[38;5;241m=\u001b[39m mymodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_input)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m (encoded_output)\n\u001b[0;32m---> 10\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoded)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3785\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3762\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3763\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3766\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3767\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3768\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3769\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3783\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3784\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m   3786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m   3787\u001b[0m             seq,\n\u001b[1;32m   3788\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3789\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3790\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3791\u001b[0m         )\n\u001b[1;32m   3792\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3793\u001b[0m     ]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3786\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3762\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3763\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3766\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3767\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3768\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3769\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3783\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3784\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m-> 3786\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3790\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3793\u001b[0m     ]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3825\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3822\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3829\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3830\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:625\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    624\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 625\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    628\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    631\u001b[0m )\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n","\u001b[0;31mTypeError\u001b[0m: argument 'ids': Can't extract `str` to `Vec`"]}],"source":["from transformers import AutoModel, AutoTokenizer, AutoConfig\n","\n","tokenizer= AutoTokenizer.from_pretrained('UFNLP/gatortron-base')\n","config=AutoConfig.from_pretrained('UFNLP/gatortron-base')\n","mymodel=AutoModel.from_pretrained('UFNLP/gatortron-base')\n","\n","encoded_input=tokenizer(\"Bone scan:  Negative for distant metastasis.\", return_tensors=\"pt\")\n","encoded_output = mymodel(**encoded_input)\n","print (encoded_output)\n","encoded_output = [\n","    output_ids[len(encoded_input):] for encoded_input, output_ids in zip(model_inputs.input_ids, generated_ids)\n","]\n","decoded = tokenizer.batch_decode(encoded_output)\n","print(decoded)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1991302,"sourceId":3288731,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
