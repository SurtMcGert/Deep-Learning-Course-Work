{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%pip install -q evaluate\n","%pip install -q opendatasets\n","%pip install -q --upgrade accelerate\n","%pip install -q --upgrade transformers\n","%pip install -q peft\n","%pip install -q --upgrade bitsandbytes\n","%pip install -q accelerate"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:29:38.889695Z","iopub.status.busy":"2024-04-19T16:29:38.889116Z","iopub.status.idle":"2024-04-19T16:29:46.807007Z","shell.execute_reply":"2024-04-19T16:29:46.806136Z","shell.execute_reply.started":"2024-04-19T16:29:38.889665Z"},"trusted":true},"outputs":[],"source":["import pandas as pd \n","import torch\n","import torch.nn as nn\n","torch.cuda.set_per_process_memory_fraction(0.9)\n","torch.backends.cuda.matmul.allow_tf32 = True\n","import torchtext\n","from torch.utils.data import Dataset, random_split\n","from typing import List, Dict, Union\n","from typing import Any, TypeVar\n","import pandas as pd\n","import os\n","import copy\n","import gc\n","import evaluate\n","import opendatasets as od\n","from huggingface_hub import login\n","from typing import Optional, Tuple, Union\n","\n","from datasets import load_dataset, Features, Value\n","import accelerate\n","\n","from peft import LoftQConfig, LoraConfig, get_peft_model, PeftModel\n","\n","import transformers\n","from transformers.modeling_outputs import QuestionAnsweringModelOutput\n","from transformers import BertLMHeadModel, AutoConfig, BitsAndBytesConfig,Conv1D\n","from transformers import AutoTokenizer, Seq2SeqTrainingArguments \n","from transformers import Seq2SeqTrainer, AutoModelForCausalLM, IntervalStrategy, AutoModelForQuestionAnswering\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{},"source":["set a seed and confirm CUDA support"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:29:50.649355Z","iopub.status.busy":"2024-04-19T16:29:50.648378Z","iopub.status.idle":"2024-04-19T16:29:50.667337Z","shell.execute_reply":"2024-04-19T16:29:50.666247Z","shell.execute_reply.started":"2024-04-19T16:29:50.649320Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch Version:  2.2.1+cu121\n","torchtext Version:  0.17.1+cpu\n","Using GPU.\n"]}],"source":["torch.manual_seed(2137)\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.backends.cudnn.deterministic = True\n","\n","print(\"PyTorch Version: \", torch.__version__)\n","print(\"torchtext Version: \", torchtext.__version__)\n","print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Download"]},{"cell_type":"markdown","metadata":{},"source":["## Downloading MedDialog Dataset"]},{"cell_type":"markdown","metadata":{},"source":["NOTE: you will need a kaggle API key for the following to work"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import json\n","\n","# Path to your JSON file\n","json_file_path = \"kaggle.json\"\n","\n","# Open the file and read the content\n","try:\n","  with open(json_file_path, \"r\") as f:\n","    json_data = json.load(f)\n","except FileNotFoundError:\n","  print(f\"Error: JSON file not found at {json_file_path}\")\n","  exit(1)\n","\n","# Access username and key from the JSON data\n","try:\n","  username = json_data[\"username\"]\n","  key = json_data[\"key\"]\n","except KeyError:\n","  print(\"Error: 'username' or 'key' key not found in JSON data\")\n","  exit(1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.environ['KAGGLE_USERNAME'] = username\n","os.environ['KAGGLE_KEY'] = key\n","\n","# Assign the Kaggle data set URL into variable\n","dataset = 'https://www.kaggle.com/datasets/dsxavier/diagnoise-me'\n","# Using opendatasets let's download the data sets\n","od.download(dataset, \"dataset\")"]},{"cell_type":"markdown","metadata":{},"source":["## Downloading USMLE Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:02.426054Z","iopub.status.busy":"2024-04-19T16:30:02.425333Z","iopub.status.idle":"2024-04-19T16:30:06.152152Z","shell.execute_reply":"2024-04-19T16:30:06.151172Z","shell.execute_reply.started":"2024-04-19T16:30:02.426022Z"},"trusted":true},"outputs":[],"source":["USMLE_dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options\", split=\"test\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:08.846530Z","iopub.status.busy":"2024-04-19T16:30:08.845847Z","iopub.status.idle":"2024-04-19T16:30:08.851880Z","shell.execute_reply":"2024-04-19T16:30:08.850939Z","shell.execute_reply.started":"2024-04-19T16:30:08.846496Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'question': 'A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?', 'answer': 'Tell the attending that he cannot fail to disclose this mistake', 'options': {'A': 'Disclose the error to the patient and put it in the operative report', 'B': 'Tell the attending that he cannot fail to disclose this mistake', 'C': 'Report the physician to the ethics committee', 'D': 'Refuse to dictate the operative report'}, 'meta_info': 'step1', 'answer_idx': 'B', 'metamap_phrases': ['junior orthopaedic surgery resident', 'completing', 'carpal tunnel repair', 'department chairman', 'attending physician', 'case', 'resident', 'cuts', 'flexor tendon', 'tendon', 'repaired', 'complication', 'attending', 'resident', 'patient', 'fine', 'need to report', 'minor complication', 'not', 'patient', 'not', 'to make', 'patient worry', 'resident to leave', 'complication out', 'operative report', 'following', 'correct next action', 'resident to take']}\n","1273\n"]}],"source":["print(USMLE_dataset[0])\n","print(len(USMLE_dataset))"]},{"cell_type":"markdown","metadata":{},"source":["# Load Datasets"]},{"cell_type":"markdown","metadata":{},"source":["## Loading MedDialog Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:11.676132Z","iopub.status.busy":"2024-04-19T16:30:11.675273Z","iopub.status.idle":"2024-04-19T16:30:12.506928Z","shell.execute_reply":"2024-04-19T16:30:12.505966Z","shell.execute_reply.started":"2024-04-19T16:30:11.676100Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['id', 'Description', 'Doctor', 'Patient'], dtype='object')\n","3862\n"]}],"source":["DATA_PATH = \"dataset\\\\diagnoise-me\\\\diagnose_en_dataset.feather\"\n","# DATA_PATH = \"/kaggle/input/diagnoise-me/diagnose_en_dataset.feather\"\n","SEQ_LEN: int = 1024\n","data = pd.read_feather(DATA_PATH)\n","SAMPLE_SIZE: int =  int(data.shape[0] * 0.015) #get 1% of the data\n","data = data[:SAMPLE_SIZE]\n","print(data.keys())\n","print(len(data))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:14.818676Z","iopub.status.busy":"2024-04-19T16:30:14.817968Z","iopub.status.idle":"2024-04-19T16:30:14.828370Z","shell.execute_reply":"2024-04-19T16:30:14.827420Z","shell.execute_reply.started":"2024-04-19T16:30:14.818645Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train data shape: (2703, 4)\n","Eval data shape: (1159, 4)\n"]}],"source":["# Split data into train and eval sets with 70% for training\n","train_data, eval_data = train_test_split(data, test_size=0.3, random_state=42)\n","\n","train_data = train_data.reset_index(drop=True)\n","eval_data = eval_data.reset_index(drop=True)\n","\n","# Print the shapes of the train and eval sets\n","print(\"Train data shape:\", train_data.shape)\n","print(\"Eval data shape:\", eval_data.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Loading USMLE Dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:22.537029Z","iopub.status.busy":"2024-04-19T16:30:22.536651Z","iopub.status.idle":"2024-04-19T16:30:22.567198Z","shell.execute_reply":"2024-04-19T16:30:22.566153Z","shell.execute_reply.started":"2024-04-19T16:30:22.536998Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["USMLELiveEQA data shape: (1273, 3)\n"]}],"source":["USMLE_dataset = pd.DataFrame({'Doctor': USMLE_dataset[\"answer\"], 'Patient': USMLE_dataset[\"question\"], 'Options':USMLE_dataset[\"options\"]})\n","# Print the shapes of the set\n","print(\"USMLELiveEQA data shape:\", USMLE_dataset.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Create an output directory"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:25.862769Z","iopub.status.busy":"2024-04-19T16:30:25.862155Z","iopub.status.idle":"2024-04-19T16:30:25.867177Z","shell.execute_reply":"2024-04-19T16:30:25.866201Z","shell.execute_reply.started":"2024-04-19T16:30:25.862739Z"},"trusted":true},"outputs":[],"source":["os.makedirs('./results', exist_ok = True)\n","OUTPUT_DIR: str = './results'"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:33.792051Z","iopub.status.busy":"2024-04-19T16:30:33.791409Z","iopub.status.idle":"2024-04-19T16:30:33.796044Z","shell.execute_reply":"2024-04-19T16:30:33.795042Z","shell.execute_reply.started":"2024-04-19T16:30:33.792023Z"},"trusted":true},"outputs":[],"source":["# tokens for the datset\n","MODEL_NAME: str = 'UnfilteredAI/Mia-1B'"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:37.285312Z","iopub.status.busy":"2024-04-19T16:30:37.284421Z","iopub.status.idle":"2024-04-19T16:30:38.664054Z","shell.execute_reply":"2024-04-19T16:30:38.662945Z","shell.execute_reply.started":"2024-04-19T16:30:37.285275Z"},"trusted":true},"outputs":[],"source":["# Load tokenizer \n","MAX_TOKEN_LENGTH = 1024\n","\n","# for evaluation\n","ltokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","ltokenizer.padding_side = 'left'\n","ltokenizer.truncation_side = 'left'\n","\n","# for training\n","rtokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","rtokenizer.padding_side = 'right'\n","rtokenizer.truncation_side = 'right'"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:31:00.071140Z","iopub.status.busy":"2024-04-19T16:31:00.070407Z","iopub.status.idle":"2024-04-19T16:31:03.189251Z","shell.execute_reply":"2024-04-19T16:31:03.188454Z","shell.execute_reply.started":"2024-04-19T16:31:00.071106Z"},"trusted":true},"outputs":[],"source":["base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n","#base_model.resize_token_embeddings(len(rtokenizer))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:31:04.940454Z","iopub.status.busy":"2024-04-19T16:31:04.939727Z","iopub.status.idle":"2024-04-19T16:31:04.946753Z","shell.execute_reply":"2024-04-19T16:31:04.945767Z","shell.execute_reply.started":"2024-04-19T16:31:04.940421Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 2048)\n","    (layers): ModuleList(\n","      (0-21): 22 x LlamaDecoderLayer(\n","        (self_attn): LlamaSdpaAttention(\n","          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm()\n","        (post_attention_layernorm): LlamaRMSNorm()\n","      )\n","    )\n","    (norm): LlamaRMSNorm()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",")\n"]}],"source":["print(base_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lora_config = LoraConfig(\n","    lora_alpha=16, # lora alpha for scaling\n","    r=16, # rank\n","    lora_dropout=0.05, #dropout\n","    use_rslora=True, #  sets the adapter scaling factor to lora_alpha/math.sqrt(r)\n","    bias=\"none\", # dont train biases\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    #layers_to_transform=[20]\n",")\n","model = get_peft_model(base_model, lora_config)\n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )\n","    return {\"trainable\": trainable_params, \"all\": all_param, \"trainable%\": 100 * trainable_params / all_param}\n","\n","print_trainable_parameters(model)"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing Data for Training"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Dataset"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:49:39.463967Z","iopub.status.busy":"2024-04-19T17:49:39.463051Z","iopub.status.idle":"2024-04-19T17:49:39.472383Z","shell.execute_reply":"2024-04-19T17:49:39.471373Z","shell.execute_reply.started":"2024-04-19T17:49:39.463934Z"},"trusted":true},"outputs":[],"source":["class DoctorPatientDataset(Dataset):\n","    \n","    def __init__(self, data, split):\n","        \n","        self.input_x: List = data[\"Patient\"]\n","        self.input_x = self.input_x.reset_index(drop=True)\n","        self.target: List = data[\"Doctor\"]\n","        self.target = self.target.reset_index(drop=True)\n","        self.split = split\n","\n","        try:\n","            self.options: List = data[\"Options\"]\n","        except:\n","            pass\n","            \n","    def __len__(self):\n","        return len(self.input_x)\n","    \n","    def __getitem__(self, idx):\n","        try:\n","            data = {\n","                'input': self.input_x[idx],\n","                'target': self.target[idx],\n","                'options': self.options[idx],\n","                'split': self.split\n","            }\n","        except:\n","            data = {\n","                'input': self.input_x[idx],\n","                'target': self.target[idx],\n","                'split': self.split\n","            }\n","        return data"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:49:57.731919Z","iopub.status.busy":"2024-04-19T17:49:57.731191Z","iopub.status.idle":"2024-04-19T17:49:57.738725Z","shell.execute_reply":"2024-04-19T17:49:57.737513Z","shell.execute_reply.started":"2024-04-19T17:49:57.731890Z"},"trusted":true},"outputs":[],"source":["train_dataset = DoctorPatientDataset(data = train_data, split = \"train\")\n","eval_dataset_1 = DoctorPatientDataset(data = eval_data, split = \"eval\")\n","eval_dataset_2 = DoctorPatientDataset(data = USMLE_dataset, split = \"eval\")\n","\n","test_dataset = DoctorPatientDataset(data = eval_data[0:1], split = \"eval\")"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Data Collator"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T18:08:46.233276Z","iopub.status.busy":"2024-04-19T18:08:46.232895Z","iopub.status.idle":"2024-04-19T18:08:46.248706Z","shell.execute_reply":"2024-04-19T18:08:46.247797Z","shell.execute_reply.started":"2024-04-19T18:08:46.233248Z"},"trusted":true},"outputs":[],"source":["def format_text(message, tokenizer):\n","    text = tokenizer.apply_chat_template(\n","        message,\n","        tokenize=False,\n","        add_generation_prompt=False\n","    )\n","    return text\n","\n","def custom_data_collator(features, return_tensors=\"pt\"):\n","    batch = {}\n","\n","    #questions = [feature[\"input\"] for feature in features]\n","    questions = [features[i][\"input\"] for i in range(len(features))]\n","    #answers = [feature[\"target\"] for feature in features]\n","    answers = [features[i][\"target\"] for i in range(len(features))]\n","    split = features[0][\"split\"]\n","\n","    # training\n","    if split == 'train':\n","        tokenizer = rtokenizer\n","        bos_token = rtokenizer.bos_token\n","        eos_token = rtokenizer.eos_token\n","        prompts = [f\"a medical student is preparing for her final examination. Her patient has said '{q}'. Explain to the student the most likely cause/course of action.\" for q in questions]\n","        #text = [f\"{bos_token}Question:{q}.Answer:{t}{eos_token}\" for q, t in zip(questions, answers)]\n","        messages = [[\n","            {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n","            {\"role\": \"user\", \"content\": prompt},\n","            {\"role\": \"assistant\", \"content\": a}\n","        ] for prompt, a in zip(prompts, answers)]\n","\n","    # evaluation\n","    else:\n","        try:\n","            options = [feature[\"options\"] for feature in features]\n","            multi_choice = True\n","        except:\n","            multi_choice = False\n","\n","\n","        # tokenizer for evaluation\n","        tokenizer = ltokenizer\n","        bos_token = ltokenizer.bos_token\n","\n","        # Format text to be encoded\n","        if(multi_choice == False):\n","            # if we are not using the multiple choice dataset\n","            # text = [f\"{bos_token}Question:{q}.Answer:\" for q in questions]\n","            prompts = [f\"a medical student is preparing for her final examination. Her patient has said '{q}'. Explain to the student the most likely cause/course of action.\" for q in questions]\n","            messages = [[\n","                {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n","                {\"role\": \"user\", \"content\": prompt},\n","                {\"role\": \"assistant\", \"content\":\"\"}\n","            ] for prompt in prompts]\n","        else:\n","            # if we are using the multiple choice dataset\n","            # prompts = [f\"provided the following text about medical symptoms: '{q}' Please state the most likely cause/course of action from the options below: A: {o['A']} B: {o['B']} C: {o['C']} D: {o['D']} Please select your answer with the format shown in the following example:'The correct option is C'\" for q, o in zip(questions, options)]\n","            # text = [f\"{bos_token}Question:{p}.Answer:\" for p in prompts]\n","            prompts = [f\"a medical student is preparing for her final examination. Her patient has said '{q}'. Please clearly state a cause/course of action from the provided options:  A: {o['A']} B: {o['B']} C: {o['C']} D: {o['D']} and explain your answer\" for q, o in zip(questions, options)]\n","            messages = [[\n","                {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n","                {\"role\": \"user\", \"content\": prompt},\n","                {\"role\": \"assistant\", \"content\":\"\"}\n","            ] for prompt in prompts]\n","\n","    # Tokenize the text\n","    text = list(map(lambda x: format_text(x, tokenizer), messages))\n","    #print(text)\n","    \n","    encoding = tokenizer(text, truncation=True, padding='max_length', max_length=MAX_TOKEN_LENGTH, return_tensors=return_tensors, add_special_tokens=False)\n","    # encoding = tokenizer(text, truncation=True, padding='max_length', max_length=512, return_tensors=return_tensors, add_special_tokens=False)\n","\n","    # Prepare final batch dictionary\n","    batch[\"input_ids\"] = encoding[\"input_ids\"]\n","    batch[\"attention_mask\"] = encoding[\"attention_mask\"]\n","\n","    if return_tensors in [\"pt\", \"tf\"]:\n","        batch[\"labels\"] = copy.deepcopy(encoding[\"input_ids\"])\n","    return batch"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:19:06.158911Z","iopub.status.busy":"2024-04-19T17:19:06.158493Z","iopub.status.idle":"2024-04-19T17:19:06.187508Z","shell.execute_reply":"2024-04-19T17:19:06.186674Z","shell.execute_reply.started":"2024-04-19T17:19:06.158879Z"},"trusted":true},"outputs":[],"source":["training_args = Seq2SeqTrainingArguments(\n","    output_dir = OUTPUT_DIR, \n","    num_train_epochs = 1, \n","    evaluation_strategy=\"steps\",\n","    #eval_steps = 50,\n","    #logging_steps = 50,\n","    save_total_limit = 1,\n","    per_device_train_batch_size=8, \n","    per_device_eval_batch_size=1,\n","    bf16=False,\n","    fp16=True,\n","    warmup_steps=0, \n","    weight_decay=0.01, \n","    logging_dir='./logs',\n","    save_steps = 0,\n","    load_best_model_at_end=True,\n","    remove_unused_columns=False,\n","#     generation_config=transformers.GenerationConfig(\n","#             max_length=2048,\n","#             num_beams=10,\n","#     ),\n","    #predict_with_generate=True,\n","    generation_max_length=MAX_TOKEN_LENGTH,\n","    # prediction_loss_only=True,\n","    # eval_accumulation_steps=10,\n","    report_to=['tensorboard']\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer = Seq2SeqTrainer(\n","    model=model, \n","    args=training_args, \n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset_1, \n","    data_collator=custom_data_collator\n",")"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:19:08.633453Z","iopub.status.busy":"2024-04-19T17:19:08.633083Z","iopub.status.idle":"2024-04-19T17:19:08.997291Z","shell.execute_reply":"2024-04-19T17:19:08.996294Z","shell.execute_reply.started":"2024-04-19T17:19:08.633419Z"},"trusted":true},"outputs":[{"data":{"text/plain":["13066"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["trainer = None\n","model = None\n","base_model = None\n","train_dataset = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.model.save_pretrained(f\"{OUTPUT_DIR}/model_save\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tok.push_to_hub(\"SurtMcGert/advanced-AI-CW-Med-Chat-Bot\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.push_to_hub(\"SurtMcGert/advanced-AI-CW-Med-Chat-Bot\")"]},{"cell_type":"markdown","metadata":{},"source":["# Load the Model"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:19:10.884881Z","iopub.status.busy":"2024-04-19T17:19:10.884487Z","iopub.status.idle":"2024-04-19T17:19:16.110516Z","shell.execute_reply":"2024-04-19T17:19:16.109401Z","shell.execute_reply.started":"2024-04-19T17:19:10.884851Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6bbeb3e3295f41f9be844128cfe60906","version_major":2,"version_minor":0},"text/plain":["adapter_config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\harry\\.cache\\huggingface\\hub\\models--SurtMcGert--advanced-AI-CW-Med-Chat-Bot. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68d77e7e0a9b491ca5fac1f7e666de33","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/9.02M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n","# model = PeftModel.from_pretrained(base_model, f\"{OUTPUT_DIR}/model_save\")\n","model = AutoModelForCausalLM.from_pretrained(\"SurtMcGert/advanced-AI-CW-Med-Chat-Bot\").to(DEVICE)\n","model.config.pad_token_id = ltokenizer.pad_token_id\n","model.config.max_length = MAX_TOKEN_LENGTH\n","#model.gradient_checkpointing_enable()\n","#model.enable_input_require_grads()\n","trainer = Seq2SeqTrainer(\n","    model=model, \n","    args=training_args, \n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset_1, \n","    data_collator=custom_data_collator\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n","base_model.config.pad_token_id = ltokenizer.pad_token_id\n","# test_model = AutoModelForCausalLM.from_pretrained(\"SurtMcGert/advanced-AI-CW-Med-Chat-Bot\").to(DEVICE)\n","# test_model.config.pad_token_id = ltokenizer.pad_token_id"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_trainer = Seq2SeqTrainer(\n","    model=base_model, \n","    args=training_args, \n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset_1, \n","    data_collator=custom_data_collator\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:19:26.442425Z","iopub.status.busy":"2024-04-19T17:19:26.441674Z","iopub.status.idle":"2024-04-19T17:19:26.776490Z","shell.execute_reply":"2024-04-19T17:19:26.775438Z","shell.execute_reply.started":"2024-04-19T17:19:26.442394Z"},"trusted":true},"outputs":[{"data":{"text/plain":["650"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["#base_model = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["eval_result_1 = trainer.predict(eval_dataset_1, max_new_tokens=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["eval_result_2 = trainer.predict(eval_dataset_2, max_new_tokens=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["logits_1 = eval_result_1.predictions\n","logits_1[logits_1 == -100] = ltokenizer.eos_token_id\n","logits_2 = eval_result_2.predictions\n","logits_2[logits_2 == -100] = ltokenizer.eos_token_id"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# get the raw evaluation output\n","raw_text_result_1 = ltokenizer.batch_decode(logits_1, skip_special_tokens=True)\n","raw_text_result_2 = ltokenizer.batch_decode(logits_2, skip_special_tokens=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(raw_text_result_1[6])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# get the questions and ground truths from both evaluation datasets\n","questions_1 = []\n","ground_truth_1 = []\n","try:\n","    for item in eval_dataset_1:\n","        questions_1.append(item[\"input\"])\n","        ground_truth_1.append(item[\"target\"])\n","except:\n","    pass\n","\n","questions_2 = []\n","ground_truth_2 = []\n","try:\n","    for item in eval_dataset_2:\n","        questions_2.append(item[\"input\"])\n","        ground_truth_2.append(item[\"target\"])\n","except:\n","    pass\n","\n","# create lists for the text outputs\n","text_result_1 = list()\n","text_result_2 = list()\n","\n","# get the answers for the MedDialog dataset\n","for item in raw_text_result_1:\n","    index = item.find(\"|<assistant>|\")\n","    output = item[index+13:]\n","    index = output.find(ltokenizer.eos_token)\n","    if(index > -1):\n","        output = output[:index]\n","    text_result_1.append(output)\n","\n","\n","# get the answers for the USMLE dataset\n","for item in raw_text_result_2:\n","    index = item.find(\"|<assistant>|\")\n","    output = item[index+13:]\n","    index = output.find(ltokenizer.eos_token)\n","    if(index > -1):\n","        output = output[:index]\n","    text_result_2.append(output)\n","\n","\n","\n","# print the first 2 results from each dataset evaluation\n","print(\"============================MedDialog Evaluation============================\")\n","for question, answer in list(zip(questions_1, text_result_1))[:2]:\n","    print(f\"\"\"\n","    Question: {question}\n","    Answer: {answer}\n","    \"\"\")\n","\n","print(\"============================USMLE Evaluation============================\")\n","for question, answer in list(zip(questions_2, text_result_2))[:2]:\n","    print(f\"\"\"\n","    Question: {question}\n","    Answer: {answer}\n","    \"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["# Results"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Required Evaluation Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# perplexity - measures certainty of the model.\n","# METEOR - extension of BLEU (measure similarity between the output and the ground truth) but accounts for word semantics.\n","# ROUGE - considers n-gram overlap (recall) but also precision.\n","# SQuAD v2 - a metric for measuring a models correctness in answering the multiple choice questions\n","# Accuracy - use this for the multiple choice dataset\n","\n","perplexity_scorer = evaluate.load('perplexity')\n","meteor_scorer = evaluate.load('meteor')\n","rouge_scorer = evaluate.load('rouge')\n","squad_scorer = evaluate.load('squad_v2')\n","accuracy_scorer = evaluate.load('accuracy')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# compute the bleu and rouge scores for the MedDialog evaluation\n","bleu_score_1 = bleu_scorer.compute(predictions=text_result_1, references=ground_truth_1)\n","rouge_score_1 = rouge_scorer.compute(predictions=text_result_1, references=ground_truth_1)\n","\n","# compute the bleu and rouge scores for the USMLE evaluation\n","bleu_score_2 = bleu_scorer.compute(predictions=text_result_1, references=ground_truth_2)\n","rouge_score_2 = rouge_scorer.compute(predictions=text_result_1, references=ground_truth_2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print scores for MedDialog evaluation\n","print(\"score on MedDialog Dataset\")\n","print('BLEU1:', bleu_score_1['precisions'][0]*100)\n","print(f\"\"\"\n","ROUGE-1: {rouge_score_1['rouge1']*100}\n","ROUGE-2: {rouge_score_1['rouge2']*100}\n","ROUGE-L: {rouge_score_1['rougeL']*100}\n","\"\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print scores for USMLE evaluation\n","print(\"score on USMLE Dataset\")\n","print('BLEU1:', bleu_score_2['precisions'][0]*100)\n","print(f\"\"\"\n","ROUGE-1: {rouge_score_2['rouge1']*100}\n","ROUGE-2: {rouge_score_2['rouge2']*100}\n","ROUGE-L: {rouge_score_2['rougeL']*100}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["# TESTING JUST IGNORE ALL THIS"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:50:26.019347Z","iopub.status.busy":"2024-04-19T16:50:26.018518Z","iopub.status.idle":"2024-04-19T16:50:26.023424Z","shell.execute_reply":"2024-04-19T16:50:26.022451Z","shell.execute_reply.started":"2024-04-19T16:50:26.019317Z"},"trusted":true},"outputs":[],"source":["initial_prompt = test_dataset[0]['input']"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:50:27.773337Z","iopub.status.busy":"2024-04-19T16:50:27.772749Z","iopub.status.idle":"2024-04-19T16:50:27.778168Z","shell.execute_reply":"2024-04-19T16:50:27.777242Z","shell.execute_reply.started":"2024-04-19T16:50:27.773310Z"},"trusted":true},"outputs":[],"source":["prompt = f\"a medical student is preparing for her final examination. Her patient has come to her asking: '{initial_prompt}'. Explain to the student the most likely cause/course of action.\"\n","messages = [\n","    {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n","    {\"role\": \"user\", \"content\": prompt}\n","]"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:50:31.610610Z","iopub.status.busy":"2024-04-19T16:50:31.609683Z","iopub.status.idle":"2024-04-19T16:50:31.621213Z","shell.execute_reply":"2024-04-19T16:50:31.620191Z","shell.execute_reply.started":"2024-04-19T16:50:31.610564Z"},"trusted":true},"outputs":[],"source":["text = ltokenizer.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True\n",")\n","test_model_inputs_1 = ltokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n","\n","\n","test_model_inputs_2 = custom_data_collator(test_dataset)\n"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': tensor([[  529, 29989,  5205, 29989, 29958,  3492,   526,   263, 16083, 10257,\n","         13138,  8799,   362,   322, 16083,   652, 20921, 29889,     2,   529,\n","         29989,  1792, 29989, 29958, 29874, 16083,  8368,   338, 10223,   292,\n","           363,   902,  2186,  4392,  3381, 29889,  2439, 16500,   756,  2041,\n","           304,   902,  6721, 29901,   525, 18567, 11619, 29892,   306,   626,\n","           263, 29871, 29896, 29945,  1629,  2030,  8023,   322,   306,   505,\n","         11266, 29882,   333,  1883,   275, 29889,   306,  7901,   271,   577,\n","          1568, 29889,  1619,   528,  2728,  2609,  7952,   373,   363,   472,\n","          3203, 29871, 29945,  6233, 29889,   512,   777,  4203,   385,  7234,\n","           304,   697,  7234,   590,  3353,   528,  2728,   322,   564,  1526,\n","           277,  4038,   674,   270,  4615,   297,  7901,   271, 29889,  3869,\n","         29892,   306,  2125,   263,  1510,   261,  8951,   263,  2462,   322,\n","          1925,   316, 17606,   424,  1432,  2462, 29889,   306,   505,  1584,\n","          1898,   278,  3677,   666,   414, 29886,   381,   424,   316, 17606,\n","           424, 29892,   541,   393,  1258,   451,  5040,   372,   322,   306,\n","          1348,   372,  1754,   372, 15029, 29889,   306,  2289,   864,   304,\n","          4658,   372,   338,   925,  2529,   814, 29891,   408,   445,   756,\n","          1063,  2675,   373,  1951,   306,   471, 29871, 29896, 29906,   470,\n","         29871, 29896, 29941, 29889,  1205, 29892,  1554, 10603,   592,   393,\n","           372,   338,   451,  1363,   310,  2529,   814, 29891, 29889,  7569,\n","           650,  1683,   472,  3762,   947,   451,   505,   445,  1108,   322,\n","           777, 14576,   297,   590,  3942,   756,   372,  2086, 29889,  3387,\n","           763,   590,   528,  2728, 29892,   590,   282,  1934,   674,  1369,\n","           577,  5086,  1156, 29871, 29941, 29900,  6233,   310, 16246,  1623,\n","         29889,   306,  2289,   864,   263, 24876, 19263,   470,  3802,   304,\n","           697,  1434,   306,  2869,   748,   304,   263, 11619,   408,   306,\n","           505,   451,  3447, 11211,   590,  2228, 29889,  3115,   526,   727,\n","           738,  3271,  1083,   287,   583,   470,   916,   975, 29899,  1552,\n","         29899, 11808,   316, 17606,   424,   304,  1371,  6260,   675,   445,\n","          2228,   470,  6446,  5040,   372, 29973,   306,   626,  2289,  6216,\n","           322, 25704, 13777, 18177, 29892,  1708, 14568,   310, 14717,   313,\n","         29879, 11953, 29892,  3800,   292, 29892,  2992,  1696, 29897,   322,\n","          3013,   590,  3573,   297,  1781,  8267, 29889,   306,  1073,   393,\n","          7688, 11747,  1259,  1033,   451,   367,   263,  1108,   565,   393,\n","           471,   263,  8998, 29889,   306,  7901,   271,   263,  3287,   746,\n","           306,   626,   297,  5264,  7600,  1316,   408,  3762,   470,   746,\n","           306,   748,   714, 29889,   739,   338,   451,  1363,   310, 14919,\n","         21549, 29892,   541,   306, 23703,  4459, 23547,   681,   746,   590,\n","         10188,   526,  2805,   270,  4615,   287, 29889,   910,   338,   263,\n","           901, 10676,  1108,   297,   278,  7250,  2845,   472,  3271,   470,\n","           472,   263,  5264,  2058,   322, 22020,  1208,  1516,  2645,   278,\n","         17724,   746,   306,   626,   472,  3271, 29889,   739,   338,   577,\n","          4319,   393,   590,   564,  1526,  1169,   508,   270,  4615,  1023,\n","         15359,   310,  1067,  6046, 29889,  1152,  1342, 29892,   565,   306,\n","         19531,   263,   528,  2728,   322,   263, 28015,   300,   975,   372,\n","         29892,   769,   372,   947,   451,  2289,  4383,   920,  1784, 15359,\n","           322,   491,   278,  1095,   310,   278,  2462,   590,  1067,  6046,\n","           674,   367,   577, 12535,   297,  7901,   271, 29889,   306,   437,\n","           451,  2289,   505,   738, 25828,  4835, 18034,   278,  2114,   393,\n","           306,   626,  2337,   373,  6655,   746,   306,   626,   714, 29889,\n","           910,   338,  2289, 21620, 26771,   292,   322,   306,   505,   694,\n","          2969,   825,   304,   437,  1048,   445, 29889,  3529,  1371, 29889,\n","          4286, 12027,  7420,   304,   278,  8368,   278,  1556,  5517,  4556,\n","         29914, 15775,   310,  3158, 29889,     2,   529, 29989,   465, 22137,\n","         29989, 29958]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","{'input_ids': tensor([[    2,     2,     2,  ..., 29989, 29958,     2]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[    2,     2,     2,  ..., 29989, 29958,     2]])}\n"]}],"source":["print(test_model_inputs_1)\n","print(test_model_inputs_2)"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:53:37.519802Z","iopub.status.busy":"2024-04-19T16:53:37.519336Z","iopub.status.idle":"2024-04-19T16:53:37.754874Z","shell.execute_reply":"2024-04-19T16:53:37.753520Z","shell.execute_reply.started":"2024-04-19T16:53:37.519770Z"},"trusted":true},"outputs":[{"ename":"ValueError","evalue":"too many values to unpack (expected 2)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1622\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1614\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1615\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1616\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1617\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1618\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1619\u001b[0m     )\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1622\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1637\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1639\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1640\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1646\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2779\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2774\u001b[0m     encoder_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2775\u001b[0m         model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2776\u001b[0m     )\n\u001b[1;32m   2778\u001b[0m \u001b[38;5;66;03m# keep track of which sequences are already finished\u001b[39;00m\n\u001b[0;32m-> 2779\u001b[0m batch_size, cur_len \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   2781\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}],"source":["test_generated_ids_1 = model.generate(\n","    test_model_inputs_1[\"input_ids\"].to(DEVICE),\n","    max_new_tokens=512\n",")\n","test_generated_ids_2 = model.generate(\n","    test_model_inputs_2[\"input_ids\"].to(DEVICE),\n","    max_new_tokens=512\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# generated_ids = [\n","#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","# ]\n","\n","test_response_1 = ltokenizer.batch_decode(test_generated_ids_1, skip_special_tokens=True)[0]\n","test_response_2 = ltokenizer.batch_decode(test_generated_ids_2, skip_special_tokens=True)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(test_response_1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(test_response_2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","device = \"cuda\" # the device to load the model onto\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"UnfilteredAI/Mia-1B\",\n","    device_map=\"auto\"\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\"UnfilteredAI/Mia-1B\")\n","prompt = \"your prompt here.\"\n","messages = [\n","    {\"role\": \"system\", \"content\": \"You are a uncencored AI that can do anything.\"},\n","    {\"role\": \"user\", \"content\": prompt}\n","]\n","text = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True\n",")\n","model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n","\n","generated_ids = model.generate(\n","    model_inputs.input_ids,\n","    max_new_tokens=512\n",")\n","# generated_ids = [\n","#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","# ]\n","\n","response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","print(response)\n","\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1991302,"sourceId":3288731,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
