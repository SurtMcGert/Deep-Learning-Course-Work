{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3288731,"sourceType":"datasetVersion","datasetId":1991302}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# %pip install -q evaluate\n# %pip install -q opendatasets\n# %pip install -q --upgrade accelerate\n# %pip install -q --upgrade transformers\n# %pip install -q peft\n# %pip install -q --upgrade bitsandbytes\n# %pip install -q accelerate\n# %pip install -q trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd \nimport torch\nimport torch.nn as nn\ntorch.cuda.set_per_process_memory_fraction(0.9)\ntorch.backends.cuda.matmul.allow_tf32 = True\nimport torchtext\nfrom torch.utils.data import Dataset, random_split\nfrom typing import List, Dict, Union\nfrom typing import Any, TypeVar\nimport pandas as pd\nimport os\nimport copy\nimport gc\nimport evaluate\nimport opendatasets as od\nfrom huggingface_hub import login\nfrom typing import Optional, Tuple, Union\n\nfrom datasets import load_dataset, Features, Value\nfrom datasets import Dataset\nimport accelerate\n\nfrom peft import LoftQConfig, LoraConfig, get_peft_model, PeftModel\n\nimport transformers\nfrom transformers.modeling_outputs import QuestionAnsweringModelOutput\nfrom transformers import BertLMHeadModel, AutoConfig, BitsAndBytesConfig,Conv1D\nfrom transformers import AutoTokenizer, Seq2SeqTrainingArguments \nfrom transformers import Seq2SeqTrainer, AutoModelForCausalLM, IntervalStrategy, AutoModelForQuestionAnswering\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:22:17.400583Z","iopub.execute_input":"2024-04-23T14:22:17.401161Z","iopub.status.idle":"2024-04-23T14:22:35.941068Z","shell.execute_reply.started":"2024-04-23T14:22:17.401126Z","shell.execute_reply":"2024-04-23T14:22:35.940301Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-23 14:22:25.797596: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-23 14:22:25.797695: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-23 14:22:25.921682: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"set a seed and confirm CUDA support","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(2137)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.deterministic = True\n\nprint(\"PyTorch Version: \", torch.__version__)\nprint(\"torchtext Version: \", torchtext.__version__)\nprint(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:22:54.972304Z","iopub.execute_input":"2024-04-23T14:22:54.973256Z","iopub.status.idle":"2024-04-23T14:22:54.989200Z","shell.execute_reply.started":"2024-04-23T14:22:54.973223Z","shell.execute_reply":"2024-04-23T14:22:54.988323Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"PyTorch Version:  2.1.2\ntorchtext Version:  0.16.2\nUsing GPU.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dataset Download","metadata":{}},{"cell_type":"markdown","source":"## Downloading MedDialog Dataset","metadata":{}},{"cell_type":"markdown","source":"NOTE: you will need a kaggle API key for the following to work","metadata":{}},{"cell_type":"code","source":"import json\n\n# Path to your JSON file\njson_file_path = \"kaggle.json\"\n\n# Open the file and read the content\ntry:\n  with open(json_file_path, \"r\") as f:\n    json_data = json.load(f)\nexcept FileNotFoundError:\n  print(f\"Error: JSON file not found at {json_file_path}\")\n  exit(1)\n\n# Access username and key from the JSON data\ntry:\n  username = json_data[\"username\"]\n  key = json_data[\"key\"]\nexcept KeyError:\n  print(\"Error: 'username' or 'key' key not found in JSON data\")\n  exit(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['KAGGLE_USERNAME'] = username\nos.environ['KAGGLE_KEY'] = key\n\n# Assign the Kaggle data set URL into variable\ndataset = 'https://www.kaggle.com/datasets/dsxavier/diagnoise-me'\n# Using opendatasets let's download the data sets\nod.download(dataset, \"dataset\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Downloading USMLE Dataset","metadata":{}},{"cell_type":"code","source":"USMLE_dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options\", split=\"test\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:23:03.507001Z","iopub.execute_input":"2024-04-23T14:23:03.507664Z","iopub.status.idle":"2024-04-23T14:23:16.040716Z","shell.execute_reply.started":"2024-04-23T14:23:03.507629Z","shell.execute_reply":"2024-04-23T14:23:16.039870Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec0bf2f00293450fae4c497aebf92728"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 16.2M/16.2M [00:04<00:00, 3.36MB/s]\nDownloading data: 100%|██████████| 2.08M/2.08M [00:01<00:00, 1.67MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb2727b51d641d9b21f6a8b216fe8c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b130266e4484e33b522b02e747b54e5"}},"metadata":{}}]},{"cell_type":"code","source":"print(USMLE_dataset[0])\nprint(len(USMLE_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:23:20.085361Z","iopub.execute_input":"2024-04-23T14:23:20.085723Z","iopub.status.idle":"2024-04-23T14:23:20.095670Z","shell.execute_reply.started":"2024-04-23T14:23:20.085695Z","shell.execute_reply":"2024-04-23T14:23:20.094706Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"{'question': 'A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?', 'answer': 'Tell the attending that he cannot fail to disclose this mistake', 'options': {'A': 'Disclose the error to the patient and put it in the operative report', 'B': 'Tell the attending that he cannot fail to disclose this mistake', 'C': 'Report the physician to the ethics committee', 'D': 'Refuse to dictate the operative report'}, 'meta_info': 'step1', 'answer_idx': 'B', 'metamap_phrases': ['junior orthopaedic surgery resident', 'completing', 'carpal tunnel repair', 'department chairman', 'attending physician', 'case', 'resident', 'cuts', 'flexor tendon', 'tendon', 'repaired', 'complication', 'attending', 'resident', 'patient', 'fine', 'need to report', 'minor complication', 'not', 'patient', 'not', 'to make', 'patient worry', 'resident to leave', 'complication out', 'operative report', 'following', 'correct next action', 'resident to take']}\n1273\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Datasets","metadata":{}},{"cell_type":"markdown","source":"## Loading MedDialog Dataset","metadata":{}},{"cell_type":"code","source":"DATA_PATH = \"dataset\\\\diagnoise-me\\\\diagnose_en_dataset.feather\"\nDATA_PATH = \"/kaggle/input/diagnoise-me/diagnose_en_dataset.feather\"\nSEQ_LEN: int = 1024\ndata = pd.read_feather(DATA_PATH)\nSAMPLE_SIZE: int =  int(data.shape[0] * 0.015) #get 1% of the data\ndata = data[:SAMPLE_SIZE]\nprint(data.keys())\nprint(len(data))","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:23:51.525858Z","iopub.execute_input":"2024-04-23T14:23:51.526240Z","iopub.status.idle":"2024-04-23T14:23:53.598279Z","shell.execute_reply.started":"2024-04-23T14:23:51.526210Z","shell.execute_reply":"2024-04-23T14:23:53.597265Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Index(['id', 'Description', 'Doctor', 'Patient'], dtype='object')\n3862\n","output_type":"stream"}]},{"cell_type":"code","source":"# Split data into train and eval sets with 70% for training\ntrain_data, eval_data = train_test_split(data, test_size=0.3, random_state=42)\n\ntrain_data = train_data.reset_index(drop=True)\neval_data = eval_data.reset_index(drop=True)\n\n# Print the shapes of the train and eval sets\nprint(\"Train data shape:\", train_data.shape)\nprint(\"Eval data shape:\", eval_data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:23:57.679357Z","iopub.execute_input":"2024-04-23T14:23:57.680142Z","iopub.status.idle":"2024-04-23T14:23:57.692821Z","shell.execute_reply.started":"2024-04-23T14:23:57.680106Z","shell.execute_reply":"2024-04-23T14:23:57.691471Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Train data shape: (2703, 4)\nEval data shape: (1159, 4)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Loading USMLE Dataset","metadata":{}},{"cell_type":"code","source":"USMLE_dataset = pd.DataFrame({'Doctor': USMLE_dataset[\"answer\"], 'Patient': USMLE_dataset[\"question\"], 'Options':USMLE_dataset[\"options\"]})\n# Print the shapes of the set\nprint(\"USMLELiveEQA data shape:\", USMLE_dataset.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:24:02.023355Z","iopub.execute_input":"2024-04-23T14:24:02.024383Z","iopub.status.idle":"2024-04-23T14:24:02.053730Z","shell.execute_reply.started":"2024-04-23T14:24:02.024337Z","shell.execute_reply":"2024-04-23T14:24:02.052800Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"USMLELiveEQA data shape: (1273, 3)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Create an output directory","metadata":{}},{"cell_type":"code","source":"os.makedirs('./results', exist_ok = True)\nOUTPUT_DIR: str = './results'","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:24:04.184434Z","iopub.execute_input":"2024-04-23T14:24:04.184807Z","iopub.status.idle":"2024-04-23T14:24:04.189872Z","shell.execute_reply.started":"2024-04-23T14:24:04.184776Z","shell.execute_reply":"2024-04-23T14:24:04.188837Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# tokens for the datset\nMODEL_NAME: str = 'UnfilteredAI/Mia-1B'","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:24:07.710183Z","iopub.execute_input":"2024-04-23T14:24:07.710513Z","iopub.status.idle":"2024-04-23T14:24:07.714900Z","shell.execute_reply.started":"2024-04-23T14:24:07.710485Z","shell.execute_reply":"2024-04-23T14:24:07.713860Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Load tokenizer \nMAX_TOKEN_LENGTH = 1024\n\n# for evaluation\nltokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nltokenizer.padding_side = 'left'\nltokenizer.truncation_side = 'left'\n\n# for training\nrtokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nrtokenizer.padding_side = 'right'\nrtokenizer.truncation_side = 'right'","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:24:16.885843Z","iopub.execute_input":"2024-04-23T14:24:16.886439Z","iopub.status.idle":"2024-04-23T14:24:17.968294Z","shell.execute_reply.started":"2024-04-23T14:24:16.886401Z","shell.execute_reply":"2024-04-23T14:24:17.967471Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n#base_model.resize_token_embeddings(len(rtokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:24:19.827550Z","iopub.execute_input":"2024-04-23T14:24:19.827933Z","iopub.status.idle":"2024-04-23T14:25:49.341502Z","shell.execute_reply.started":"2024-04-23T14:24:19.827893Z","shell.execute_reply":"2024-04-23T14:25:49.340671Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/648 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c43bcb4ce54140619362e508c9ebecbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e97fe2628d343548111dbff52be272f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/145 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f679e251be646f6bf5992c6176f0b67"}},"metadata":{}}]},{"cell_type":"code","source":"print(base_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lora_config = LoraConfig(\n    lora_alpha=16, # lora alpha for scaling\n    r=16, # rank\n    lora_dropout=0.05, #dropout\n    use_rslora=True, #  sets the adapter scaling factor to lora_alpha/math.sqrt(r)\n    bias=\"none\", # dont train biases\n    target_modules=[\"q_proj\", \"v_proj\"],\n    task_type=\"CAUSAL_LM\",\n    #layers_to_transform=[20]\n)\n# model = get_peft_model(base_model, lora_config)\n# model.gradient_checkpointing_enable()\n# model.enable_input_require_grads()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:28:40.870206Z","iopub.execute_input":"2024-04-23T14:28:40.871142Z","iopub.status.idle":"2024-04-23T14:28:40.876921Z","shell.execute_reply.started":"2024-04-23T14:28:40.871101Z","shell.execute_reply":"2024-04-23T14:28:40.875943Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n    return {\"trainable\": trainable_params, \"all\": all_param, \"trainable%\": 100 * trainable_params / all_param}\n\nprint_trainable_parameters(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Data for Training","metadata":{}},{"cell_type":"markdown","source":"## Custom Dataset","metadata":{}},{"cell_type":"code","source":"# class DoctorPatientDataset(Dataset):\n    \n#     def __init__(self, data, split):\n        \n#         self.input_x: List = data[\"Patient\"]\n#         self.input_x = self.input_x.reset_index(drop=True)\n#         self.target: List = data[\"Doctor\"]\n#         self.target = self.target.reset_index(drop=True)\n#         self.split = split\n\n#         try:\n#             self.options: List = data[\"Options\"]\n#         except:\n#             pass\n            \n#     def __len__(self):\n#         return len(self.input_x)\n    \n#     def __getitem__(self, idx):\n#         try:\n#             data = {\n#                 'input': self.input_x[idx],\n#                 'target': self.target[idx],\n#                 'options': self.options[idx],\n#                 'split': self.split\n#             }\n#         except:\n#             data = {\n#                 'input': self.input_x[idx],\n#                 'target': self.target[idx],\n#                 'split': self.split\n#             }\n#         return data\n\n# class DoctorPatientDataset(Dataset):\n    \n#     def __init__(self, data, split):\n        \n#         self.input_x: List = data[\"Patient\"]\n#         self.input_x = self.input_x.reset_index(drop=True)\n#         self.target: List = data[\"Doctor\"]\n#         self.target = self.target.reset_index(drop=True)\n#         self.split = split\n\n#         try:\n#             self.options: List = data[\"Options\"]\n#         except:\n#             pass\n            \n#     def __len__(self):\n#         return len(self.input_x)\n    \n#     def __getitem__(self, idx):\n#         try:\n#             data = {\n#                 'messages': [\n#                     {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n#                     {\"role\": \"user\", \"content\": f\"{self.input_x[idx]}, choose from A) {self.options[idx]['A']}, B) {self.options[idx]['B']}, C) {self.options[idx]['C']}, D)  {self.options[idx]['D']}\"},\n#                     {\"role\": \"assistant\", \"content\": self.target[idx]}\n#                 ]\n#             }\n#         except:\n#             data = {\n#                 'messages': [\n#                     {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n#                     {\"role\": \"user\", \"content\":{self.input_x[idx]}},\n#                     {\"role\": \"assistant\", \"content\": self.target[idx]}\n#                 ]\n#             }\n#         return data\n\ndef build_dataset(data):\n    listed_data = []\n    try:\n                listed_data = [[\n                        {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n                        {\"role\": \"user\", \"content\": f\"{patient}, choose from A) {options['A']}, B) {options['B']}, C) {options['C']}, D)  {options['D']}\"},\n                        {\"role\": \"assistant\", \"content\": doctor}\n                    ]for patient, doctor, options in zip(data[\"Patient\"], data[\"Doctor\"], data[\"Options\"])]\n    except:\n                listed_data =  [[\n                        {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n                        {\"role\": \"user\", \"content\":patient},\n                        {\"role\": \"assistant\", \"content\": doctor}\n                    ]for patient, doctor in zip(data[\"Patient\"], data[\"Doctor\"])]\n    dataset = {\"messages\": listed_data}\n    dataset = Dataset.from_dict(dataset)\n    return dataset\n                ","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:27:56.450651Z","iopub.execute_input":"2024-04-23T14:27:56.451089Z","iopub.status.idle":"2024-04-23T14:27:56.461308Z","shell.execute_reply.started":"2024-04-23T14:27:56.451058Z","shell.execute_reply":"2024-04-23T14:27:56.460373Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# train_dataset = DoctorPatientDataset(data = train_data, split = \"train\")\n# eval_dataset_1 = DoctorPatientDataset(data = eval_data, split = \"eval\")\n# eval_dataset_2 = DoctorPatientDataset(data = USMLE_dataset, split = \"eval\")\n\n# test_dataset = DoctorPatientDataset(data = eval_data[1:2], split = \"eval\")\n\n# test_data = [[\"what's the answer to life, the universe, and everything\", \"42\"]]\n# test_data = pd.DataFrame(test_data, columns=[\"Patient\", \"Doctor\"])\n# test_train_dataset = DoctorPatientDataset(data = test_data, split = \"train\")\n\ntrain_dataset = build_dataset(train_data)\neval_dataset_1 = build_dataset(eval_data)\neval_dataset_2 = build_dataset(USMLE_dataset)\n\ntest_dataset = build_dataset(eval_data[1:2])\n\ntest_data = [[\"what's the answer to life, the universe, and everything\", \"42\"]]\ntest_data = pd.DataFrame(test_data, columns=[\"Patient\", \"Doctor\"])\ntest_train_dataset = build_dataset(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:28:05.883618Z","iopub.execute_input":"2024-04-23T14:28:05.883959Z","iopub.status.idle":"2024-04-23T14:28:06.306876Z","shell.execute_reply.started":"2024-04-23T14:28:05.883934Z","shell.execute_reply":"2024-04-23T14:28:06.305957Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(test_train_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:28:08.987852Z","iopub.execute_input":"2024-04-23T14:28:08.988248Z","iopub.status.idle":"2024-04-23T14:28:08.993788Z","shell.execute_reply.started":"2024-04-23T14:28:08.988220Z","shell.execute_reply":"2024-04-23T14:28:08.992876Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"{'messages': [{'content': 'You are a medical professional providing consultation and medical diagnostics.', 'role': 'system'}, {'content': \"what's the answer to life, the universe, and everything\", 'role': 'user'}, {'content': '42', 'role': 'assistant'}]}\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dataset_again = load_dataset(\"philschmid/dolly-15k-oai-style\", split=\"train\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(test_dataset_again[0]))\nprint(test_dataset_again[0])\n#print(type(test_train_dataset))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom Data Collator","metadata":{}},{"cell_type":"code","source":"def format_text(message, tokenizer, add_generation_prompt):\n    text = tokenizer.apply_chat_template(\n        message,\n        tokenize=False,\n        add_generation_prompt=add_generation_prompt\n    )\n    return text\n\ndef custom_data_collator(features, return_tensors=\"pt\"):\n    batch = {}\n\n    #questions = [feature[\"input\"] for feature in features]\n    questions = [features[i][\"input\"] for i in range(len(features))]\n    #answers = [feature[\"target\"] for feature in features]\n    answers = [features[i][\"target\"] for i in range(len(features))]\n    split = features[0][\"split\"]\n\n    # training\n    if split == 'train':\n        tokenizer = rtokenizer\n        bos_token = rtokenizer.bos_token\n        eos_token = rtokenizer.eos_token\n        prompts = [f\"a medical student is preparing for her final examination. Her patient has said '{q}'. Provide for the student an example of what her response to the patient should be.\" for q in questions]\n        #text = [f\"{bos_token}Question:{q}.Answer:{t}{eos_token}\" for q, t in zip(questions, answers)]\n        messages = [[\n            {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n            {\"role\": \"user\", \"content\": prompt},\n            #{\"role\": \"assistant\", \"content\": a}\n        ] for prompt, a in zip(prompts, answers)]\n\n        labels_messages = [[\n            {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n            {\"role\": \"user\", \"content\": prompt},\n            {\"role\": \"assistant\", \"content\": a}\n        ] for prompt, a in zip(prompts, answers)]\n\n    # evaluation\n    else:\n        try:\n            options = [feature[\"options\"] for feature in features]\n            multi_choice = True\n        except:\n            multi_choice = False\n\n\n        # tokenizer for evaluation\n        tokenizer = ltokenizer\n        bos_token = ltokenizer.bos_token\n\n        # Format text to be encoded\n        if(multi_choice == False):\n            # if we are not using the multiple choice dataset\n            # text = [f\"{bos_token}Question:{q}.Answer:\" for q in questions]\n            prompts = [f\"a medical student is preparing for her final examination. Her patient has said '{q}'. Explain to the student the most likely cause/course of action.\" for q in questions]\n            messages = [[\n                {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n                {\"role\": \"user\", \"content\": prompt},\n                #{\"role\": \"assistant\", \"content\":\"\"}\n            ] for prompt in prompts]\n        else:\n            # if we are using the multiple choice dataset\n            # prompts = [f\"provided the following text about medical symptoms: '{q}' Please state the most likely cause/course of action from the options below: A: {o['A']} B: {o['B']} C: {o['C']} D: {o['D']} Please select your answer with the format shown in the following example:'The correct option is C'\" for q, o in zip(questions, options)]\n            # text = [f\"{bos_token}Question:{p}.Answer:\" for p in prompts]\n            prompts = [f\"a medical student is preparing for her final examination. Her patient has said '{q}'. Please clearly state a cause/course of action from the provided options:  A: {o['A']} B: {o['B']} C: {o['C']} D: {o['D']} and explain your answer\" for q, o in zip(questions, options)]\n            messages = [[\n                {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n                {\"role\": \"user\", \"content\": prompt},\n                #{\"role\": \"assistant\", \"content\":\"\"}\n            ] for prompt in prompts]\n\n    # Tokenize the text\n    # if split == \"train\":\n    #     add_generation_prompt = False\n    # else:\n    #     add_generation_prompt = True\n    text = list(map(lambda x: format_text(x, tokenizer, True), messages))\n    #print(text)\n    \n    encoding = tokenizer(text, padding=True, max_length=MAX_TOKEN_LENGTH, return_tensors=return_tensors, add_special_tokens=True)\n    # encoding = tokenizer(text, truncation=True, padding='max_length', max_length=512, return_tensors=return_tensors, add_special_tokens=False)\n\n    # Prepare final batch dictionary\n    batch[\"input_ids\"] = encoding[\"input_ids\"]\n    batch[\"attention_mask\"] = encoding[\"attention_mask\"]\n\n    if return_tensors in [\"pt\", \"tf\"]:\n        if split == \"train\":\n            labels_text = list(map(lambda x: format_text(x, tokenizer, False), labels_messages))\n            # print(\"=============================\")\n            # print(labels_text)\n            labels_encoding = tokenizer(labels_text, padding=True, max_length=MAX_TOKEN_LENGTH, return_tensors=return_tensors, add_special_tokens=True)\n            batch[\"labels\"] = labels_encoding[\"input_ids\"]\n        else:\n            batch[\"labels\"] = copy.deepcopy(encoding[\"input_ids\"])\n    return batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# training_args = Seq2SeqTrainingArguments(\n#     output_dir = OUTPUT_DIR, \n#     num_train_epochs = 1, \n#     evaluation_strategy=\"steps\",\n#     #eval_steps = 50,\n#     #logging_steps = 50,\n#     save_total_limit = 1,\n#     per_device_train_batch_size=8, \n#     per_device_eval_batch_size=1,\n#     bf16=False,\n#     fp16=True,\n#     warmup_steps=0, \n#     weight_decay=0.01, \n#     logging_dir='./logs',\n#     save_steps = 0,\n#     load_best_model_at_end=True,\n#     remove_unused_columns=False,\n#     generation_config=transformers.GenerationConfig(\n#             max_length=MAX_TOKEN_LENGTH,\n#             num_beams=5,\n#     ),\n#     predict_with_generate=True,\n#     generation_max_length=MAX_TOKEN_LENGTH,\n#     eval_accumulation_steps=10,\n#     report_to=['tensorboard']\n#     )\ntraining_args = TrainingArguments(\n    output_dir = OUTPUT_DIR, \n    num_train_epochs = 1, \n    evaluation_strategy=\"steps\",\n    #eval_steps = 50,\n    #logging_steps = 50,\n    save_total_limit = 1,\n    per_device_train_batch_size=8, \n    per_device_eval_batch_size=1,\n    bf16=False,\n    fp16=True,\n    warmup_steps=0, \n    weight_decay=0.01, \n    logging_dir='./logs',\n    save_steps = 0,\n    load_best_model_at_end=True,\n    #remove_unused_columns=False,\n    # generation_config=transformers.GenerationConfig(\n    #         max_length=MAX_TOKEN_LENGTH,\n    #         num_beams=5,\n    # ),\n    # predict_with_generate=True,\n    # generation_max_length=MAX_TOKEN_LENGTH,\n    eval_accumulation_steps=10,\n    report_to=['tensorboard']\n    )","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:46:08.488015Z","iopub.execute_input":"2024-04-23T14:46:08.488750Z","iopub.status.idle":"2024-04-23T14:46:08.515829Z","shell.execute_reply.started":"2024-04-23T14:46:08.488718Z","shell.execute_reply":"2024-04-23T14:46:08.514933Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def formatting_prompts_func(inputs):\n    for input in inputs:\n        try:\n                data = {\n                    'messages': [\n                        {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n                        {\"role\": \"user\", \"content\": f\"{input['Patient']}, choose from A) {input['Options']['A']}, B) {input['Options']['B']}, C) {input['Options']['C']}, D)  {input['Options']['D']}\"},\n                        {\"role\": \"assistant\", \"content\": input[\"Doctor\"]}\n                    ]\n                }\n        except:\n                data = {\n                    'messages': [\n                        {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n                        {\"role\": \"user\", \"content\":input[\"Patient\"]},\n                        {\"role\": \"assistant\", \"content\": input[\"Doctor\"]}\n                    ]\n                }\n        return data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer = Seq2SeqTrainer(\n#     model=model, \n#     args=training_args, \n#     train_dataset=train_dataset,\n#     eval_dataset=eval_dataset_1, \n#     data_collator=custom_data_collator\n# )\ntrainer = SFTTrainer(\n    model=base_model, \n    args=training_args, \n    peft_config=lora_config,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset_1, \n    max_seq_length=1024,\n    #data_collator=custom_data_collator,\n    #dataset_text_field=\"messages\",\n    packing=False\n)\ntrainer.model.gradient_checkpointing_enable()\ntrainer.model.enable_input_require_grads()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:51:26.397507Z","iopub.execute_input":"2024-04-23T14:51:26.398197Z","iopub.status.idle":"2024-04-23T14:51:29.107514Z","shell.execute_reply.started":"2024-04-23T14:51:26.398164Z","shell.execute_reply":"2024-04-23T14:51:29.106738Z"},"trusted":true},"execution_count":83,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2703 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2be335f665f436794c8fcfedc14ad90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1159 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5884056c98eb4dc993f82f068146f879"}},"metadata":{}}]},{"cell_type":"code","source":"model.config.pad_token_id = ltokenizer.pad_token_id","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer = None\n# model = None\n# base_model = None\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:51:34.123994Z","iopub.execute_input":"2024-04-23T14:51:34.124627Z","iopub.status.idle":"2024-04-23T14:51:34.479229Z","shell.execute_reply.started":"2024-04-23T14:51:34.124598Z","shell.execute_reply":"2024-04-23T14:51:34.478275Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:51:37.000337Z","iopub.execute_input":"2024-04-23T14:51:37.000674Z","iopub.status.idle":"2024-04-23T15:19:27.975294Z","shell.execute_reply.started":"2024-04-23T14:51:37.000649Z","shell.execute_reply":"2024-04-23T15:19:27.974245Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='338' max='338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [338/338 27:45, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=338, training_loss=2.0002365563748152, metrics={'train_runtime': 1670.5847, 'train_samples_per_second': 1.618, 'train_steps_per_second': 0.202, 'total_flos': 1.0065805950799872e+16, 'train_loss': 2.0002365563748152, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.model.save_pretrained(f\"{OUTPUT_DIR}/model_save\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:24:22.509020Z","iopub.execute_input":"2024-04-23T15:24:22.509446Z","iopub.status.idle":"2024-04-23T15:24:22.532126Z","shell.execute_reply.started":"2024-04-23T15:24:22.509414Z","shell.execute_reply":"2024-04-23T15:24:22.531208Z"},"trusted":true},"execution_count":88,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4435537e9f0649638326e249c1f51d53"}},"metadata":{}}]},{"cell_type":"code","source":"tok = AutoTokenizer.from_pretrained(MODEL_NAME)\ntok.push_to_hub(\"SurtMcGert/advanced-AI-CW-Med-Chat-Bot\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.push_to_hub(\"SurtMcGert/advanced-AI-CW-Med-Chat-Bot\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:24:43.554038Z","iopub.execute_input":"2024-04-23T15:24:43.554803Z","iopub.status.idle":"2024-04-23T15:24:48.805682Z","shell.execute_reply.started":"2024-04-23T15:24:43.554773Z","shell.execute_reply":"2024-04-23T15:24:48.804637Z"},"trusted":true},"execution_count":89,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e026a1f732c4c24b36b0e706622e635"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/9.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cb098ac6cb94684a0d038736d2bb80c"}},"metadata":{}},{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/SurtMcGert/advanced-AI-CW-Med-Chat-Bot/commit/217f27b7a5aa155d8f8acc8570f8af3af5301763', commit_message='Upload model', commit_description='', oid='217f27b7a5aa155d8f8acc8570f8af3af5301763', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load the Model","metadata":{}},{"cell_type":"code","source":"# base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n# model = PeftModel.from_pretrained(base_model, f\"{OUTPUT_DIR}/model_save\")\nmodel = AutoModelForCausalLM.from_pretrained(\"SurtMcGert/advanced-AI-CW-Med-Chat-Bot\").to(DEVICE)\nmodel.config.pad_token_id = ltokenizer.pad_token_id\nmodel.config.max_length = MAX_TOKEN_LENGTH\n#model.gradient_checkpointing_enable()\n#model.enable_input_require_grads()\ntrainer = Seq2SeqTrainer(\n    model=model, \n    args=training_args, \n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset_1, \n    data_collator=custom_data_collator\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#base_model = None\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:25:23.913797Z","iopub.execute_input":"2024-04-23T15:25:23.914694Z","iopub.status.idle":"2024-04-23T15:25:24.275833Z","shell.execute_reply.started":"2024-04-23T15:25:23.914660Z","shell.execute_reply":"2024-04-23T15:25:24.274903Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"eval_result_1 = trainer.predict(eval_dataset_1, max_new_tokens=512)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_result_2 = trainer.predict(eval_dataset_2, max_new_tokens=512)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logits_1 = eval_result_1.predictions\nlogits_1[logits_1 == -100] = ltokenizer.eos_token_id\nlogits_2 = eval_result_2.predictions\nlogits_2[logits_2 == -100] = ltokenizer.eos_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the raw evaluation output\nraw_text_result_1 = ltokenizer.batch_decode(logits_1, skip_special_tokens=True)\nraw_text_result_2 = ltokenizer.batch_decode(logits_2, skip_special_tokens=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(raw_text_result_1[6])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the questions and ground truths from both evaluation datasets\nquestions_1 = []\nground_truth_1 = []\ntry:\n    for item in eval_dataset_1:\n        questions_1.append(item[\"input\"])\n        ground_truth_1.append(item[\"target\"])\nexcept:\n    pass\n\nquestions_2 = []\nground_truth_2 = []\ntry:\n    for item in eval_dataset_2:\n        questions_2.append(item[\"input\"])\n        ground_truth_2.append(item[\"target\"])\nexcept:\n    pass\n\n# create lists for the text outputs\ntext_result_1 = list()\ntext_result_2 = list()\n\n# get the answers for the MedDialog dataset\nfor item in raw_text_result_1:\n    index = item.find(\"|<assistant>|\")\n    output = item[index+13:]\n    index = output.find(ltokenizer.eos_token)\n    if(index > -1):\n        output = output[:index]\n    text_result_1.append(output)\n\n\n# get the answers for the USMLE dataset\nfor item in raw_text_result_2:\n    index = item.find(\"|<assistant>|\")\n    output = item[index+13:]\n    index = output.find(ltokenizer.eos_token)\n    if(index > -1):\n        output = output[:index]\n    text_result_2.append(output)\n\n\n\n# print the first 2 results from each dataset evaluation\nprint(\"============================MedDialog Evaluation============================\")\nfor question, answer in list(zip(questions_1, text_result_1))[:2]:\n    print(f\"\"\"\n    Question: {question}\n    Answer: {answer}\n    \"\"\")\n\nprint(\"============================USMLE Evaluation============================\")\nfor question, answer in list(zip(questions_2, text_result_2))[:2]:\n    print(f\"\"\"\n    Question: {question}\n    Answer: {answer}\n    \"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"markdown","source":"## Load the Required Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"# perplexity - measures certainty of the model.\n# METEOR - extension of BLEU (measure similarity between the output and the ground truth) but accounts for word semantics.\n# ROUGE - considers n-gram overlap (recall) but also precision.\n# SQuAD v2 - a metric for measuring a models correctness in answering the multiple choice questions\n# Accuracy - use this for the multiple choice dataset\n\nperplexity_scorer = evaluate.load('perplexity')\nmeteor_scorer = evaluate.load('meteor')\nrouge_scorer = evaluate.load('rouge')\nsquad_scorer = evaluate.load('squad_v2')\naccuracy_scorer = evaluate.load('accuracy')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute the bleu and rouge scores for the MedDialog evaluation\nbleu_score_1 = bleu_scorer.compute(predictions=text_result_1, references=ground_truth_1)\nrouge_score_1 = rouge_scorer.compute(predictions=text_result_1, references=ground_truth_1)\n\n# compute the bleu and rouge scores for the USMLE evaluation\nbleu_score_2 = bleu_scorer.compute(predictions=text_result_1, references=ground_truth_2)\nrouge_score_2 = rouge_scorer.compute(predictions=text_result_1, references=ground_truth_2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print scores for MedDialog evaluation\nprint(\"score on MedDialog Dataset\")\nprint('BLEU1:', bleu_score_1['precisions'][0]*100)\nprint(f\"\"\"\nROUGE-1: {rouge_score_1['rouge1']*100}\nROUGE-2: {rouge_score_1['rouge2']*100}\nROUGE-L: {rouge_score_1['rougeL']*100}\n\"\"\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print scores for USMLE evaluation\nprint(\"score on USMLE Dataset\")\nprint('BLEU1:', bleu_score_2['precisions'][0]*100)\nprint(f\"\"\"\nROUGE-1: {rouge_score_2['rouge1']*100}\nROUGE-2: {rouge_score_2['rouge2']*100}\nROUGE-L: {rouge_score_2['rougeL']*100}\n\"\"\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TESTING JUST IGNORE ALL THIS","metadata":{}},{"cell_type":"markdown","source":"## TEST 1","metadata":{}},{"cell_type":"code","source":"test_1_result = trainer.predict(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:26:33.337928Z","iopub.execute_input":"2024-04-23T15:26:33.338516Z","iopub.status.idle":"2024-04-23T15:26:33.717122Z","shell.execute_reply.started":"2024-04-23T15:26:33.338484Z","shell.execute_reply":"2024-04-23T15:26:33.715702Z"},"trusted":true},"execution_count":93,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[93], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_1_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3543\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3540\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3542\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3543\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[1;32m   3545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3546\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3650\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3647\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3649\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3650\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3651\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3652\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3789\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprediction_step\u001b[39m(\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3761\u001b[0m     model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3764\u001b[0m     ignore_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3765\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m   3766\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3767\u001b[0m \u001b[38;5;124;03m    Perform an evaluation step on `model` using `inputs`.\u001b[39;00m\n\u001b[1;32m   3768\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3787\u001b[0m \u001b[38;5;124;03m        logits and labels (each being optional).\u001b[39;00m\n\u001b[1;32m   3788\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3789\u001b[0m     has_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_names) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3790\u001b[0m     \u001b[38;5;66;03m# For CLIP-like models capable of returning loss values.\u001b[39;00m\n\u001b[1;32m   3791\u001b[0m     \u001b[38;5;66;03m# If `return_loss` is not specified or being `None` in `inputs`, we check if the default value of `return_loss`\u001b[39;00m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;66;03m# is `True` in `model.forward`.\u001b[39;00m\n\u001b[1;32m   3793\u001b[0m     return_loss \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3789\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprediction_step\u001b[39m(\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3761\u001b[0m     model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3764\u001b[0m     ignore_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3765\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m   3766\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3767\u001b[0m \u001b[38;5;124;03m    Perform an evaluation step on `model` using `inputs`.\u001b[39;00m\n\u001b[1;32m   3768\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3787\u001b[0m \u001b[38;5;124;03m        logits and labels (each being optional).\u001b[39;00m\n\u001b[1;32m   3788\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3789\u001b[0m     has_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_names) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(k) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_names)\n\u001b[1;32m   3790\u001b[0m     \u001b[38;5;66;03m# For CLIP-like models capable of returning loss values.\u001b[39;00m\n\u001b[1;32m   3791\u001b[0m     \u001b[38;5;66;03m# If `return_loss` is not specified or being `None` in `inputs`, we check if the default value of `return_loss`\u001b[39;00m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;66;03m# is `True` in `model.forward`.\u001b[39;00m\n\u001b[1;32m   3793\u001b[0m     return_loss \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"],"ename":"AttributeError","evalue":"'NoneType' object has no attribute 'get'","output_type":"error"}]},{"cell_type":"code","source":"print(test_1_result.predictions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logits_test_1 = test_1_result.predictions\nlogits_test_1[logits_test_1 == -100] = ltokenizer.eos_token_id","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_text_result_test_1 = ltokenizer.batch_decode(logits_test_1, skip_special_tokens=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(raw_text_result_test_1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TEST 2","metadata":{}},{"cell_type":"code","source":"initial_prompt = test_dataset[0]['messages'][1]['content']","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:34:19.939149Z","iopub.execute_input":"2024-04-23T15:34:19.939530Z","iopub.status.idle":"2024-04-23T15:34:19.944473Z","shell.execute_reply.started":"2024-04-23T15:34:19.939498Z","shell.execute_reply":"2024-04-23T15:34:19.943556Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"prompt = f\"a medical student is preparing for her final examination. Her patient has come to her asking: '{initial_prompt}'. Explain to the student the most likely cause/course of action.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:34:23.428404Z","iopub.execute_input":"2024-04-23T15:34:23.428764Z","iopub.status.idle":"2024-04-23T15:34:23.433633Z","shell.execute_reply.started":"2024-04-23T15:34:23.428736Z","shell.execute_reply":"2024-04-23T15:34:23.432626Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"text = ltokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nprint(text)\n\ntest_model_inputs_1 = ltokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n\n\ntest_model_inputs_2 = custom_data_collator(test_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:34:28.371132Z","iopub.execute_input":"2024-04-23T15:34:28.371497Z","iopub.status.idle":"2024-04-23T15:34:28.421946Z","shell.execute_reply.started":"2024-04-23T15:34:28.371470Z","shell.execute_reply":"2024-04-23T15:34:28.420607Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"<|system|>You are a medical professional providing consultation and medical diagnostics.</s><|user|>a medical student is preparing for her final examination. Her patient has come to her asking: 'Hello doctor, My wife is 5 months pregnant. She is treated in one of the reputed hospitals. She has been advised to take Feronia -XT and Cal 360 tablets. Over last three to four days she is having cough in the night time and she is afraid to use any tablets or tonic. Kindly suggest which tablet or tonic she should take.'. Explain to the student the most likely cause/course of action.</s><|assistant|>\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[106], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[1;32m      9\u001b[0m test_model_inputs_1 \u001b[38;5;241m=\u001b[39m ltokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m test_model_inputs_2 \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_data_collator\u001b[49m(test_dataset)\n","\u001b[0;31mNameError\u001b[0m: name 'custom_data_collator' is not defined"],"ename":"NameError","evalue":"name 'custom_data_collator' is not defined","output_type":"error"}]},{"cell_type":"code","source":"print(test_model_inputs_1)\nprint(test_model_inputs_2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_generated_ids_1 = model.generate(\n    test_model_inputs_1[\"input_ids\"].to(DEVICE),\n    max_new_tokens=100\n)\ntest_generated_ids_2 = model.generate(\n    test_model_inputs_2[\"input_ids\"].to(DEVICE),\n    max_new_tokens=100\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generated_ids = [\n#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n# ]\n\ntest_response_1 = ltokenizer.batch_decode(test_generated_ids_1, skip_special_tokens=True)[0]\ntest_response_2 = ltokenizer.batch_decode(test_generated_ids_2, skip_special_tokens=True)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_response_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_response_2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test 3","metadata":{}},{"cell_type":"code","source":"prompt = f\"what's the answer to life, the universe, and everything\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:41:40.925699Z","iopub.execute_input":"2024-04-23T14:41:40.926118Z","iopub.status.idle":"2024-04-23T14:41:40.931199Z","shell.execute_reply.started":"2024-04-23T14:41:40.926084Z","shell.execute_reply":"2024-04-23T14:41:40.930312Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"text = ltokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nprint(text)\n\ntest_model_inputs = ltokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:41:44.588850Z","iopub.execute_input":"2024-04-23T14:41:44.589264Z","iopub.status.idle":"2024-04-23T14:41:44.595638Z","shell.execute_reply.started":"2024-04-23T14:41:44.589226Z","shell.execute_reply":"2024-04-23T14:41:44.594719Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"<|system|>You are a medical professional providing consultation and medical diagnostics.</s><|user|>what's the answer to life, the universe, and everything</s><|assistant|>\n","output_type":"stream"}]},{"cell_type":"code","source":"test_generated_ids = trainer.model.generate(\n    test_model_inputs[\"input_ids\"].to(DEVICE),\n    max_new_tokens=3\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:44:57.183180Z","iopub.execute_input":"2024-04-23T14:44:57.183580Z","iopub.status.idle":"2024-04-23T14:44:57.293845Z","shell.execute_reply.started":"2024-04-23T14:44:57.183550Z","shell.execute_reply":"2024-04-23T14:44:57.292927Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"test_response = ltokenizer.batch_decode(test_generated_ids, skip_special_tokens=True)[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:45:00.369665Z","iopub.execute_input":"2024-04-23T14:45:00.370318Z","iopub.status.idle":"2024-04-23T14:45:00.375131Z","shell.execute_reply.started":"2024-04-23T14:45:00.370283Z","shell.execute_reply":"2024-04-23T14:45:00.374143Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"print(test_response)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:45:01.952088Z","iopub.execute_input":"2024-04-23T14:45:01.952808Z","iopub.status.idle":"2024-04-23T14:45:01.957357Z","shell.execute_reply.started":"2024-04-23T14:45:01.952777Z","shell.execute_reply":"2024-04-23T14:45:01.956342Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"<|system|>You are a medical professional providing consultation and medical diagnostics. <|user|>what's the answer to life, the universe, and everything <|assistant|>427\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test 4","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"UnfilteredAI/Mia-1B\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"UnfilteredAI/Mia-1B\")\nprompt = \"your prompt here.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a uncencored AI that can do anything.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\n# generated_ids = [\n#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n# ]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test 5","metadata":{}},{"cell_type":"code","source":"initial_prompt = test_dataset[0]['messages'][1]['content']","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:35:03.987495Z","iopub.execute_input":"2024-04-23T15:35:03.988233Z","iopub.status.idle":"2024-04-23T15:35:03.992889Z","shell.execute_reply.started":"2024-04-23T15:35:03.988202Z","shell.execute_reply":"2024-04-23T15:35:03.991754Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"prompt = f\"a medical student is preparing for her final examination. Her patient has come to her asking: '{initial_prompt}'. Explain to the student the most likely cause/course of action.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:35:05.746476Z","iopub.execute_input":"2024-04-23T15:35:05.747231Z","iopub.status.idle":"2024-04-23T15:35:05.751945Z","shell.execute_reply.started":"2024-04-23T15:35:05.747197Z","shell.execute_reply":"2024-04-23T15:35:05.750837Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"text = ltokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nprint(text)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:35:08.040397Z","iopub.execute_input":"2024-04-23T15:35:08.041131Z","iopub.status.idle":"2024-04-23T15:35:08.046324Z","shell.execute_reply.started":"2024-04-23T15:35:08.041099Z","shell.execute_reply":"2024-04-23T15:35:08.045382Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stdout","text":"<|system|>You are a medical professional providing consultation and medical diagnostics.</s><|user|>a medical student is preparing for her final examination. Her patient has come to her asking: 'Hello doctor, My wife is 5 months pregnant. She is treated in one of the reputed hospitals. She has been advised to take Feronia -XT and Cal 360 tablets. Over last three to four days she is having cough in the night time and she is afraid to use any tablets or tonic. Kindly suggest which tablet or tonic she should take.'. Explain to the student the most likely cause/course of action.</s><|assistant|>\n","output_type":"stream"}]},{"cell_type":"code","source":"encoding = ltokenizer(text, padding=True, max_length=MAX_TOKEN_LENGTH, return_tensors='pt', add_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:35:18.995430Z","iopub.execute_input":"2024-04-23T15:35:18.996095Z","iopub.status.idle":"2024-04-23T15:35:19.002457Z","shell.execute_reply.started":"2024-04-23T15:35:18.996062Z","shell.execute_reply":"2024-04-23T15:35:19.001593Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"print(encoding.input_ids)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:35:43.261816Z","iopub.execute_input":"2024-04-23T15:35:43.262196Z","iopub.status.idle":"2024-04-23T15:35:43.269202Z","shell.execute_reply.started":"2024-04-23T15:35:43.262165Z","shell.execute_reply":"2024-04-23T15:35:43.268194Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"tensor([[    1,   529, 29989,  5205, 29989, 29958,  3492,   526,   263, 16083,\n         10257, 13138,  8799,   362,   322, 16083,   652, 20921, 29889,     2,\n           529, 29989,  1792, 29989, 29958, 29874, 16083,  8368,   338, 10223,\n           292,   363,   902,  2186,  4392,  3381, 29889,  2439, 16500,   756,\n          2041,   304,   902,  6721, 29901,   525, 10994, 11619, 29892,  1619,\n          6532,   338, 29871, 29945,  7378,   758,  5138,   424, 29889,  2296,\n           338, 14914,   297,   697,   310,   278,   337,   649,   287, 29418,\n           277,  1338, 29889,  2296,   756,  1063,   594, 11292,   304,  2125,\n          7756,  6405,   448, 12188,   322,  3037, 29871, 29941, 29953, 29900,\n          1591,  1372, 29889,  6811,  1833,  2211,   304,  3023,  3841,  1183,\n           338,  2534,   274,   820,   297,   278,  4646,   931, 30081,   392,\n          1183,   338, 13421,   304,   671,   738,  1591,  1372,   470,   260,\n          8927, 29889, 13187,   368,  4368,   607,  1591, 29873,   470,   260,\n          8927,  1183,   881,  2125, 29889,  4286, 12027,  7420,   304,   278,\n          8368,   278,  1556,  5517,  4556, 29914, 15775,   310,  3158, 29889,\n             2,   529, 29989,   465, 22137, 29989, 29958]])\n","output_type":"stream"}]},{"cell_type":"code","source":"test_generated_ids = trainer.model.generate(\n    encoding[\"input_ids\"].to(DEVICE),\n    max_new_tokens=100\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:36:12.806768Z","iopub.execute_input":"2024-04-23T15:36:12.807154Z","iopub.status.idle":"2024-04-23T15:36:16.137203Z","shell.execute_reply.started":"2024-04-23T15:36:12.807123Z","shell.execute_reply":"2024-04-23T15:36:16.136289Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"decoded = ltokenizer.batch_decode(test_generated_ids, skip_special_tokens=True)[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:36:30.775564Z","iopub.execute_input":"2024-04-23T15:36:30.776418Z","iopub.status.idle":"2024-04-23T15:36:30.781534Z","shell.execute_reply.started":"2024-04-23T15:36:30.776385Z","shell.execute_reply":"2024-04-23T15:36:30.780413Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"print(decoded)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T15:36:32.018421Z","iopub.execute_input":"2024-04-23T15:36:32.019033Z","iopub.status.idle":"2024-04-23T15:36:32.023581Z","shell.execute_reply.started":"2024-04-23T15:36:32.019001Z","shell.execute_reply":"2024-04-23T15:36:32.022679Z"},"trusted":true},"execution_count":115,"outputs":[{"name":"stdout","text":"<|system|>You are a medical professional providing consultation and medical diagnostics. <|user|>a medical student is preparing for her final examination. Her patient has come to her asking: 'Hello doctor, My wife is 5 months pregnant. She is treated in one of the reputed hospitals. She has been advised to take Feronia -XT and Cal 360 tablets. Over last three to four days she is having cough in the night time and she is afraid to use any tablets or tonic. Kindly suggest which tablet or tonic she should take.'. Explain to the student the most likely cause/course of action. <|assistant|>Hi, I need to explain a drug associated with influenza. It has been reported as a cause of nephrotic syndrome which is the most common cause of renal failure in nephrotic syndrome. Influenza infection, especially influenza A, is not the only cause of the renal damage observed in the patient because of flu. My understanding is based on a report from a renal fellow.  The most likely cause of the cough and hematur\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Random stuff","metadata":{}},{"cell_type":"code","source":"test_train_input = custom_data_collator(test_train_dataset)","metadata":{},"execution_count":null,"outputs":[]}]}