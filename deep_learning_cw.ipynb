{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":89,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["DEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"]},{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["DEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"]},{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["DEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"]},{"name":"stdout","output_type":"stream","text":["^C\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["DEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"]}],"source":["# %pip install -q evaluate\n","# %pip install -q opendatasets\n","# %pip install -q --upgrade accelerate\n","# %pip install -q --upgrade transformers\n","# %pip install -q peft\n","# %pip install -q --upgrade bitsandbytes\n","# %pip install -q accelerate"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:29:38.889695Z","iopub.status.busy":"2024-04-19T16:29:38.889116Z","iopub.status.idle":"2024-04-19T16:29:46.807007Z","shell.execute_reply":"2024-04-19T16:29:46.806136Z","shell.execute_reply.started":"2024-04-19T16:29:38.889665Z"},"trusted":true},"outputs":[],"source":["import pandas as pd \n","import torch\n","import torch.nn as nn\n","torch.cuda.set_per_process_memory_fraction(0.9)\n","torch.backends.cuda.matmul.allow_tf32 = True\n","import torchtext\n","from torch.utils.data import Dataset, random_split\n","from typing import List, Dict, Union\n","from typing import Any, TypeVar\n","import pandas as pd\n","import os\n","import copy\n","import gc\n","import evaluate\n","import opendatasets as od\n","from huggingface_hub import login\n","from typing import Optional, Tuple, Union\n","\n","from datasets import load_dataset, Features, Value\n","import accelerate\n","\n","from peft import LoftQConfig, LoraConfig, get_peft_model, PeftModel\n","\n","import transformers\n","from transformers.modeling_outputs import QuestionAnsweringModelOutput\n","from transformers import BertLMHeadModel, AutoConfig, BitsAndBytesConfig,Conv1D\n","from transformers import AutoTokenizer, Seq2SeqTrainingArguments \n","from transformers import Seq2SeqTrainer, AutoModelForCausalLM, IntervalStrategy, AutoModelForQuestionAnswering\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{},"source":["set a seed and confirm CUDA support"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:29:50.649355Z","iopub.status.busy":"2024-04-19T16:29:50.648378Z","iopub.status.idle":"2024-04-19T16:29:50.667337Z","shell.execute_reply":"2024-04-19T16:29:50.666247Z","shell.execute_reply.started":"2024-04-19T16:29:50.649320Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch Version:  2.2.1+cu121\n","torchtext Version:  0.17.1+cpu\n","Using GPU.\n"]}],"source":["torch.manual_seed(2137)\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.backends.cudnn.deterministic = True\n","\n","print(\"PyTorch Version: \", torch.__version__)\n","print(\"torchtext Version: \", torchtext.__version__)\n","print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Download"]},{"cell_type":"markdown","metadata":{},"source":["## Downloading MedDialog Dataset"]},{"cell_type":"markdown","metadata":{},"source":["NOTE: you will need a kaggle API key for the following to work"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import json\n","\n","# Path to your JSON file\n","json_file_path = \"kaggle.json\"\n","\n","# Open the file and read the content\n","try:\n","  with open(json_file_path, \"r\") as f:\n","    json_data = json.load(f)\n","except FileNotFoundError:\n","  print(f\"Error: JSON file not found at {json_file_path}\")\n","  exit(1)\n","\n","# Access username and key from the JSON data\n","try:\n","  username = json_data[\"username\"]\n","  key = json_data[\"key\"]\n","except KeyError:\n","  print(\"Error: 'username' or 'key' key not found in JSON data\")\n","  exit(1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.environ['KAGGLE_USERNAME'] = username\n","os.environ['KAGGLE_KEY'] = key\n","\n","# Assign the Kaggle data set URL into variable\n","dataset = 'https://www.kaggle.com/datasets/dsxavier/diagnoise-me'\n","# Using opendatasets let's download the data sets\n","od.download(dataset, \"dataset\")"]},{"cell_type":"markdown","metadata":{},"source":["## Downloading USMLE Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:02.426054Z","iopub.status.busy":"2024-04-19T16:30:02.425333Z","iopub.status.idle":"2024-04-19T16:30:06.152152Z","shell.execute_reply":"2024-04-19T16:30:06.151172Z","shell.execute_reply.started":"2024-04-19T16:30:02.426022Z"},"trusted":true},"outputs":[],"source":["USMLE_dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options\", split=\"test\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:08.846530Z","iopub.status.busy":"2024-04-19T16:30:08.845847Z","iopub.status.idle":"2024-04-19T16:30:08.851880Z","shell.execute_reply":"2024-04-19T16:30:08.850939Z","shell.execute_reply.started":"2024-04-19T16:30:08.846496Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'question': 'A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?', 'answer': 'Tell the attending that he cannot fail to disclose this mistake', 'options': {'A': 'Disclose the error to the patient and put it in the operative report', 'B': 'Tell the attending that he cannot fail to disclose this mistake', 'C': 'Report the physician to the ethics committee', 'D': 'Refuse to dictate the operative report'}, 'meta_info': 'step1', 'answer_idx': 'B', 'metamap_phrases': ['junior orthopaedic surgery resident', 'completing', 'carpal tunnel repair', 'department chairman', 'attending physician', 'case', 'resident', 'cuts', 'flexor tendon', 'tendon', 'repaired', 'complication', 'attending', 'resident', 'patient', 'fine', 'need to report', 'minor complication', 'not', 'patient', 'not', 'to make', 'patient worry', 'resident to leave', 'complication out', 'operative report', 'following', 'correct next action', 'resident to take']}\n","1273\n"]}],"source":["print(USMLE_dataset[0])\n","print(len(USMLE_dataset))"]},{"cell_type":"markdown","metadata":{},"source":["# Load Datasets"]},{"cell_type":"markdown","metadata":{},"source":["## Loading MedDialog Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:11.676132Z","iopub.status.busy":"2024-04-19T16:30:11.675273Z","iopub.status.idle":"2024-04-19T16:30:12.506928Z","shell.execute_reply":"2024-04-19T16:30:12.505966Z","shell.execute_reply.started":"2024-04-19T16:30:11.676100Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['id', 'Description', 'Doctor', 'Patient'], dtype='object')\n","3862\n"]}],"source":["DATA_PATH = \"dataset\\\\diagnoise-me\\\\diagnose_en_dataset.feather\"\n","# DATA_PATH = \"/kaggle/input/diagnoise-me/diagnose_en_dataset.feather\"\n","SEQ_LEN: int = 1024\n","data = pd.read_feather(DATA_PATH)\n","SAMPLE_SIZE: int =  int(data.shape[0] * 0.015) #get 1% of the data\n","data = data[:SAMPLE_SIZE]\n","print(data.keys())\n","print(len(data))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:14.818676Z","iopub.status.busy":"2024-04-19T16:30:14.817968Z","iopub.status.idle":"2024-04-19T16:30:14.828370Z","shell.execute_reply":"2024-04-19T16:30:14.827420Z","shell.execute_reply.started":"2024-04-19T16:30:14.818645Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train data shape: (2703, 4)\n","Eval data shape: (1159, 4)\n"]}],"source":["# Split data into train and eval sets with 70% for training\n","train_data, eval_data = train_test_split(data, test_size=0.3, random_state=42)\n","\n","train_data = train_data.reset_index(drop=True)\n","eval_data = eval_data.reset_index(drop=True)\n","\n","# Print the shapes of the train and eval sets\n","print(\"Train data shape:\", train_data.shape)\n","print(\"Eval data shape:\", eval_data.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Loading USMLE Dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:22.537029Z","iopub.status.busy":"2024-04-19T16:30:22.536651Z","iopub.status.idle":"2024-04-19T16:30:22.567198Z","shell.execute_reply":"2024-04-19T16:30:22.566153Z","shell.execute_reply.started":"2024-04-19T16:30:22.536998Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["USMLELiveEQA data shape: (1273, 3)\n"]}],"source":["USMLE_dataset = pd.DataFrame({'Doctor': USMLE_dataset[\"answer\"], 'Patient': USMLE_dataset[\"question\"], 'Options':USMLE_dataset[\"options\"]})\n","# Print the shapes of the set\n","print(\"USMLELiveEQA data shape:\", USMLE_dataset.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Create an output directory"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:25.862769Z","iopub.status.busy":"2024-04-19T16:30:25.862155Z","iopub.status.idle":"2024-04-19T16:30:25.867177Z","shell.execute_reply":"2024-04-19T16:30:25.866201Z","shell.execute_reply.started":"2024-04-19T16:30:25.862739Z"},"trusted":true},"outputs":[],"source":["os.makedirs('./results', exist_ok = True)\n","OUTPUT_DIR: str = './results'"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:33.792051Z","iopub.status.busy":"2024-04-19T16:30:33.791409Z","iopub.status.idle":"2024-04-19T16:30:33.796044Z","shell.execute_reply":"2024-04-19T16:30:33.795042Z","shell.execute_reply.started":"2024-04-19T16:30:33.792023Z"},"trusted":true},"outputs":[],"source":["# tokens for the datset\n","MODEL_NAME: str = 'UnfilteredAI/Mia-1B'"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:30:37.285312Z","iopub.status.busy":"2024-04-19T16:30:37.284421Z","iopub.status.idle":"2024-04-19T16:30:38.664054Z","shell.execute_reply":"2024-04-19T16:30:38.662945Z","shell.execute_reply.started":"2024-04-19T16:30:37.285275Z"},"trusted":true},"outputs":[],"source":["# Load tokenizer \n","MAX_TOKEN_LENGTH = 1024\n","\n","# for evaluation\n","ltokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","ltokenizer.padding_side = 'left'\n","ltokenizer.truncation_side = 'left'\n","\n","# for training\n","rtokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","rtokenizer.padding_side = 'right'\n","rtokenizer.truncation_side = 'right'"]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:31:00.071140Z","iopub.status.busy":"2024-04-19T16:31:00.070407Z","iopub.status.idle":"2024-04-19T16:31:03.189251Z","shell.execute_reply":"2024-04-19T16:31:03.188454Z","shell.execute_reply.started":"2024-04-19T16:31:00.071106Z"},"trusted":true},"outputs":[],"source":["base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n","#base_model.resize_token_embeddings(len(rtokenizer))"]},{"cell_type":"code","execution_count":93,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:31:04.940454Z","iopub.status.busy":"2024-04-19T16:31:04.939727Z","iopub.status.idle":"2024-04-19T16:31:04.946753Z","shell.execute_reply":"2024-04-19T16:31:04.945767Z","shell.execute_reply.started":"2024-04-19T16:31:04.940421Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 2048)\n","    (layers): ModuleList(\n","      (0-21): 22 x LlamaDecoderLayer(\n","        (self_attn): LlamaSdpaAttention(\n","          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm()\n","        (post_attention_layernorm): LlamaRMSNorm()\n","      )\n","    )\n","    (norm): LlamaRMSNorm()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",")\n"]}],"source":["print(base_model)"]},{"cell_type":"code","execution_count":94,"metadata":{"trusted":true},"outputs":[],"source":["lora_config = LoraConfig(\n","    lora_alpha=16, # lora alpha for scaling\n","    r=16, # rank\n","    lora_dropout=0.05, #dropout\n","    use_rslora=True, #  sets the adapter scaling factor to lora_alpha/math.sqrt(r)\n","    bias=\"none\", # dont train biases\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    #layers_to_transform=[20]\n",")\n","model = get_peft_model(base_model, lora_config)\n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()"]},{"cell_type":"code","execution_count":74,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 2252800 || all params: 1102301184 || trainable%: 0.20437245579516677\n"]},{"data":{"text/plain":["{'trainable': 2252800, 'all': 1102301184, 'trainable%': 0.20437245579516677}"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )\n","    return {\"trainable\": trainable_params, \"all\": all_param, \"trainable%\": 100 * trainable_params / all_param}\n","\n","print_trainable_parameters(model)"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing Data for Training"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:49:39.463967Z","iopub.status.busy":"2024-04-19T17:49:39.463051Z","iopub.status.idle":"2024-04-19T17:49:39.472383Z","shell.execute_reply":"2024-04-19T17:49:39.471373Z","shell.execute_reply.started":"2024-04-19T17:49:39.463934Z"},"trusted":true},"outputs":[],"source":["class DoctorPatientDataset(Dataset):\n","    \n","    def __init__(self, data, split):\n","        \n","        self.input_x: List = data[\"Patient\"]\n","        self.input_x = self.input_x.reset_index(drop=True)\n","        self.target: List = data[\"Doctor\"]\n","        self.target = self.target.reset_index(drop=True)\n","        self.split = split\n","\n","        try:\n","            self.options: List = data[\"Options\"]\n","        except:\n","            pass\n","            \n","    def __len__(self):\n","        return len(self.input_x)\n","    \n","    def __getitem__(self, idx):\n","        try:\n","            data = {\n","                'input': self.input_x[idx],\n","                'target': self.target[idx],\n","                'options': self.options[idx],\n","                'split': self.split\n","            }\n","        except:\n","            data = {\n","                'input': self.input_x[idx],\n","                'target': self.target[idx],\n","                'split': self.split\n","            }\n","        return data"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:49:57.731919Z","iopub.status.busy":"2024-04-19T17:49:57.731191Z","iopub.status.idle":"2024-04-19T17:49:57.738725Z","shell.execute_reply":"2024-04-19T17:49:57.737513Z","shell.execute_reply.started":"2024-04-19T17:49:57.731890Z"},"trusted":true},"outputs":[],"source":["train_dataset = DoctorPatientDataset(data = train_data, split = \"train\")\n","eval_dataset_1 = DoctorPatientDataset(data = eval_data, split = \"eval\")\n","eval_dataset_2 = DoctorPatientDataset(data = USMLE_dataset, split = \"eval\")\n","\n","test_dataset = DoctorPatientDataset(data = eval_data[1:2], split = \"eval\")\n","test_train_dataset = DoctorPatientDataset(data = train_data[0:1], split = \"train\")"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Data Collator"]},{"cell_type":"code","execution_count":106,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T18:08:46.233276Z","iopub.status.busy":"2024-04-19T18:08:46.232895Z","iopub.status.idle":"2024-04-19T18:08:46.248706Z","shell.execute_reply":"2024-04-19T18:08:46.247797Z","shell.execute_reply.started":"2024-04-19T18:08:46.233248Z"},"trusted":true},"outputs":[],"source":["def format_text(message, tokenizer, add_generation_prompt):\n","    text = tokenizer.apply_chat_template(\n","        message,\n","        tokenize=False,\n","        add_generation_prompt=add_generation_prompt\n","    )\n","    return text\n","\n","def custom_data_collator(features, return_tensors=\"pt\"):\n","    batch = {}\n","\n","    #questions = [feature[\"input\"] for feature in features]\n","    questions = [features[i][\"input\"] for i in range(len(features))]\n","    #answers = [feature[\"target\"] for feature in features]\n","    answers = [features[i][\"target\"] for i in range(len(features))]\n","    split = features[0][\"split\"]\n","\n","    # training\n","    if split == 'train':\n","        tokenizer = rtokenizer\n","        bos_token = rtokenizer.bos_token\n","        eos_token = rtokenizer.eos_token\n","        prompts = [f\"a medical student is preparing for her final examination. Her patient has said '{q}'. Provide for the student an example of what her response to the patient should be.\" for q in questions]\n","        #text = [f\"{bos_token}Question:{q}.Answer:{t}{eos_token}\" for q, t in zip(questions, answers)]\n","        messages = [[\n","            {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n","            {\"role\": \"user\", \"content\": prompt},\n","            #{\"role\": \"assistant\", \"content\": a}\n","        ] for prompt, a in zip(prompts, answers)]\n","\n","        labels_messages = [[\n","            {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n","            {\"role\": \"user\", \"content\": prompt},\n","            {\"role\": \"assistant\", \"content\": a}\n","        ] for prompt, a in zip(prompts, answers)]\n","\n","    # evaluation\n","    else:\n","        try:\n","            options = [feature[\"options\"] for feature in features]\n","            multi_choice = True\n","        except:\n","            multi_choice = False\n","\n","\n","        # tokenizer for evaluation\n","        tokenizer = ltokenizer\n","        bos_token = ltokenizer.bos_token\n","\n","        # Format text to be encoded\n","        if(multi_choice == False):\n","            # if we are not using the multiple choice dataset\n","            # text = [f\"{bos_token}Question:{q}.Answer:\" for q in questions]\n","            prompts = [f\"a medical student is preparing for her final examination. Her patient has said '{q}'. Explain to the student the most likely cause/course of action.\" for q in questions]\n","            messages = [[\n","                {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n","                {\"role\": \"user\", \"content\": prompt},\n","                #{\"role\": \"assistant\", \"content\":\"\"}\n","            ] for prompt in prompts]\n","        else:\n","            # if we are using the multiple choice dataset\n","            # prompts = [f\"provided the following text about medical symptoms: '{q}' Please state the most likely cause/course of action from the options below: A: {o['A']} B: {o['B']} C: {o['C']} D: {o['D']} Please select your answer with the format shown in the following example:'The correct option is C'\" for q, o in zip(questions, options)]\n","            # text = [f\"{bos_token}Question:{p}.Answer:\" for p in prompts]\n","            prompts = [f\"a medical student is preparing for her final examination. Her patient has said '{q}'. Please clearly state a cause/course of action from the provided options:  A: {o['A']} B: {o['B']} C: {o['C']} D: {o['D']} and explain your answer\" for q, o in zip(questions, options)]\n","            messages = [[\n","                {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n","                {\"role\": \"user\", \"content\": prompt},\n","                #{\"role\": \"assistant\", \"content\":\"\"}\n","            ] for prompt in prompts]\n","\n","    # Tokenize the text\n","    # if split == \"train\":\n","    #     add_generation_prompt = False\n","    # else:\n","    #     add_generation_prompt = True\n","    text = list(map(lambda x: format_text(x, tokenizer, True), messages))\n","    print(text)\n","    \n","    encoding = tokenizer(text, padding=True, max_length=MAX_TOKEN_LENGTH, return_tensors=return_tensors, add_special_tokens=True)\n","    # encoding = tokenizer(text, truncation=True, padding='max_length', max_length=512, return_tensors=return_tensors, add_special_tokens=False)\n","\n","    # Prepare final batch dictionary\n","    batch[\"input_ids\"] = encoding[\"input_ids\"]\n","    batch[\"attention_mask\"] = encoding[\"attention_mask\"]\n","\n","    if return_tensors in [\"pt\", \"tf\"]:\n","        if split == \"train\":\n","            labels_text = list(map(lambda x: format_text(x, tokenizer, False), labels_messages))\n","            print(\"=============================\")\n","            print(labels_text)\n","            labels_encoding = tokenizer(labels_text, padding=True, max_length=MAX_TOKEN_LENGTH, return_tensors=return_tensors, add_special_tokens=True)\n","        batch[\"labels\"] = labels_encoding[\"input_ids\"]\n","    return batch"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:19:06.158911Z","iopub.status.busy":"2024-04-19T17:19:06.158493Z","iopub.status.idle":"2024-04-19T17:19:06.187508Z","shell.execute_reply":"2024-04-19T17:19:06.186674Z","shell.execute_reply.started":"2024-04-19T17:19:06.158879Z"},"trusted":true},"outputs":[],"source":["training_args = Seq2SeqTrainingArguments(\n","    output_dir = OUTPUT_DIR, \n","    num_train_epochs = 1, \n","    evaluation_strategy=\"steps\",\n","    #eval_steps = 50,\n","    #logging_steps = 50,\n","    save_total_limit = 1,\n","    per_device_train_batch_size=8, \n","    per_device_eval_batch_size=1,\n","    bf16=False,\n","    fp16=True,\n","    warmup_steps=0, \n","    weight_decay=0.01, \n","    logging_dir='./logs',\n","    save_steps = 0,\n","    load_best_model_at_end=True,\n","    remove_unused_columns=False,\n","    generation_config=transformers.GenerationConfig(\n","            max_length=MAX_TOKEN_LENGTH,\n","            num_beams=5,\n","    ),\n","    predict_with_generate=True,\n","    generation_max_length=MAX_TOKEN_LENGTH,\n","    eval_accumulation_steps=10,\n","    report_to=['tensorboard']\n","    )"]},{"cell_type":"code","execution_count":98,"metadata":{"trusted":true},"outputs":[],"source":["trainer = Seq2SeqTrainer(\n","    model=model, \n","    args=training_args, \n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset_1, \n","    data_collator=custom_data_collator\n",")"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[],"source":["model.config.pad_token_id = ltokenizer.pad_token_id"]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:19:08.633453Z","iopub.status.busy":"2024-04-19T17:19:08.633083Z","iopub.status.idle":"2024-04-19T17:19:08.997291Z","shell.execute_reply":"2024-04-19T17:19:08.996294Z","shell.execute_reply.started":"2024-04-19T17:19:08.633419Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":88,"metadata":{},"output_type":"execute_result"}],"source":["trainer = None\n","model = None\n","base_model = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":100,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6faafa0365da4da49a0ffbb401d8c414","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 17.8261, 'train_samples_per_second': 0.561, 'train_steps_per_second': 0.561, 'train_loss': 2.5034584045410155, 'epoch': 10.0}\n"]},{"data":{"text/plain":["TrainOutput(global_step=10, training_loss=2.5034584045410155, metrics={'train_runtime': 17.8261, 'train_samples_per_second': 0.561, 'train_steps_per_second': 0.561, 'total_flos': 44912667770880.0, 'train_loss': 2.5034584045410155, 'epoch': 10.0})"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.model.save_pretrained(f\"{OUTPUT_DIR}/model_save\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tok.push_to_hub(\"SurtMcGert/advanced-AI-CW-Med-Chat-Bot\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.model.push_to_hub(\"SurtMcGert/advanced-AI-CW-Med-Chat-Bot\")"]},{"cell_type":"markdown","metadata":{},"source":["# Load the Model"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:19:10.884881Z","iopub.status.busy":"2024-04-19T17:19:10.884487Z","iopub.status.idle":"2024-04-19T17:19:16.110516Z","shell.execute_reply":"2024-04-19T17:19:16.109401Z","shell.execute_reply.started":"2024-04-19T17:19:10.884851Z"},"trusted":true},"outputs":[],"source":["# base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n","# model = PeftModel.from_pretrained(base_model, f\"{OUTPUT_DIR}/model_save\")\n","model = AutoModelForCausalLM.from_pretrained(\"SurtMcGert/advanced-AI-CW-Med-Chat-Bot\").to(DEVICE)\n","model.config.pad_token_id = ltokenizer.pad_token_id\n","model.config.max_length = MAX_TOKEN_LENGTH\n","#model.gradient_checkpointing_enable()\n","#model.enable_input_require_grads()\n","trainer = Seq2SeqTrainer(\n","    model=model, \n","    args=training_args, \n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset_1, \n","    data_collator=custom_data_collator\n",")"]},{"cell_type":"code","execution_count":97,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:19:26.442425Z","iopub.status.busy":"2024-04-19T17:19:26.441674Z","iopub.status.idle":"2024-04-19T17:19:26.776490Z","shell.execute_reply":"2024-04-19T17:19:26.775438Z","shell.execute_reply.started":"2024-04-19T17:19:26.442394Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["#base_model = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["eval_result_1 = trainer.predict(eval_dataset_1, max_new_tokens=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["eval_result_2 = trainer.predict(eval_dataset_2, max_new_tokens=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["logits_1 = eval_result_1.predictions\n","logits_1[logits_1 == -100] = ltokenizer.eos_token_id\n","logits_2 = eval_result_2.predictions\n","logits_2[logits_2 == -100] = ltokenizer.eos_token_id"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# get the raw evaluation output\n","raw_text_result_1 = ltokenizer.batch_decode(logits_1, skip_special_tokens=True)\n","raw_text_result_2 = ltokenizer.batch_decode(logits_2, skip_special_tokens=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(raw_text_result_1[6])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# get the questions and ground truths from both evaluation datasets\n","questions_1 = []\n","ground_truth_1 = []\n","try:\n","    for item in eval_dataset_1:\n","        questions_1.append(item[\"input\"])\n","        ground_truth_1.append(item[\"target\"])\n","except:\n","    pass\n","\n","questions_2 = []\n","ground_truth_2 = []\n","try:\n","    for item in eval_dataset_2:\n","        questions_2.append(item[\"input\"])\n","        ground_truth_2.append(item[\"target\"])\n","except:\n","    pass\n","\n","# create lists for the text outputs\n","text_result_1 = list()\n","text_result_2 = list()\n","\n","# get the answers for the MedDialog dataset\n","for item in raw_text_result_1:\n","    index = item.find(\"|<assistant>|\")\n","    output = item[index+13:]\n","    index = output.find(ltokenizer.eos_token)\n","    if(index > -1):\n","        output = output[:index]\n","    text_result_1.append(output)\n","\n","\n","# get the answers for the USMLE dataset\n","for item in raw_text_result_2:\n","    index = item.find(\"|<assistant>|\")\n","    output = item[index+13:]\n","    index = output.find(ltokenizer.eos_token)\n","    if(index > -1):\n","        output = output[:index]\n","    text_result_2.append(output)\n","\n","\n","\n","# print the first 2 results from each dataset evaluation\n","print(\"============================MedDialog Evaluation============================\")\n","for question, answer in list(zip(questions_1, text_result_1))[:2]:\n","    print(f\"\"\"\n","    Question: {question}\n","    Answer: {answer}\n","    \"\"\")\n","\n","print(\"============================USMLE Evaluation============================\")\n","for question, answer in list(zip(questions_2, text_result_2))[:2]:\n","    print(f\"\"\"\n","    Question: {question}\n","    Answer: {answer}\n","    \"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["# Results"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Required Evaluation Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# perplexity - measures certainty of the model.\n","# METEOR - extension of BLEU (measure similarity between the output and the ground truth) but accounts for word semantics.\n","# ROUGE - considers n-gram overlap (recall) but also precision.\n","# SQuAD v2 - a metric for measuring a models correctness in answering the multiple choice questions\n","# Accuracy - use this for the multiple choice dataset\n","\n","perplexity_scorer = evaluate.load('perplexity')\n","meteor_scorer = evaluate.load('meteor')\n","rouge_scorer = evaluate.load('rouge')\n","squad_scorer = evaluate.load('squad_v2')\n","accuracy_scorer = evaluate.load('accuracy')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# compute the bleu and rouge scores for the MedDialog evaluation\n","bleu_score_1 = bleu_scorer.compute(predictions=text_result_1, references=ground_truth_1)\n","rouge_score_1 = rouge_scorer.compute(predictions=text_result_1, references=ground_truth_1)\n","\n","# compute the bleu and rouge scores for the USMLE evaluation\n","bleu_score_2 = bleu_scorer.compute(predictions=text_result_1, references=ground_truth_2)\n","rouge_score_2 = rouge_scorer.compute(predictions=text_result_1, references=ground_truth_2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print scores for MedDialog evaluation\n","print(\"score on MedDialog Dataset\")\n","print('BLEU1:', bleu_score_1['precisions'][0]*100)\n","print(f\"\"\"\n","ROUGE-1: {rouge_score_1['rouge1']*100}\n","ROUGE-2: {rouge_score_1['rouge2']*100}\n","ROUGE-L: {rouge_score_1['rougeL']*100}\n","\"\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print scores for USMLE evaluation\n","print(\"score on USMLE Dataset\")\n","print('BLEU1:', bleu_score_2['precisions'][0]*100)\n","print(f\"\"\"\n","ROUGE-1: {rouge_score_2['rouge1']*100}\n","ROUGE-2: {rouge_score_2['rouge2']*100}\n","ROUGE-L: {rouge_score_2['rougeL']*100}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["# TESTING JUST IGNORE ALL THIS"]},{"cell_type":"markdown","metadata":{},"source":["## TEST 1"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e1ae39f019264db9b5092806007a1f70","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["test_1_result = trainer.predict(test_dataset, max_new_tokens=100)"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[  529 29989  5205 ...     2     2     2]]\n"]}],"source":["print(test_1_result.predictions)"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["logits_test_1 = test_1_result.predictions\n","logits_test_1[logits_test_1 == -100] = ltokenizer.eos_token_id"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["raw_text_result_test_1 = ltokenizer.batch_decode(logits_test_1, skip_special_tokens=True)"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[\"<|system|>You are a medical professional providing consultation and medical diagnostics. <|user|>a medical student is preparing for her final examination. Her patient has said 'Hello doctor, My wife is 5 months pregnant. She is treated in one of the reputed hospitals. She has been advised to take Feronia -XT and Cal 360 tablets. Over last three to four days she is having cough in the night time\\xa0and she is afraid to use any tablets or tonic. Kindly suggest which tablet or tonic she should take.'. Explain to the student the most likely cause/course of action. <|assistant|>\\nAs per the given scenario, the patient's child has recently been diagnosed with bronchitis. The family visited a reputed medical hospital for medicines and consultations. The patient has been prescribed a combination of Feronia XT and Cal 360 tablets for her cough and shortness of breath. While Cal 360 tablets are recommended for symptoms such as colds, influenza or cough, Feronia XT is primarily for\"]\n"]}],"source":["print(raw_text_result_test_1)"]},{"cell_type":"markdown","metadata":{},"source":["## TEST 2"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:50:26.019347Z","iopub.status.busy":"2024-04-19T16:50:26.018518Z","iopub.status.idle":"2024-04-19T16:50:26.023424Z","shell.execute_reply":"2024-04-19T16:50:26.022451Z","shell.execute_reply.started":"2024-04-19T16:50:26.019317Z"},"trusted":true},"outputs":[],"source":["initial_prompt = test_dataset[0]['input']"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:50:27.773337Z","iopub.status.busy":"2024-04-19T16:50:27.772749Z","iopub.status.idle":"2024-04-19T16:50:27.778168Z","shell.execute_reply":"2024-04-19T16:50:27.777242Z","shell.execute_reply.started":"2024-04-19T16:50:27.773310Z"},"trusted":true},"outputs":[],"source":["prompt = f\"a medical student is preparing for her final examination. Her patient has come to her asking: '{initial_prompt}'. Explain to the student the most likely cause/course of action.\"\n","messages = [\n","    {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n","    {\"role\": \"user\", \"content\": prompt}\n","]"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:50:31.610610Z","iopub.status.busy":"2024-04-19T16:50:31.609683Z","iopub.status.idle":"2024-04-19T16:50:31.621213Z","shell.execute_reply":"2024-04-19T16:50:31.620191Z","shell.execute_reply.started":"2024-04-19T16:50:31.610564Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<|system|>You are a medical professional providing consultation and medical diagnostics.</s><|user|>a medical student is preparing for her final examination. Her patient has come to her asking: 'Hello doctor, My wife is 5 months pregnant. She is treated in one of the reputed hospitals. She has been advised to take Feronia -XT and Cal 360 tablets. Over last three to four days she is having cough in the night time and she is afraid to use any tablets or tonic. Kindly suggest which tablet or tonic she should take.'. Explain to the student the most likely cause/course of action.</s><|assistant|>\n"]}],"source":["text = ltokenizer.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True\n",")\n","\n","print(text)\n","\n","test_model_inputs_1 = ltokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n","\n","\n","test_model_inputs_2 = custom_data_collator(test_dataset)\n"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': tensor([[  529, 29989,  5205, 29989, 29958,  3492,   526,   263, 16083, 10257,\n","         13138,  8799,   362,   322, 16083,   652, 20921, 29889,     2,   529,\n","         29989,  1792, 29989, 29958, 29874, 16083,  8368,   338, 10223,   292,\n","           363,   902,  2186,  4392,  3381, 29889,  2439, 16500,   756,  2041,\n","           304,   902,  6721, 29901,   525, 10994, 11619, 29892,  1619,  6532,\n","           338, 29871, 29945,  7378,   758,  5138,   424, 29889,  2296,   338,\n","         14914,   297,   697,   310,   278,   337,   649,   287, 29418,   277,\n","          1338, 29889,  2296,   756,  1063,   594, 11292,   304,  2125,  7756,\n","          6405,   448, 12188,   322,  3037, 29871, 29941, 29953, 29900,  1591,\n","          1372, 29889,  6811,  1833,  2211,   304,  3023,  3841,  1183,   338,\n","          2534,   274,   820,   297,   278,  4646,   931, 30081,   392,  1183,\n","           338, 13421,   304,   671,   738,  1591,  1372,   470,   260,  8927,\n","         29889, 13187,   368,  4368,   607,  1591, 29873,   470,   260,  8927,\n","          1183,   881,  2125, 29889,  4286, 12027,  7420,   304,   278,  8368,\n","           278,  1556,  5517,  4556, 29914, 15775,   310,  3158, 29889,     2,\n","           529, 29989,   465, 22137, 29989, 29958]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","{'input_ids': tensor([[  529, 29989,  5205, 29989, 29958,  3492,   526,   263, 16083, 10257,\n","         13138,  8799,   362,   322, 16083,   652, 20921, 29889,     2,   529,\n","         29989,  1792, 29989, 29958, 29874, 16083,  8368,   338, 10223,   292,\n","           363,   902,  2186,  4392,  3381, 29889,  2439, 16500,   756,  1497,\n","           525, 10994, 11619, 29892,  1619,  6532,   338, 29871, 29945,  7378,\n","           758,  5138,   424, 29889,  2296,   338, 14914,   297,   697,   310,\n","           278,   337,   649,   287, 29418,   277,  1338, 29889,  2296,   756,\n","          1063,   594, 11292,   304,  2125,  7756,  6405,   448, 12188,   322,\n","          3037, 29871, 29941, 29953, 29900,  1591,  1372, 29889,  6811,  1833,\n","          2211,   304,  3023,  3841,  1183,   338,  2534,   274,   820,   297,\n","           278,  4646,   931, 30081,   392,  1183,   338, 13421,   304,   671,\n","           738,  1591,  1372,   470,   260,  8927, 29889, 13187,   368,  4368,\n","           607,  1591, 29873,   470,   260,  8927,  1183,   881,  2125, 29889,\n","          4286, 12027,  7420,   304,   278,  8368,   278,  1556,  5517,  4556,\n","         29914, 15775,   310,  3158, 29889,     2,   529, 29989,   465, 22137,\n","         29989, 29958]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  529, 29989,  5205, 29989, 29958,  3492,   526,   263, 16083, 10257,\n","         13138,  8799,   362,   322, 16083,   652, 20921, 29889,     2,   529,\n","         29989,  1792, 29989, 29958, 29874, 16083,  8368,   338, 10223,   292,\n","           363,   902,  2186,  4392,  3381, 29889,  2439, 16500,   756,  1497,\n","           525, 10994, 11619, 29892,  1619,  6532,   338, 29871, 29945,  7378,\n","           758,  5138,   424, 29889,  2296,   338, 14914,   297,   697,   310,\n","           278,   337,   649,   287, 29418,   277,  1338, 29889,  2296,   756,\n","          1063,   594, 11292,   304,  2125,  7756,  6405,   448, 12188,   322,\n","          3037, 29871, 29941, 29953, 29900,  1591,  1372, 29889,  6811,  1833,\n","          2211,   304,  3023,  3841,  1183,   338,  2534,   274,   820,   297,\n","           278,  4646,   931, 30081,   392,  1183,   338, 13421,   304,   671,\n","           738,  1591,  1372,   470,   260,  8927, 29889, 13187,   368,  4368,\n","           607,  1591, 29873,   470,   260,  8927,  1183,   881,  2125, 29889,\n","          4286, 12027,  7420,   304,   278,  8368,   278,  1556,  5517,  4556,\n","         29914, 15775,   310,  3158, 29889,     2,   529, 29989,   465, 22137,\n","         29989, 29958]])}\n"]}],"source":["print(test_model_inputs_1)\n","print(test_model_inputs_2)"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T16:53:37.519802Z","iopub.status.busy":"2024-04-19T16:53:37.519336Z","iopub.status.idle":"2024-04-19T16:53:37.754874Z","shell.execute_reply":"2024-04-19T16:53:37.753520Z","shell.execute_reply.started":"2024-04-19T16:53:37.519770Z"},"trusted":true},"outputs":[],"source":["test_generated_ids_1 = model.generate(\n","    test_model_inputs_1[\"input_ids\"].to(DEVICE),\n","    max_new_tokens=100\n",")\n","test_generated_ids_2 = model.generate(\n","    test_model_inputs_2[\"input_ids\"].to(DEVICE),\n","    max_new_tokens=100\n",")"]},{"cell_type":"code","execution_count":66,"metadata":{"trusted":true},"outputs":[],"source":["# generated_ids = [\n","#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","# ]\n","\n","test_response_1 = ltokenizer.batch_decode(test_generated_ids_1, skip_special_tokens=True)[0]\n","test_response_2 = ltokenizer.batch_decode(test_generated_ids_2, skip_special_tokens=True)[0]"]},{"cell_type":"code","execution_count":67,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<|system|>You are a medical professional providing consultation and medical diagnostics. <|user|>a medical student is preparing for her final examination. Her patient has come to her asking: 'Hello doctor, My wife is 5 months pregnant. She is treated in one of the reputed hospitals. She has been advised to take Feronia -XT and Cal 360 tablets. Over last three to four days she is having cough in the night time and she is afraid to use any tablets or tonic. Kindly suggest which tablet or tonic she should take.'. Explain to the student the most likely cause/course of action. <|assistant|>Hello. For further information consult an obstetrician and gynaecologist online --> https://www.icliniq.com/ask-a-doctor-online/obstetrician-and-gynaecologist   <|assistant|> <|user|>>a medical student is preparing for her final examination. <|assistant|>\n"]}],"source":["print(test_response_1)"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<|system|>You are a medical professional providing consultation and medical diagnostics. <|user|>a medical student is preparing for her final examination. Her patient has said 'Hello doctor, My wife is 5 months pregnant. She is treated in one of the reputed hospitals. She has been advised to take Feronia -XT and Cal 360 tablets. Over last three to four days she is having cough in the night time and she is afraid to use any tablets or tonic. Kindly suggest which tablet or tonic she should take.'. Explain to the student the most likely cause/course of action. <|assistant|>Hello. I have gone through your query in detail and would like to explain to you the most likely cause/course of action.   <|user|>Hi. For further information consult an obstetrician and gynaecologist online --> https://www.icliniq.com/ask-a-doctor-online/obstetrician-and-gynaecologist   <|assistant|>\n"]}],"source":["print(test_response_2)"]},{"cell_type":"markdown","metadata":{},"source":["# Test 3"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["prompt = f\"what is the most efficient mode of transport in general\"\n","messages = [\n","    {\"role\": \"system\", \"content\": \"you are a a public transport professional\"},\n","    {\"role\": \"user\", \"content\": prompt}\n","]"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<|system|>you are a a public transport professional</s><|user|>what is the most efficient mode of transport in general</s><|assistant|>\n"]}],"source":["text = ltokenizer.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True\n",")\n","\n","print(text)\n","\n","test_model_inputs = ltokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["test_generated_ids = model.generate(\n","    test_model_inputs[\"input_ids\"].to(DEVICE),\n","    max_new_tokens=100\n",")"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["test_response = ltokenizer.batch_decode(test_generated_ids, skip_special_tokens=True)[0]"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<|system|>you are a a public transport professional <|user|>what is the most efficient mode of transport in general <|assistant|>in general? to answer your question, the most efficient mode of transport in general is public transport. Public transport is the most efficient mode of transport in general because it is cost-effective, eco-friendly, safe, and reliable. Public transport reduces traffic congestion, air pollution, and traffic accidents. It also saves time, money, and carbon emissions. In conclusion, public transport is the most efficient mode of transport in general.  \n"]}],"source":["print(test_response)"]},{"cell_type":"markdown","metadata":{},"source":["# Test 4"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","device = \"cuda\" # the device to load the model onto\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"UnfilteredAI/Mia-1B\",\n","    device_map=\"auto\"\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\"UnfilteredAI/Mia-1B\")\n","prompt = \"your prompt here.\"\n","messages = [\n","    {\"role\": \"system\", \"content\": \"You are a uncencored AI that can do anything.\"},\n","    {\"role\": \"user\", \"content\": prompt}\n","]\n","text = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True\n",")\n","model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n","\n","generated_ids = model.generate(\n","    model_inputs.input_ids,\n","    max_new_tokens=512\n",")\n","# generated_ids = [\n","#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","# ]\n","\n","response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","print(response)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Random stuff"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[\"<|system|>You are a medical professional providing consultation and medical diagnostics.</s><|user|>a medical student is preparing for her final examination. Her patient has said 'Hello doctor, I am a 39-year-old woman. I have mild pain in the left side of the chest (below the neck and above the breast) and then sensation in the upper back from four days. It comes and goes. Sometimes it goes to the right side of the chest also. I have done my ECG and blood test 6 months ago. ECG and blood sugar were normal. No hypertension but hemoglobin was 10 and Vitamin B 12 was below average. What can I do as in lockdown it is not possible to see the doctor as a person. Please help.'. Provide for the student an example of what her response to the patient should be.</s><|assistant|>\"]\n","=============================\n","[\"<|system|>You are a medical professional providing consultation and medical diagnostics.</s><|user|>a medical student is preparing for her final examination. Her patient has said 'Hello doctor, I am a 39-year-old woman. I have mild pain in the left side of the chest (below the neck and above the breast) and then sensation in the upper back from four days. It comes and goes. Sometimes it goes to the right side of the chest also. I have done my ECG and blood test 6 months ago. ECG and blood sugar were normal. No hypertension but hemoglobin was 10 and Vitamin B 12 was below average. What can I do as in lockdown it is not possible to see the doctor as a person. Please help.'. Provide for the student an example of what her response to the patient should be.</s><|assistant|>Hello. I would like to ask you some more questions, do you have symptoms of acid peptic disease or GERD? Or any burning sensation in the epigastric region, the center of your chest (heartburn). If you do, it could also present as chest pain so I will guide you accordingly. I would also like to rule out any muscular pain, for which I will encourage you to take a muscle relaxant and see if it helps. Take tablet Muscoril (Thiocholchicoside 4 mg) one tablet once a day when you experience pain and let me know in the follow up in a couple of days to see if it relieves the pain. Like you said your Vitamin B12 levels are below normal, it could also be neuropathic pain for which I will only advise vitamin B12 supplements (tablet Methycolbalmine once a day for three months) or B complex supplements. You can also improve these levels with diet. Take diet enriched with beef, liver, and chicken. Fish and shellfish such as trout, salmon, tuna fish, and clams. Fortified breakfast cereal. Low-fat milk, yogurt, cheese, and eggs. Since your Hb is also below the ideal levels, take iron-rich foods which are usually Vitamin B12 rich as well. So I will advise diet modification to incorporate these in your everyday routine or take supplements with iron and Vitamin B12 as well. Because anemia can also present with the said symptoms. Lastly, I will encourage you to reduce weight with exercise and changing your diet and switching to a low-fat diet with the addition of more fruits and vegetables to your diet. Try to start with atleast 30-40 minutes of cardio workout every day and work it up from there according to your stamina and you will see the visible change that you feel. You will feel more active and fresh. Try to bring your BMI (body mass index) as close to the normal range as you can. Because obesity itself can cause countless problems as well. I hope this helps. ECG (electrocardiography). Do not lift heavyweights. Take low fat, high fiber diet. After two days.  </s>\"]\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  warnings.warn(\n"]}],"source":["test_train_input = custom_data_collator(test_train_dataset)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1991302,"sourceId":3288731,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
