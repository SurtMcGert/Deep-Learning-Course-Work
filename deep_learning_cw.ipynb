{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3288731,"sourceType":"datasetVersion","datasetId":1991302}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# %pip install -q evaluate\n# %pip install -q opendatasets\n# %pip install -q --upgrade accelerate\n# %pip install -q --upgrade transformers\n# %pip install -q peft\n# %pip install -q --upgrade bitsandbytes\n# %pip install -q accelerate\n# %pip install -q trl\n# %pip install -q nltk\n# %pip install -q -U nltk\n# %pip install -q rouge_score\n# %pip install -q bert_score","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:48:06.265338Z","iopub.execute_input":"2024-04-26T15:48:06.265711Z","iopub.status.idle":"2024-04-26T15:48:18.753889Z","shell.execute_reply.started":"2024-04-26T15:48:06.265680Z","shell.execute_reply":"2024-04-26T15:48:18.752703Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.1.2)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.1.4)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.40.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.31.0)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.66.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.22.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.4.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2024.2.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd \nimport torch\nimport torch.nn as nn\ntorch.cuda.set_per_process_memory_fraction(0.9)\ntorch.backends.cuda.matmul.allow_tf32 = True\nimport torchtext\nfrom torch.utils.data import Dataset, random_split\nfrom typing import List, Dict, Union\nfrom typing import Any, TypeVar\nimport pandas as pd\nimport os\nimport copy\nimport gc\nimport evaluate\nimport opendatasets as od\nfrom huggingface_hub import login\nfrom typing import Optional, Tuple, Union\nimport statistics\n\nfrom datasets import load_dataset, Features, Value\nfrom datasets import Dataset\nimport accelerate\n\nfrom peft import LoftQConfig, LoraConfig, get_peft_model, PeftModel\n\nimport transformers\nfrom transformers.modeling_outputs import QuestionAnsweringModelOutput\nfrom transformers import BertLMHeadModel, AutoConfig, BitsAndBytesConfig,Conv1D\nfrom transformers import AutoTokenizer, Seq2SeqTrainingArguments \nfrom transformers import Seq2SeqTrainer, AutoModelForCausalLM, IntervalStrategy, AutoModelForQuestionAnswering\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-04-26T16:39:44.588509Z","iopub.execute_input":"2024-04-26T16:39:44.588907Z","iopub.status.idle":"2024-04-26T16:39:44.602306Z","shell.execute_reply.started":"2024-04-26T16:39:44.588875Z","shell.execute_reply":"2024-04-26T16:39:44.601219Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"set a seed and confirm CUDA support","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(2137)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.deterministic = True\n\nprint(\"PyTorch Version: \", torch.__version__)\nprint(\"torchtext Version: \", torchtext.__version__)\nprint(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:21:40.266744Z","iopub.execute_input":"2024-04-26T15:21:40.267448Z","iopub.status.idle":"2024-04-26T15:21:40.283904Z","shell.execute_reply.started":"2024-04-26T15:21:40.267413Z","shell.execute_reply":"2024-04-26T15:21:40.282932Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"PyTorch Version:  2.1.2\ntorchtext Version:  0.16.2\nUsing GPU.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dataset Download","metadata":{}},{"cell_type":"markdown","source":"## Downloading MedDialog Dataset","metadata":{}},{"cell_type":"markdown","source":"NOTE: you will need a kaggle API key for the following to work","metadata":{}},{"cell_type":"code","source":"import json\n\n# Path to JSON file\njson_file_path = \"kaggle.json\"\n\n# Open the file and read the content\ntry:\n  with open(json_file_path, \"r\") as f:\n    json_data = json.load(f)\nexcept FileNotFoundError:\n  print(f\"Error: JSON file not found at {json_file_path}\")\n  exit(1)\n\n# Access username and key from the JSON data\ntry:\n  username = json_data[\"username\"]\n  key = json_data[\"key\"]\nexcept KeyError:\n  print(\"Error: 'username' or 'key' key not found in JSON data\")\n  exit(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['KAGGLE_USERNAME'] = username\nos.environ['KAGGLE_KEY'] = key\n\n# Assign the Kaggle data set URL into variable\ndataset = 'https://www.kaggle.com/datasets/dsxavier/diagnoise-me'\n# Using opendatasets let's download the data sets\nod.download(dataset, \"dataset\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Downloading USMLE Dataset","metadata":{}},{"cell_type":"code","source":"USMLE_dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options\", split=\"test\")","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:21:45.529891Z","iopub.execute_input":"2024-04-26T15:21:45.530862Z","iopub.status.idle":"2024-04-26T15:21:47.239360Z","shell.execute_reply.started":"2024-04-26T15:21:45.530823Z","shell.execute_reply":"2024-04-26T15:21:47.238555Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(USMLE_dataset[0])\nprint(len(USMLE_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:21:48.116591Z","iopub.execute_input":"2024-04-26T15:21:48.116985Z","iopub.status.idle":"2024-04-26T15:21:48.122747Z","shell.execute_reply.started":"2024-04-26T15:21:48.116952Z","shell.execute_reply":"2024-04-26T15:21:48.121820Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{'question': 'A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?', 'answer': 'Tell the attending that he cannot fail to disclose this mistake', 'options': {'A': 'Disclose the error to the patient and put it in the operative report', 'B': 'Tell the attending that he cannot fail to disclose this mistake', 'C': 'Report the physician to the ethics committee', 'D': 'Refuse to dictate the operative report'}, 'meta_info': 'step1', 'answer_idx': 'B', 'metamap_phrases': ['junior orthopaedic surgery resident', 'completing', 'carpal tunnel repair', 'department chairman', 'attending physician', 'case', 'resident', 'cuts', 'flexor tendon', 'tendon', 'repaired', 'complication', 'attending', 'resident', 'patient', 'fine', 'need to report', 'minor complication', 'not', 'patient', 'not', 'to make', 'patient worry', 'resident to leave', 'complication out', 'operative report', 'following', 'correct next action', 'resident to take']}\n1273\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Datasets","metadata":{}},{"cell_type":"markdown","source":"## Loading MedDialog Dataset","metadata":{}},{"cell_type":"code","source":"is_kaggle = (\n    \"KAGGLE_CLOUD\" in os.environ or \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n)\nif is_kaggle:\n    DATA_PATH = \"/kaggle/input/diagnoise-me/diagnose_en_dataset.feather\"\nelse:\n    DATA_PATH = \"dataset\\\\diagnoise-me\\\\diagnose_en_dataset.feather\"\n\nSEQ_LEN: int = 1024\ndata = pd.read_feather(DATA_PATH)\nSAMPLE_SIZE: int =  int(data.shape[0] * 0.015) #get 1% of the data\ndata = data[:SAMPLE_SIZE]\nprint(data.keys())\nprint(len(data))","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:21:50.926012Z","iopub.execute_input":"2024-04-26T15:21:50.926376Z","iopub.status.idle":"2024-04-26T15:21:51.690935Z","shell.execute_reply.started":"2024-04-26T15:21:50.926344Z","shell.execute_reply":"2024-04-26T15:21:51.689607Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Index(['id', 'Description', 'Doctor', 'Patient'], dtype='object')\n3862\n","output_type":"stream"}]},{"cell_type":"code","source":"# Split data into train and eval sets with 70% for training\ntrain_data, eval_data = train_test_split(data, test_size=0.3, random_state=42)\n\ntrain_data = train_data.reset_index(drop=True)\neval_data = eval_data.reset_index(drop=True)\n\n# Print the shapes of the train and eval sets\nprint(\"Train data shape:\", train_data.shape)\nprint(\"Eval data shape:\", eval_data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:21:59.420279Z","iopub.execute_input":"2024-04-26T15:21:59.420645Z","iopub.status.idle":"2024-04-26T15:21:59.431123Z","shell.execute_reply.started":"2024-04-26T15:21:59.420614Z","shell.execute_reply":"2024-04-26T15:21:59.430094Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Train data shape: (2703, 4)\nEval data shape: (1159, 4)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Loading USMLE Dataset","metadata":{}},{"cell_type":"code","source":"USMLE_dataset = pd.DataFrame({'Doctor': USMLE_dataset[\"answer\"], 'Patient': USMLE_dataset[\"question\"], 'Options':USMLE_dataset[\"options\"]})\n# Print the shapes of the set\nprint(\"USMLELiveEQA data shape:\", USMLE_dataset.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:22:02.551540Z","iopub.execute_input":"2024-04-26T15:22:02.551951Z","iopub.status.idle":"2024-04-26T15:22:02.581581Z","shell.execute_reply.started":"2024-04-26T15:22:02.551919Z","shell.execute_reply":"2024-04-26T15:22:02.580612Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"USMLELiveEQA data shape: (1273, 3)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Create an output directory","metadata":{}},{"cell_type":"code","source":"os.makedirs('./results', exist_ok = True)\nOUTPUT_DIR: str = './results'","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:22:06.329005Z","iopub.execute_input":"2024-04-26T15:22:06.329727Z","iopub.status.idle":"2024-04-26T15:22:06.334209Z","shell.execute_reply.started":"2024-04-26T15:22:06.329694Z","shell.execute_reply":"2024-04-26T15:22:06.333160Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# tokens for the datset\nMODEL_NAME: str = 'UnfilteredAI/Mia-1B'","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:22:07.784655Z","iopub.execute_input":"2024-04-26T15:22:07.785633Z","iopub.status.idle":"2024-04-26T15:22:07.789562Z","shell.execute_reply.started":"2024-04-26T15:22:07.785591Z","shell.execute_reply":"2024-04-26T15:22:07.788578Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Load tokenizer \nMAX_TOKEN_LENGTH = 1024\n\n# for evaluation\nltokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nltokenizer.padding_side = 'left'\nltokenizer.truncation_side = 'left'\n\n# for training\nrtokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nrtokenizer.padding_side = 'right'\nrtokenizer.truncation_side = 'right'","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:22:09.192892Z","iopub.execute_input":"2024-04-26T15:22:09.193295Z","iopub.status.idle":"2024-04-26T15:22:09.758668Z","shell.execute_reply.started":"2024-04-26T15:22:09.193264Z","shell.execute_reply":"2024-04-26T15:22:09.757771Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n#base_model.resize_token_embeddings(len(rtokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:22:57.551840Z","iopub.execute_input":"2024-04-26T15:22:57.552249Z","iopub.status.idle":"2024-04-26T15:23:00.256205Z","shell.execute_reply.started":"2024-04-26T15:22:57.552221Z","shell.execute_reply":"2024-04-26T15:23:00.255244Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(base_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lora_config = LoraConfig(\n    lora_alpha=16, # lora alpha for scaling\n    r=16, # rank\n    lora_dropout=0.05, #dropout\n    use_rslora=True, #  sets the adapter scaling factor to lora_alpha/math.sqrt(r)\n    bias=\"none\", # dont train biases\n    target_modules=[\"q_proj\", \"v_proj\"],\n    task_type=\"CAUSAL_LM\",\n    #layers_to_transform=[20]\n)\n# model = get_peft_model(base_model, lora_config)\n# model.gradient_checkpointing_enable()\n# model.enable_input_require_grads()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def print_trainable_parameters(model):\n#     \"\"\"\n#     Prints the number of trainable parameters in the model.\n#     \"\"\"\n#     trainable_params = 0\n#     all_param = 0\n#     for _, param in model.named_parameters():\n#         all_param += param.numel()\n#         if param.requires_grad:\n#             trainable_params += param.numel()\n#     print(\n#         f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n#     )\n#     return {\"trainable\": trainable_params, \"all\": all_param, \"trainable%\": 100 * trainable_params / all_param}\n\n# print_trainable_parameters(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Data for Training","metadata":{}},{"cell_type":"markdown","source":"## Custom Dataset","metadata":{}},{"cell_type":"code","source":"# class DoctorPatientDataset(Dataset):\n    \n#     def __init__(self, data, split):\n        \n#         self.input_x: List = data[\"Patient\"]\n#         self.input_x = self.input_x.reset_index(drop=True)\n#         self.target: List = data[\"Doctor\"]\n#         self.target = self.target.reset_index(drop=True)\n#         self.split = split\n\n#         try:\n#             self.options: List = data[\"Options\"]\n#         except:\n#             pass\n            \n#     def __len__(self):\n#         return len(self.input_x)\n    \n#     def __getitem__(self, idx):\n#         try:\n#             data = {\n#                 'input': self.input_x[idx],\n#                 'target': self.target[idx],\n#                 'options': self.options[idx],\n#                 'split': self.split\n#             }\n#         except:\n#             data = {\n#                 'input': self.input_x[idx],\n#                 'target': self.target[idx],\n#                 'split': self.split\n#             }\n#         return data\n\n# class DoctorPatientDataset(Dataset):\n    \n#     def __init__(self, data, split):\n        \n#         self.input_x: List = data[\"Patient\"]\n#         self.input_x = self.input_x.reset_index(drop=True)\n#         self.target: List = data[\"Doctor\"]\n#         self.target = self.target.reset_index(drop=True)\n#         self.split = split\n\n#         try:\n#             self.options: List = data[\"Options\"]\n#         except:\n#             pass\n            \n#     def __len__(self):\n#         return len(self.input_x)\n    \n#     def __getitem__(self, idx):\n#         try:\n#             data = {\n#                 'messages': [\n#                     {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n#                     {\"role\": \"user\", \"content\": f\"{self.input_x[idx]}, choose from A) {self.options[idx]['A']}, B) {self.options[idx]['B']}, C) {self.options[idx]['C']}, D)  {self.options[idx]['D']}\"},\n#                     {\"role\": \"assistant\", \"content\": self.target[idx]}\n#                 ]\n#             }\n#         except:\n#             data = {\n#                 'messages': [\n#                     {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n#                     {\"role\": \"user\", \"content\":{self.input_x[idx]}},\n#                     {\"role\": \"assistant\", \"content\": self.target[idx]}\n#                 ]\n#             }\n#         return data\n\ndef build_dataset(data):\n    listed_data = []\n    try:\n                listed_data = [[\n                        {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n                        {\"role\": \"user\", \"content\": f\"{patient}, choose from A) {options['A']}, B) {options['B']}, C) {options['C']}, D)  {options['D']}\"},\n                        {\"role\": \"assistant\", \"content\": doctor}\n                    ]for patient, doctor, options in zip(data[\"Patient\"], data[\"Doctor\"], data[\"Options\"])]\n    except:\n                listed_data =  [[\n                        {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n                        {\"role\": \"user\", \"content\":patient},\n                        {\"role\": \"assistant\", \"content\": doctor}\n                    ]for patient, doctor in zip(data[\"Patient\"], data[\"Doctor\"])]\n    dataset = {\"messages\": listed_data}\n    dataset = Dataset.from_dict(dataset)\n    return dataset\n                ","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:22:18.060160Z","iopub.execute_input":"2024-04-26T15:22:18.060814Z","iopub.status.idle":"2024-04-26T15:22:18.070998Z","shell.execute_reply.started":"2024-04-26T15:22:18.060782Z","shell.execute_reply":"2024-04-26T15:22:18.070051Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# train_dataset = DoctorPatientDataset(data = train_data, split = \"train\")\n# eval_dataset_1 = DoctorPatientDataset(data = eval_data, split = \"eval\")\n# eval_dataset_2 = DoctorPatientDataset(data = USMLE_dataset, split = \"eval\")\n\n# test_dataset = DoctorPatientDataset(data = eval_data[1:2], split = \"eval\")\n\n# test_data = [[\"what's the answer to life, the universe, and everything\", \"42\"]]\n# test_data = pd.DataFrame(test_data, columns=[\"Patient\", \"Doctor\"])\n# test_train_dataset = DoctorPatientDataset(data = test_data, split = \"train\")\n\ntrain_dataset = build_dataset(train_data)\neval_dataset_1 = build_dataset(eval_data[0:10])\neval_dataset_2 = build_dataset(USMLE_dataset[0:10])\n\ntest_dataset = build_dataset(train_data[0:1])\n\n# test_data = [[\"what's the answer to life, the universe, and everything\", \"42\"]]\n# test_data = pd.DataFrame(test_data, columns=[\"Patient\", \"Doctor\"])\ntest_train_dataset = build_dataset(train_data[0:1])","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:22:20.495948Z","iopub.execute_input":"2024-04-26T15:22:20.496600Z","iopub.status.idle":"2024-04-26T15:22:20.581772Z","shell.execute_reply.started":"2024-04-26T15:22:20.496567Z","shell.execute_reply":"2024-04-26T15:22:20.580812Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(test_train_dataset[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom Data Collator","metadata":{}},{"cell_type":"code","source":"def format_text(message, tokenizer, add_generation_prompt):\n    text = tokenizer.apply_chat_template(\n        message,\n        tokenize=False,\n        add_generation_prompt=add_generation_prompt\n    )\n    return text\n\ndef custom_data_collator(features, return_tensors=\"pt\"):\n    batch = {}\n\n    tokenizer = ltokenizer\n\n    messages = [feature['messages'][0:2] for feature in features]\n\n    text = list(map(lambda x: format_text(x, tokenizer, True), messages))\n\n#     print(text)\n    \n    encoding = tokenizer(text, padding=True, max_length=MAX_TOKEN_LENGTH, return_tensors=return_tensors, add_special_tokens=True)\n    # encoding = tokenizer(text, truncation=True, padding='max_length', max_length=512, return_tensors=return_tensors, add_special_tokens=False)\n\n    # Prepare final batch dictionary\n    batch[\"input_ids\"] = encoding[\"input_ids\"]\n    batch[\"attention_mask\"] = encoding[\"attention_mask\"]\n\n    # if return_tensors in [\"pt\", \"tf\"]:\n    #     if split == \"train\":\n    #         labels_text = list(map(lambda x: format_text(x, tokenizer, False), labels_messages))\n    #         # print(\"=============================\")\n    #         # print(labels_text)\n    #         labels_encoding = tokenizer(labels_text, padding=True, max_length=MAX_TOKEN_LENGTH, return_tensors=return_tensors, add_special_tokens=True)\n    #         batch[\"labels\"] = labels_encoding[\"input_ids\"]\n    #     else:\n    #         batch[\"labels\"] = copy.deepcopy(encoding[\"input_ids\"])\n    return batch","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:22:25.188366Z","iopub.execute_input":"2024-04-26T15:22:25.188738Z","iopub.status.idle":"2024-04-26T15:22:25.196910Z","shell.execute_reply.started":"2024-04-26T15:22:25.188706Z","shell.execute_reply":"2024-04-26T15:22:25.195817Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir = OUTPUT_DIR, \n    num_train_epochs = 1, \n    evaluation_strategy=\"steps\",\n    #eval_steps = 50,\n    #logging_steps = 50,\n    save_total_limit = 1,\n    per_device_train_batch_size=8, \n    per_device_eval_batch_size=1,\n    bf16=False,\n    fp16=True,\n    warmup_steps=0, \n    weight_decay=0.01, \n    logging_dir='./logs',\n    save_steps = 0,\n    load_best_model_at_end=True,\n    eval_accumulation_steps=10,\n    report_to=['tensorboard']\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def formatting_prompts_func(inputs):\n#     for input in inputs:\n#         try:\n#                 data = {\n#                     'messages': [\n#                         {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n#                         {\"role\": \"user\", \"content\": f\"{input['Patient']}, choose from A) {input['Options']['A']}, B) {input['Options']['B']}, C) {input['Options']['C']}, D)  {input['Options']['D']}\"},\n#                         {\"role\": \"assistant\", \"content\": input[\"Doctor\"]}\n#                     ]\n#                 }\n#         except:\n#                 data = {\n#                     'messages': [\n#                         {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n#                         {\"role\": \"user\", \"content\":input[\"Patient\"]},\n#                         {\"role\": \"assistant\", \"content\": input[\"Doctor\"]}\n#                     ]\n#                 }\n#         return data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer = Seq2SeqTrainer(\n#     model=model, \n#     args=training_args, \n#     train_dataset=train_dataset,\n#     eval_dataset=eval_dataset_1, \n#     data_collator=custom_data_collator\n# )\ntrainer = SFTTrainer(\n    model=base_model, \n    args=training_args, \n    peft_config=lora_config,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset_1, \n    max_seq_length=1024,\n    #data_collator=custom_data_collator,\n    #dataset_text_field=\"messages\",\n    packing=False\n)\ntrainer.model.gradient_checkpointing_enable()\ntrainer.model.enable_input_require_grads()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.pad_token_id = ltokenizer.pad_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = None\nmodel = None\nbase_model = None\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.save_pretrained(f\"{OUTPUT_DIR}/model_save\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok = AutoTokenizer.from_pretrained(MODEL_NAME)\ntok.push_to_hub(\"SurtMcGert/advanced-AI-CW-Med-Chat-Bot\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.push_to_hub(\"SurtMcGert/advanced-AI-CW-Med-Chat-Bot\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the Model","metadata":{}},{"cell_type":"code","source":"eval_args = Seq2SeqTrainingArguments(\n    output_dir = OUTPUT_DIR, \n    num_train_epochs = 1, \n    evaluation_strategy=\"steps\",\n    save_total_limit = 1,\n    per_device_train_batch_size=8, \n    per_device_eval_batch_size=1,\n    bf16=False,\n    fp16=True,\n    warmup_steps=0, \n    weight_decay=0.01, \n    logging_dir='./logs',\n    save_steps = 0,\n    load_best_model_at_end=True,\n    remove_unused_columns=False,\n    generation_config=transformers.GenerationConfig(\n            max_new_tokens=100, # the max number of new tokens to generate\n            early_stopping=True, # stop when unlikely to find better candidates\n            repetition_penalty = 1.5, # ads a penalty for repetition\n            num_beams=25, # num of beams\n            num_beam_groups=1, # num of beam groups\n            do_sample=True, # use sampling instead of greedy search\n            temperature=0.5, # modulates the next token probabilities\n            diversity_penalty=0.0, # ads a penalty for generating unoriginal tokens\n            encoder_repetition_penalty=0.01, # ads a penalty for producing tokens that are not in the original input\n            no_repeat_ngram_size=5, # all ngrams of this size can only occur once\n            # guidance_scale = 1, # Higher guidance scale encourages the model to generate samples that are more closely linked to the input prompt, usually at the expense of poorer quality.\n            # length_penalty=-5 # promotes shorter token sequences\n    ),\n    predict_with_generate=True,\n    eval_accumulation_steps=10,\n    report_to=['none']\n    )","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:26:52.126004Z","iopub.execute_input":"2024-04-26T18:26:52.126359Z","iopub.status.idle":"2024-04-26T18:26:52.154666Z","shell.execute_reply.started":"2024-04-26T18:26:52.126330Z","shell.execute_reply":"2024-04-26T18:26:52.153902Z"},"trusted":true},"execution_count":268,"outputs":[]},{"cell_type":"code","source":"# base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n# model = PeftModel.from_pretrained(base_model, f\"{OUTPUT_DIR}/model_save\")\nmodel = PeftModel.from_pretrained(base_model, \"SurtMcGert/advanced-AI-CW-Med-Chat-Bot\", revision='0752cf1a926ea913cc2061679627cc779d68c20f', force_download=True)\n#model = AutoModelForCausalLM.from_pretrained(\"SurtMcGert/advanced-AI-CW-Med-Chat-Bot\", force_download=True, config=config, from_tf=True, revision='217f27b7a5aa155d8f8acc8570f8af3af5301763').to(DEVICE)\nmodel.config.pad_token_id = ltokenizer.pad_token_id\nmodel.config.max_length = MAX_TOKEN_LENGTH\n#model.gradient_checkpointing_enable()\n#model.enable_input_require_grads()\nevaluator = Seq2SeqTrainer(\n    model=model, \n    args=eval_args, \n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset_1, \n    data_collator=custom_data_collator\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:27:09.465858Z","iopub.execute_input":"2024-04-26T18:27:09.466234Z","iopub.status.idle":"2024-04-26T18:27:09.686150Z","shell.execute_reply.started":"2024-04-26T18:27:09.466205Z","shell.execute_reply":"2024-04-26T18:27:09.685374Z"},"trusted":true},"execution_count":269,"outputs":[]},{"cell_type":"code","source":"# model = None\n# evaluator = None\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:27:12.367155Z","iopub.execute_input":"2024-04-26T18:27:12.367521Z","iopub.status.idle":"2024-04-26T18:27:12.861416Z","shell.execute_reply.started":"2024-04-26T18:27:12.367489Z","shell.execute_reply":"2024-04-26T18:27:12.860380Z"},"trusted":true},"execution_count":270,"outputs":[{"execution_count":270,"output_type":"execute_result","data":{"text/plain":"81"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"eval_result_1 = evaluator.predict(eval_dataset_1)\n#eval_result_1 = evaluator.predict(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:27:14.580486Z","iopub.execute_input":"2024-04-26T18:27:14.581163Z","iopub.status.idle":"2024-04-26T18:28:36.415641Z","shell.execute_reply.started":"2024-04-26T18:27:14.581128Z","shell.execute_reply":"2024-04-26T18:28:36.414650Z"},"trusted":true},"execution_count":271,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"eval_result_2 = evaluator.predict(eval_dataset_2)\n#eval_result_2 = evaluator.predict(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T16:56:27.719054Z","iopub.execute_input":"2024-04-26T16:56:27.719465Z","iopub.status.idle":"2024-04-26T16:58:10.773123Z","shell.execute_reply.started":"2024-04-26T16:56:27.719433Z","shell.execute_reply":"2024-04-26T16:58:10.772205Z"},"trusted":true},"execution_count":113,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"logits_1 = eval_result_1.predictions\nlogits_1[logits_1 == -100] = ltokenizer.eos_token_id\nlogits_2 = eval_result_2.predictions\nlogits_2[logits_2 == -100] = ltokenizer.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:30:32.780703Z","iopub.execute_input":"2024-04-26T18:30:32.781567Z","iopub.status.idle":"2024-04-26T18:30:32.786922Z","shell.execute_reply.started":"2024-04-26T18:30:32.781533Z","shell.execute_reply":"2024-04-26T18:30:32.785918Z"},"trusted":true},"execution_count":272,"outputs":[]},{"cell_type":"code","source":"# get the raw evaluation output\nraw_text_result_1 = ltokenizer.batch_decode(logits_1, skip_special_tokens=True)\nraw_text_result_2 = ltokenizer.batch_decode(logits_2, skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:30:35.981860Z","iopub.execute_input":"2024-04-26T18:30:35.982207Z","iopub.status.idle":"2024-04-26T18:30:35.993739Z","shell.execute_reply.started":"2024-04-26T18:30:35.982180Z","shell.execute_reply":"2024-04-26T18:30:35.992813Z"},"trusted":true},"execution_count":273,"outputs":[]},{"cell_type":"code","source":"# get the questions and ground truths from both evaluation datasets\nquestions_1 = []\nground_truth_1 = []\ntry:\n    for item in eval_dataset_1['messages']:\n        questions_1.append(item[1][\"content\"])\n        ground_truth_1.append(item[2][\"content\"])\nexcept:\n    pass\n\nquestions_2 = []\nground_truth_2 = []\ntry:\n    for item in eval_dataset_2['messages']:\n        questions_2.append(item[1][\"content\"])\n        ground_truth_2.append(item[2][\"content\"])\nexcept:\n    pass\n\n# create lists for the text outputs\ntext_result_1 = list()\ntext_result_2 = list()\n\n# get the answers for the MedDialog dataset\nfor item in raw_text_result_1:\n    index = item.find(\"<|assistant|>\")\n    output = item[index+13:]\n    index = output.find(ltokenizer.eos_token)\n    if(index > -1):\n        output = output[:index]\n    text_result_1.append(output)\n\n\n# get the answers for the USMLE dataset\nfor item in raw_text_result_2:\n    index = item.find(\"<|assistant|>\")\n    output = item[index+13:]\n    index = output.find(ltokenizer.eos_token)\n    if(index > -1):\n        output = output[:index]\n    text_result_2.append(output)\n    \n    \n# save to csv\ndata = {\"question\": questions_1, \"ground_truth\": ground_truth_1, \"prediction\": text_result_1}\ndf = pd.DataFrame(data)\ndf.to_csv(\"eval_output_1.csv\", index=False) \ndata = {\"question\": questions_2, \"ground_truth\": ground_truth_2, \"prediction\": text_result_2}\ndf = pd.DataFrame(data)\ndf.to_csv(\"eval_output_2.csv\", index=False)  \n\n\n\n# print the first 2 results from each dataset evaluation\nprint(\"============================MedDialog Evaluation============================\")\nfor question, gt, answer in list(zip(questions_1, ground_truth_1, text_result_1))[:2]:\n    print(f\"\"\"\n    Question: {question}\n    Ground Truth: {gt}\n    Prediction: {answer}\n    \"\"\")\n\nprint(\"============================USMLE Evaluation============================\")\nfor question, gt, answer in list(zip(questions_2, ground_truth_2, text_result_2))[:2]:\n    print(f\"\"\"\n    Question: {question}\n    Ground Truth: {gt}\n    Prediction: {answer}\n    \"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:30:38.909410Z","iopub.execute_input":"2024-04-26T18:30:38.909792Z","iopub.status.idle":"2024-04-26T18:30:38.929163Z","shell.execute_reply.started":"2024-04-26T18:30:38.909747Z","shell.execute_reply":"2024-04-26T18:30:38.928206Z"},"trusted":true},"execution_count":274,"outputs":[{"name":"stdout","text":"============================MedDialog Evaluation============================\n\n    Question: Hi doctor, I am a 15 year old boy and I have hyperhidrosis. I sweat so much. My shirt cannot stay on for at least 5 minutes. In some half an hour to one hour my whole shirt and armpit area will drench in sweat. Yes, I take a shower twice a day and put deodorant every day. I have even tried the antiperspirant deodorant, but that did not stop it and I think it made it worse. I really want to believe it is just puberty as this has been going on since I was 12 or 13. But, something tells me that it is not because of puberty. Everyone else at school does not have this problem and some relatives in my family has it too. Just like my shirt, my pants will start soaking after 30 minutes of sitting down. I really want a diagnosis or close to one before I actually go to a doctor as I have not yet regarding my issue. Also are there any home remedies or other over-the-counter deodorant to help minimize this issue or completely stop it? I am really fit and regularly lift weights, play lots of sports (soccer, boxing, etc.,) and keep my body in good shape. I know that weight lifting could not be a problem if that was a suggestion. I sweat a lot when I am in social places such as school or when I go out. It is not because of anxiety, but I rarely feel nervous when my arms are getting drenched. This is a more serious problem in the morning either at home or at a social place and gradually calms during the afternoon when I am at home. It is so bad that my armpits can drench two layers of clothing. For example, if I wear a shirt and a jacket over it, then it does not really matter how many layers and by the end of the day my clothing will be soaked in sweat. I do not really have any symptoms besides the fact that I am always on alert when I am out. This is really embarrassing and I have no idea what to do about this. Please help.\n    Ground Truth: Hi. For further information consult an internal medicine physician online --> https://www.icliniq.com/ask-a-doctor-online/internal-medicine-physician  \n    Prediction: Hi. I have gone through your query in detail and can understand your concern. For further information consult an internal medicine physician online --> https://www.icliniq.com/ask-a-doctor-online/internal-medicine-physician   Revert back with the answers to the above questions to your doctor of paediatrics and pediatric gynaecology and obstetrics and gynecology (obstetrics and gynaec\n    \n\n    Question: Hello doctor, My wife is 5 months pregnant. She is treated in one of the reputed hospitals. She has been advised to take Feronia -XT and Cal 360 tablets. Over last three to four days she is having cough in the night time and she is afraid to use any tablets or tonic. Kindly suggest which tablet or tonic she should take.\n    Ground Truth: Hi. Blood test to check for hemoglobin level, total count and differential count. Cough in pregnancy. Revert back after the investigations to an obstetrician and gynaecologist online.---> https://www.icliniq.com/ask-a-doctor-online/obstetrician-and-gynaecologist  \n    Prediction: Hello. For further information consult an obstetrician and gynaecologist online --> https://www.icliniq.com/ask-a-doctor-online/obstetrician-and-gynaecologist   I hope this helps.   If you have any further queries, please don't hesitate to ask.   Best regards,   For more information consult a general practitioner online --> https   Do not hesitate to contact me if you\n    \n============================USMLE Evaluation============================\n\n    Question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?, choose from A) Disclose the error to the patient and put it in the operative report, B) Tell the attending that he cannot fail to disclose this mistake, C) Report the physician to the ethics committee, D)  Refuse to dictate the operative report\n    Ground Truth: Tell the attending that he cannot fail to disclose this mistake\n    Prediction: A) Disclose the error to the patient and put it in the operative report   B) Tell the attending that he cannot fail to disclose this mistake   C) Report the physician to the ethics committee   D)  Refuse to dictate the operative report\n\nA) Disclose the error to the patient and put it in the operative report   B) Tell the attending that he cannot fail to disclose this mistake   C) Report the physician\n    \n\n    Question: A 67-year-old man with transitional cell carcinoma of the bladder comes to the physician because of a 2-day history of ringing sensation in his ear. He received this first course of neoadjuvant chemotherapy 1 week ago. Pure tone audiometry shows a sensorineural hearing loss of 45 dB. The expected beneficial effect of the drug that caused this patient's symptoms is most likely due to which of the following actions?, choose from A) Inhibition of proteasome, B) Hyperstabilization of microtubules, C) Generation of free radicals, D)  Cross-linking of DNA\n    Ground Truth: Cross-linking of DNA\n    Prediction: A) Inhibition of proteasome, B) Hyperstabilization of microtubules, C) Generation of free radicals, D) Cross-linking of DNA   A) Inhibition of proteasome, B) Hyperstabilization of microtubules, C) Generation of free radicals, D) Cross-linking of DNA   A) Inhibition of proteasome, B) Hyperstabilization of microtubules,\n    \n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Load The Evaluations","metadata":{}},{"cell_type":"code","source":"file_path = \"eval_output_1.csv\"\n\n# Read the CSV file into a pandas DataFrame\ndf = pd.read_csv(file_path)\n\n# Extract data into separate variables\nquestions_1 = df['question'].tolist()\nground_truth_1 = df['ground_truth'].tolist()\ntext_result_1 = df['prediction'].tolist()\n\n\nfile_path = \"eval_output_2.csv\"\n\n# Read the CSV file into a pandas DataFrame\ndf = pd.read_csv(file_path)\n\n# Extract data into separate variables\nquestions_2 = df['question'].tolist()\nground_truth_2 = df['ground_truth'].tolist()\ntext_result_2 = df['prediction'].tolist()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:34:28.494406Z","iopub.execute_input":"2024-04-26T15:34:28.494835Z","iopub.status.idle":"2024-04-26T15:34:28.504918Z","shell.execute_reply.started":"2024-04-26T15:34:28.494804Z","shell.execute_reply":"2024-04-26T15:34:28.504036Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"markdown","source":"## Load the Required Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"# perplexity - measures certainty of the model.\n# METEOR - extension of BLEU (measure similarity between the output and the ground truth) but accounts for word semantics.\n# ROUGE - considers n-gram overlap (recall) but also precision.\n# SQuAD v2 - a metric for measuring a models correctness in answering the multiple choice questions\n# Accuracy - use this for the multiple choice dataset\n\nperplexity_scorer = evaluate.load('perplexity')\nmeteor_scorer = evaluate.load('meteor')\nrouge_scorer = evaluate.load('rouge')\nsquad_scorer = evaluate.load('squad_v2')\naccuracy_scorer = evaluate.load('accuracy')\nbert_scorer = evaluate.load(\"bertscore\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:21:41.240106Z","iopub.execute_input":"2024-04-26T18:21:41.241064Z","iopub.status.idle":"2024-04-26T18:21:46.078272Z","shell.execute_reply.started":"2024-04-26T18:21:41.241026Z","shell.execute_reply":"2024-04-26T18:21:46.077464Z"},"trusted":true},"execution_count":258,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# compute the bleu and rouge scores for the MedDialog evaluation\nperplexity_score_1 = perplexity_scorer.compute(model_id='gpt2', predictions=text_result_1, references=ground_truth_1)\nmeteor_score_1 = meteor_scorer.compute(predictions=text_result_1, references=ground_truth_1)\nrouge_score_1 = rouge_scorer.compute(predictions=text_result_1, references=ground_truth_1)\nbert_score_1 = bert_scorer.compute(predictions=text_result_1, references=ground_truth_1, lang=\"en\")\nbert_score_1['precision'] = statistics.mean(bert_score_1['precision'])\nbert_score_1['recall'] = statistics.mean(bert_score_1['recall'])\nbert_score_1['f1'] = statistics.mean(bert_score_1['f1'])\n\n# compute the bleu and rouge scores for the USMLE evaluation\n# accuracy_score_2 = accuracy_scorer.compute(predictions=text_result_2, references=ground_truth_2)\n# squad_text_result_2 = [{'id': i,\n#                         'prediction_text': text,\n#                         'no_answer_probability': 0.0\n#                        }for i, text in enumerate(text_result_2)]\n\n# squad_ground_truth_2 = [{'id': i,\n#                         'answers': text,\n#                         'no_answer_threshold': 0.0\n#                        }for i, text in enumerate(ground_truth_2)]\n# squad_score_1 = squad_scorer.compute(predictions=text_result_2, references=ground_truth_2)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:31:55.564485Z","iopub.execute_input":"2024-04-26T18:31:55.564916Z","iopub.status.idle":"2024-04-26T18:31:57.463826Z","shell.execute_reply.started":"2024-04-26T18:31:55.564880Z","shell.execute_reply":"2024-04-26T18:31:57.462763Z"},"trusted":true},"execution_count":275,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5045d2271af647e1933493f140fa0087"}},"metadata":{}}]},{"cell_type":"code","source":"# print scores for MedDialog evaluation\nprint(perplexity_score_1)\nprint(meteor_score_1)\nprint(rouge_score_1)\nprint(bert_score_1)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:32:04.275709Z","iopub.execute_input":"2024-04-26T18:32:04.276181Z","iopub.status.idle":"2024-04-26T18:32:04.281949Z","shell.execute_reply.started":"2024-04-26T18:32:04.276142Z","shell.execute_reply":"2024-04-26T18:32:04.280961Z"},"trusted":true},"execution_count":276,"outputs":[{"name":"stdout","text":"{'perplexities': [28.271732330322266, 16.25998878479004, 17.626953125, 43.81818389892578, 10.108345985412598, 20.51332664489746, 11.719710350036621, 49.175899505615234, 45.97214126586914, 10.865894317626953], 'mean_perplexity': 25.43321762084961}\n{'meteor': 0.42984826791769315}\n{'rouge1': 0.41588898786287953, 'rouge2': 0.3053402803947466, 'rougeL': 0.36414487416179, 'rougeLsum': 0.36039881272811514}\n{'precision': 0.8633840978145599, 'recall': 0.8899352192878723, 'f1': 0.8755414724349976, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.40.1)'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# print scores for USMLE evaluation\n# print(accuracy_score_2)\nprint(squad_score_1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TESTING JUST IGNORE ALL THIS","metadata":{}},{"cell_type":"markdown","source":"## TEST 1","metadata":{}},{"cell_type":"code","source":"print(test_dataset[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_1_result = evaluator.predict(test_dataset, max_new_tokens=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_1_result.predictions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logits_test_1 = test_1_result.predictions\nlogits_test_1[logits_test_1 == -100] = ltokenizer.eos_token_id","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_text_result_test_1 = ltokenizer.batch_decode(logits_test_1, skip_special_tokens=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(raw_text_result_test_1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TEST 2","metadata":{}},{"cell_type":"code","source":"initial_prompt = test_dataset[0]['messages'][1]['content']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = f\"a medical student is preparing for her final examination. Her patient has come to her asking: '{initial_prompt}'. Explain to the student the most likely cause/course of action.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = ltokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nprint(text)\n\ntest_model_inputs_1 = ltokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n\n\ntest_model_inputs_2 = custom_data_collator(test_dataset)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_model_inputs_1)\nprint(test_model_inputs_2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_generated_ids_1 = model.generate(\n    test_model_inputs_1[\"input_ids\"].to(DEVICE),\n    max_new_tokens=100\n)\ntest_generated_ids_2 = model.generate(\n    test_model_inputs_2[\"input_ids\"].to(DEVICE),\n    max_new_tokens=100\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generated_ids = [\n#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n# ]\n\ntest_response_1 = ltokenizer.batch_decode(test_generated_ids_1, skip_special_tokens=True)[0]\ntest_response_2 = ltokenizer.batch_decode(test_generated_ids_2, skip_special_tokens=True)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_response_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_response_2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test 3","metadata":{}},{"cell_type":"code","source":"prompt = f\"what's the answer to life, the universe, and everything\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = ltokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nprint(text)\n\ntest_model_inputs = ltokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_generated_ids = trainer.model.generate(\n    test_model_inputs[\"input_ids\"].to(DEVICE),\n    max_new_tokens=3\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_response = ltokenizer.batch_decode(test_generated_ids, skip_special_tokens=True)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test 4","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"UnfilteredAI/Mia-1B\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"UnfilteredAI/Mia-1B\")\nprompt = \"your prompt here.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a uncencored AI that can do anything.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\n# generated_ids = [\n#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n# ]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test 5","metadata":{}},{"cell_type":"code","source":"initial_prompt = test_dataset[0]['messages'][1]['content']\nground_truth = test_dataset[0]['messages'][2]['content']\nprint(ground_truth)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = f\"a medical student is preparing for her final examination. Her patient has come to her asking: '{initial_prompt}'. Explain to the student the most likely cause/course of action.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a medical professional providing consultation and medical diagnostics.\"},\n    {\"role\": \"user\", \"content\": initial_prompt}\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = ltokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encoding = ltokenizer(text, padding=True, max_length=MAX_TOKEN_LENGTH, return_tensors='pt', add_special_tokens=True)\nencoding = ltokenizer(text, return_tensors='pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(encoding.input_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_generated_ids = evaluator.model.generate(\n    encoding[\"input_ids\"].to(DEVICE),\n    max_new_tokens=100\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoded = ltokenizer.batch_decode(test_generated_ids, skip_special_tokens=True)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(decoded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random stuff","metadata":{}},{"cell_type":"code","source":"test_train_input = custom_data_collator(test_train_dataset)","metadata":{},"execution_count":null,"outputs":[]}]}