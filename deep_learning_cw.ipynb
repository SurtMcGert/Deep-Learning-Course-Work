{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from typing import List, Dict, Union\n",
    "from typing import Any, TypeVar\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "import gc\n",
    "import evaluate\n",
    "import opendatasets as od\n",
    "\n",
    "from datasets import load_dataset, Features, Value\n",
    "\n",
    "from transformers import AutoTokenizer, TrainingArguments \n",
    "from transformers import Seq2SeqTrainer, AutoModelForCausalLM, IntervalStrategy\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set a seed and confirm CUDA support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.2.1+cu121\n",
      "torchtext Version:  0.17.1+cpu\n",
      "Using GPU.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2137)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"torchtext Version: \", torchtext.__version__)\n",
    "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading MedDialog Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: you will need a kaggle API key for the following to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSON file\n",
    "json_file_path = \"kaggle.json\"\n",
    "\n",
    "# Open the file and read the content\n",
    "try:\n",
    "  with open(json_file_path, \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "  print(f\"Error: JSON file not found at {json_file_path}\")\n",
    "  exit(1)\n",
    "\n",
    "# Access username and key from the JSON data\n",
    "try:\n",
    "  username = json_data[\"username\"]\n",
    "  key = json_data[\"key\"]\n",
    "except KeyError:\n",
    "  print(\"Error: 'username' or 'key' key not found in JSON data\")\n",
    "  exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading diagnoise-me.zip to dataset\\diagnoise-me\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191M/191M [00:41<00:00, 4.82MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = username\n",
    "os.environ['KAGGLE_KEY'] = key\n",
    "\n",
    "# Assign the Kaggle data set URL into variable\n",
    "dataset = 'https://www.kaggle.com/datasets/dsxavier/diagnoise-me'\n",
    "# Using opendatasets let's download the data sets\n",
    "od.download(dataset, \"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading LiveEQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveEQA_dataset = load_dataset(\"truehealth/liveqa\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questionid': 'Q1', 'subject': None, 'message': 'Literature on Cardiac amyloidosis.  Please let me know where I can get literature on Cardiac amyloidosis.  My uncle died yesterday from this disorder.  Since this is such a rare disorder, and to honor his memory, I would like to distribute literature at his funeral service.  I am a retired NIH employee, so I am familiar with the campus in case you have literature at NIH that I can come and pick up.  Thank you ', 'focus': 'cardiac amyloidosis', 'type': 'information', 'answerid': 'Q1-S1-A1', 'pairid': '1', 'answer': 'Cardiac amyloidosis is a disorder caused by deposits of an abnormal protein (amyloid) in the heart tissue. These deposits make it hard for the heart to work properly.'}\n"
     ]
    }
   ],
   "source": [
    "print(liveEQA_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MedDialog Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'Description', 'Doctor', 'Patient'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"dataset\\\\diagnoise-me\\\\diagnose_en_dataset.feather\"\n",
    "SEQ_LEN: int = 1024\n",
    "data = pd.read_feather(DATA_PATH)\n",
    "print(data.keys())\n",
    "\n",
    "# data = data['Patient'].values\n",
    "\n",
    "\n",
    "# SAMPLE_SIZE: int =  int(data.shape[0] * 0.01) #get 1% of the data\n",
    "# _data = [el[:SEQ_LEN]  for el in data[:SAMPLE_SIZE]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (205975, 4)\n",
      "Eval data shape: (51494, 4)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and eval sets with 80% for training\n",
    "train_data, eval_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the train and eval sets\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Eval data shape:\", eval_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading LiveEQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiveEQA data shape: (635, 2)\n"
     ]
    }
   ],
   "source": [
    "liveEQA_dataset = pd.DataFrame({'Doctor': liveEQA_dataset[\"answer\"], 'Patient': liveEQA_dataset[\"message\"]})\n",
    "# Print the shapes of the set\n",
    "print(\"LiveEQA data shape:\", liveEQA_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./results', exist_ok = True)\n",
    "OUTPUT_DIR: str = './results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens for the datset\n",
    "MODEL_NAME: str = 'EleutherAI/gpt-neo-125M'\n",
    "BOS_TOKEN: str = '<|startoftext|>'\n",
    "EOS_TOKEN: str = '<|endoftext|>'\n",
    "PAD_TOKEN: str = '<|pad|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer \n",
    "MAX_TOKEN_LENGTH = 1024\n",
    "\n",
    "# for evaluation\n",
    "ltokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, bos_token = BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN)\n",
    "ltokenizer.padding_side = 'left'\n",
    "ltokenizer.truncation_side = 'left'\n",
    "\n",
    "# for training\n",
    "rtokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, bos_token = BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN)\n",
    "rtokenizer.padding_side = 'right'\n",
    "rtokenizer.truncation_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e759bed80d416e8bd33e55afcbb353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d1cd9016054b69addcdb35dd45f368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b689b543508249ddbfd7693a44141916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  AutoModelForCausalLM.from_pretrained(MODEL_NAME).cuda()\n",
    "model.resize_token_embeddings(len(rtokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoctorPatientDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, split):\n",
    "        \n",
    "        self.input_x: List = data[\"Patient\"]\n",
    "        self.target: List = data[\"Doctor\"]\n",
    "        self.split = split\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = {\n",
    "            'input': self.input_x[idx],\n",
    "            'target': self.target[idx],\n",
    "            'split': self.split\n",
    "        }\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DoctorPatientDataset(data = train_data, split = \"train\")\n",
    "eval_dataset_1 = DoctorPatientDataset(data = eval_data, split = \"eval\")\n",
    "eval_dataset_2 = DoctorPatientDataset(data = liveEQA_dataset, split = \"eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_data_collator(features, return_tensors=\"pt\"):\n",
    "    batch = {}\n",
    "\n",
    "    questions = [feature[\"input\"] for feature in features]\n",
    "    answers = [feature[\"target\"] for feature in features]\n",
    "    split = features[0][\"split\"]\n",
    "\n",
    "    # training\n",
    "    if split == 'train':\n",
    "        tokenizer = rtokenizer\n",
    "        bos_token = rtokenizer.bos_token\n",
    "        eos_token = rtokenizer.eos_token\n",
    "        text = [f\"{bos_token}Question:{q}.Answer:{t}{eos_token}\" for q, t in zip(questions, answers)]\n",
    "\n",
    "    # evaluation\n",
    "    else:\n",
    "        # Format text to be encoded\n",
    "        tokenizer = ltokenizer\n",
    "        bos_token = ltokenizer.bos_token\n",
    "        text = [f\"{bos_token}Context:{q}.Target:\" for q in questions]\n",
    "\n",
    "\n",
    "    # Tokenize the text\n",
    "    encoding = tokenizer(text, truncation=True, padding='max_length', max_length=MAX_TOKEN_LENGTH, return_tensors=return_tensors, add_special_tokens=False)\n",
    "\n",
    "    # Prepare final batch dictionary\n",
    "    batch[\"input_ids\"] = encoding[\"input_ids\"]\n",
    "    batch[\"attention_mask\"] = encoding[\"attention_mask\"]\n",
    "\n",
    "    if return_tensors in [\"pt\", \"tf\"]:\n",
    "        batch[\"labels\"] = copy.deepcopy(encoding[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = OUTPUT_DIR, \n",
    "    num_train_epochs = 2, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 5000,\n",
    "    logging_steps = 5000,\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit = 1,\n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=1, \n",
    "    warmup_steps=50, \n",
    "    weight_decay=0.01, \n",
    "    logging_dir='./logs', \n",
    "    load_best_model_at_end=True,\n",
    "    report_to=['tensorboard']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset_1, \n",
    "    data_collator=custom_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(f\"{OUTPUT_DIR}/model_save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(f\"{OUTPUT_DIR}/model_save\")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset_1, \n",
    "    data_collator=custom_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_1 = trainer.predict(eval_dataset_1, max_new_tokens=MAX_TOKEN_LENGTH)\n",
    "eval_result_2 = trainer.predict(eval_dataset_2, max_new_tokens=MAX_TOKEN_LENGTH)\n",
    "logits_1 = eval_result_1.predictions\n",
    "logits_1[logits_1 == -100] = ltokenizer.eos_token_id\n",
    "logits_2 = eval_result_2.predictions\n",
    "logits_2[logits_2 == -100] = ltokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the raw evaluation output\n",
    "raw_text_result_1 = ltokenizer.batch_decode(logits_1, skip_special_tokens=True)\n",
    "raw_text_result_2 = ltokenizer.batch_decode(logits_2, skip_special_tokens=True)\n",
    "\n",
    "# get the questions and ground truths from both evaluation datasets\n",
    "questions_1 = []\n",
    "ground_truth_1 = []\n",
    "try:\n",
    "    for item in eval_dataset_1:\n",
    "        questions_1.append(item[\"input\"])\n",
    "        ground_truth_1.append(item[\"target\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "questions_2 = []\n",
    "ground_truth_2 = []\n",
    "try:\n",
    "    for item in eval_dataset_2:\n",
    "        questions_2.append(item[\"input\"])\n",
    "        ground_truth_2.append(item[\"target\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# create lists for the text outputs\n",
    "text_result_1 = list()\n",
    "text_result_2 = list()\n",
    "\n",
    "# get the answers for the MedDialog dataset\n",
    "for item in raw_text_result_1:\n",
    "    index = item.find(\"Answer:\")\n",
    "    output = item[index+7:]\n",
    "    index = output.find(ltokenizer.eos_token)\n",
    "    if(index > -1):\n",
    "        output = output[:index]\n",
    "    text_result_1.append(output)\n",
    "\n",
    "\n",
    "# get the answers for the LiveEQA dataset\n",
    "for item in raw_text_result_2:\n",
    "    index = item.find(\"Answer:\")\n",
    "    output = item[index+7:]\n",
    "    index = output.find(ltokenizer.eos_token)\n",
    "    if(index > -1):\n",
    "        output = output[:index]\n",
    "    text_result_2.append(output)\n",
    "\n",
    "\n",
    "\n",
    "# print the first 2 results from each dataset evaluation\n",
    "print(\"============================MedDialog Evaluation============================\")\n",
    "for question, answer in list(zip(questions_1, text_result_1))[:2]:\n",
    "    print(f\"\"\"\n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    \"\"\")\n",
    "\n",
    "print(\"============================LiveEQA Evaluation============================\")\n",
    "for question, answer in list(zip(questions_2, text_result_2))[:2]:\n",
    "    print(f\"\"\"\n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the evaluation metrics\n",
    "bleu_scorer = evaluate.load('bleu')\n",
    "rouge_scorer = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the bleu and rouge scores for the MedDialog evaluation\n",
    "bleu_score_1 = bleu_scorer.compute(predictions=text_result_1, references=ground_truth_1)\n",
    "rouge_score_1 = rouge_scorer.compute(predictions=text_result_1, references=ground_truth_1)\n",
    "\n",
    "# compute the bleu and rouge scores for the LiveEAQ evaluation\n",
    "bleu_score_2 = bleu_scorer.compute(predictions=text_result_1, references=ground_truth_2)\n",
    "rouge_score_2 = rouge_scorer.compute(predictions=text_result_1, references=ground_truth_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print scores for MedDialog evaluation\n",
    "print(\"score on MedDialog Dataset\")\n",
    "print('BLEU1:', bleu_score_1['precisions'][0]*100)\n",
    "print(f\"\"\"\n",
    "ROUGE-1: {rouge_score_1['rouge1']*100}\n",
    "ROUGE-2: {rouge_score_1['rouge2']*100}\n",
    "ROUGE-L: {rouge_score_1['rougeL']*100}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print scores for LiveEAQ evaluation\n",
    "print(\"score on LiveEQA Dataset\")\n",
    "print('BLEU1:', bleu_score_2['precisions'][0]*100)\n",
    "print(f\"\"\"\n",
    "ROUGE-1: {rouge_score_2['rouge1']*100}\n",
    "ROUGE-2: {rouge_score_2['rouge2']*100}\n",
    "ROUGE-L: {rouge_score_2['rougeL']*100}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
